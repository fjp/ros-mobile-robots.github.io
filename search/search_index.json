{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Welcome to DiffBot Documentation This project guides you on how to build an autonomous two wheel differential drive robot. The robot can operate on a Raspberry Pi 4 B or NVIDIA Jetson Nano Developer Kit running ROS Noetic or ROS Melodic middleware on Ubuntu Mate 20.04 and Ubuntu 18.04 respectively. With a motor driver and two actuators it can drive autonomously to a desired location while sensing its environment using sensors, such as a camera and an laser scanner to avoid obstacles. Odometry wheel encoders (also refered to as speed sensors) combined with an inertial measurement unit (IMU) are used for localization. The project is split into multiple parts, to adress the following main aspects of the robot. Bill of Materials (BOM) and the theory behind the parts. Theory of (mobile) robots . Assembly of the robot platform and the components. Setup of ROS (Noetic or Melodic) on either Raspberry Pi 4 B or Jetson Nano, which are both Single Board Computers (SBC) and are the brain of the robot. Modeling the Robot in Blender and URDF to simulate it in Gazebo. ROS packages and nodes: Hardware drivers to interact with the hardware components High level nodes for perception, navigation, localization and control. Use the menu to learn more about the ROS packages and other components of the robot. Note Using a Jetson Nano instead of a Raspberry Pi is also possible. See the Jetson Nano Setup section in this documentation for more details. To run ROS Noetic Docker is needed. Source Code The source code for this project can be found in the ros-mobile-robots/diffbot GitHub repository. Best Practices and REP The project tries to follow the ROS best practices as good as possible. This includes examples and patterns on producing and contributing high quality code, as well as on testing, and other quality oriented practices, like continuous integration. You can read more about it on the ROS Quality wiki . This includes also following the advices given in the ROS Enhancement Proposals (REPs) . Throughout the documentation links to corresponding REPs are given. The wiki section ROS developer's guide is a good starting point for getting used to the common practices for developing components to be shared with the community. It includes links to naming conventions (e.g. for packages) and ROS C++ and Python style guides. Other good resources to learn more about ROS best practices is the Autonomous Systems Lab of ETH Zurich. Note Your contributions to the code or documentation are most welcome but please try to follow the mentioned best pratices where possible. Testing, Debugging and CI For a ROS catkin workspace explaining gTest and rostest see Ros-Test-Example and its documentation . To run tests with catkin-tools, see Building and running tests . To get a workspace that allows a debugger to stop at breakpoints, it is required to build the catkin workspace with Debug Symbols. For this the command catkin build --save-config --cmake-args -DCMAKE_BUILD_TYPE=Debug is used, mentioned in the catkin-tools cheat sheet . This repository makes use of automated builds when new code is pushed or a pull reuqest is made to this repository. For this the Travis and GitHub actions configurations (yml files) from ROS Industrial CI are used. Documentation The documentation is using material design theme , which is based on MkDocs . Future code documentation will make use of doxygen and rosdoc_lite . References Helpful resources to bring your own robots into ROS are: Understand ROS Concepts Follow ROS Tutorials such as Using ROS on your custom Robot Books: Robot Operating System (ROS) for Absolute Beginners from Apress by Lentin Joseph Programming Robots with ROS A Practical Introduction to the Robot Operating System from O'Reilly Media Mastering ROS for Robotics Programming Second Edition from Packt Elements of Robotics Robots and Their Applications from Springer ROS Robot Programming Book for Free! Handbook from Robotis written by Turtlebot3 Developers Courses: Robocademy ROS Online Course for Beginner Udacity Robotics Software Engineer","title":"Home"},{"location":"index.html#welcome-to-diffbot-documentation","text":"This project guides you on how to build an autonomous two wheel differential drive robot. The robot can operate on a Raspberry Pi 4 B or NVIDIA Jetson Nano Developer Kit running ROS Noetic or ROS Melodic middleware on Ubuntu Mate 20.04 and Ubuntu 18.04 respectively. With a motor driver and two actuators it can drive autonomously to a desired location while sensing its environment using sensors, such as a camera and an laser scanner to avoid obstacles. Odometry wheel encoders (also refered to as speed sensors) combined with an inertial measurement unit (IMU) are used for localization. The project is split into multiple parts, to adress the following main aspects of the robot. Bill of Materials (BOM) and the theory behind the parts. Theory of (mobile) robots . Assembly of the robot platform and the components. Setup of ROS (Noetic or Melodic) on either Raspberry Pi 4 B or Jetson Nano, which are both Single Board Computers (SBC) and are the brain of the robot. Modeling the Robot in Blender and URDF to simulate it in Gazebo. ROS packages and nodes: Hardware drivers to interact with the hardware components High level nodes for perception, navigation, localization and control. Use the menu to learn more about the ROS packages and other components of the robot. Note Using a Jetson Nano instead of a Raspberry Pi is also possible. See the Jetson Nano Setup section in this documentation for more details. To run ROS Noetic Docker is needed.","title":"Welcome to DiffBot Documentation"},{"location":"index.html#source-code","text":"The source code for this project can be found in the ros-mobile-robots/diffbot GitHub repository.","title":"Source Code"},{"location":"index.html#best-practices-and-rep","text":"The project tries to follow the ROS best practices as good as possible. This includes examples and patterns on producing and contributing high quality code, as well as on testing, and other quality oriented practices, like continuous integration. You can read more about it on the ROS Quality wiki . This includes also following the advices given in the ROS Enhancement Proposals (REPs) . Throughout the documentation links to corresponding REPs are given. The wiki section ROS developer's guide is a good starting point for getting used to the common practices for developing components to be shared with the community. It includes links to naming conventions (e.g. for packages) and ROS C++ and Python style guides. Other good resources to learn more about ROS best practices is the Autonomous Systems Lab of ETH Zurich. Note Your contributions to the code or documentation are most welcome but please try to follow the mentioned best pratices where possible.","title":"Best Practices and REP"},{"location":"index.html#testing-debugging-and-ci","text":"For a ROS catkin workspace explaining gTest and rostest see Ros-Test-Example and its documentation . To run tests with catkin-tools, see Building and running tests . To get a workspace that allows a debugger to stop at breakpoints, it is required to build the catkin workspace with Debug Symbols. For this the command catkin build --save-config --cmake-args -DCMAKE_BUILD_TYPE=Debug is used, mentioned in the catkin-tools cheat sheet . This repository makes use of automated builds when new code is pushed or a pull reuqest is made to this repository. For this the Travis and GitHub actions configurations (yml files) from ROS Industrial CI are used.","title":"Testing, Debugging and CI"},{"location":"index.html#documentation","text":"The documentation is using material design theme , which is based on MkDocs . Future code documentation will make use of doxygen and rosdoc_lite .","title":"Documentation"},{"location":"index.html#references","text":"Helpful resources to bring your own robots into ROS are: Understand ROS Concepts Follow ROS Tutorials such as Using ROS on your custom Robot Books: Robot Operating System (ROS) for Absolute Beginners from Apress by Lentin Joseph Programming Robots with ROS A Practical Introduction to the Robot Operating System from O'Reilly Media Mastering ROS for Robotics Programming Second Edition from Packt Elements of Robotics Robots and Their Applications from Springer ROS Robot Programming Book for Free! Handbook from Robotis written by Turtlebot3 Developers Courses: Robocademy ROS Online Course for Beginner Udacity Robotics Software Engineer","title":"References"},{"location":"DG01D-E-motor-with-encoder.html","text":"Motor with Wheel Encoder The DG01D-E is a single hobby motor with a hall speed encoder. This motor requires a voltage between 3-9V, has a gearbox ratio of 1:48 and a speed of 90RPM at 4.5V. The voltage between positive and negative is determined according to the power supply voltage of the single chip microcomputer used, generally 3.3V or 5V is used. The hall sensor can sense the North and South poles of its magnetic plate. When the hall sensor senses the South of the magnetic plate, the hall output will result in a high level. Meanwhile the North is the inverse and, when sensed, the hall output will result a low level. Terminal Pin Layout The pins on the product are as follows, when looking at the connector on the housing, motor down/connector up, from right to left. The colors correspond to the included cable when plugged in to the connection slot. G (Blue): hall power negative H1 (Green): hall H1 output signal, square wave H2 (Yellow): hall H2 output signal, square wave V (Orange): hall power positive M+ (Red): motor positive pole M- (Brown): motor negative pole The following image shows the motor from its side with the corresponding pin descriptions: DG01D-E Motor with encoder pin description. Wheel Encoder Measurements This section shows oscilloscope waveform measurements of the quadrature encoder in the DG01D-E motor. The motor is connected to the Grove I2C Motor Driver that is powerd with 10 VDC. The motor_example.py applies 50-100% of the 10 VDC which leads to the following output voltages on the motor: Voltage sweep measurements 0:00 Forward Speed 50: 6.5 VDC 0:12 Back Speed 50: 6.5 VDC 0:23 Forward Speed 60: 6.9 VDC 0:34 Back Speed 60: 6.9 VDC 0:46 Forward Speed 70: 7.2 VDC 0:56 Back Speed 70: 7.2 VDC 1:07 Forward 80: 7.3 VDC 1:18 Back 80: 7.3 VDC 1:29 Forward 90: 7.6 VDC 1:41 Back 90: 7.6 VDC 1:52 Forward 100: 7.9 VDC 2:02 Back 100: 7.9 VDC At the bottom of the pico scope window the cycle time, duty cycle, high and low pulse width measurements are shown for both encoder signals. Oscilloscope is the PicoScope 3000 Series with 2 Channels. To download and install Pico Scope software on Linux refer to the documentation . Summary of installation instructions Add repository to the updater 1 sudo bash -c 'echo \"deb https://labs.picotech.com/debian/ picoscope main\" >/etc/apt/sources.list.d/picoscope.list' Import public key 1 wget -O - https://labs.picotech.com/debian/dists/picoscope/Release.gpg.key | sudo apt-key add - Update package manager cache 1 sudo apt-get update Install PicoScope 1 sudo apt-get install picoscope","title":"Motor and Encoder"},{"location":"DG01D-E-motor-with-encoder.html#motor-with-wheel-encoder","text":"The DG01D-E is a single hobby motor with a hall speed encoder. This motor requires a voltage between 3-9V, has a gearbox ratio of 1:48 and a speed of 90RPM at 4.5V. The voltage between positive and negative is determined according to the power supply voltage of the single chip microcomputer used, generally 3.3V or 5V is used. The hall sensor can sense the North and South poles of its magnetic plate. When the hall sensor senses the South of the magnetic plate, the hall output will result in a high level. Meanwhile the North is the inverse and, when sensed, the hall output will result a low level.","title":"Motor with Wheel Encoder"},{"location":"DG01D-E-motor-with-encoder.html#terminal-pin-layout","text":"The pins on the product are as follows, when looking at the connector on the housing, motor down/connector up, from right to left. The colors correspond to the included cable when plugged in to the connection slot. G (Blue): hall power negative H1 (Green): hall H1 output signal, square wave H2 (Yellow): hall H2 output signal, square wave V (Orange): hall power positive M+ (Red): motor positive pole M- (Brown): motor negative pole The following image shows the motor from its side with the corresponding pin descriptions: DG01D-E Motor with encoder pin description.","title":"Terminal Pin Layout"},{"location":"DG01D-E-motor-with-encoder.html#wheel-encoder-measurements","text":"This section shows oscilloscope waveform measurements of the quadrature encoder in the DG01D-E motor. The motor is connected to the Grove I2C Motor Driver that is powerd with 10 VDC. The motor_example.py applies 50-100% of the 10 VDC which leads to the following output voltages on the motor: Voltage sweep measurements 0:00 Forward Speed 50: 6.5 VDC 0:12 Back Speed 50: 6.5 VDC 0:23 Forward Speed 60: 6.9 VDC 0:34 Back Speed 60: 6.9 VDC 0:46 Forward Speed 70: 7.2 VDC 0:56 Back Speed 70: 7.2 VDC 1:07 Forward 80: 7.3 VDC 1:18 Back 80: 7.3 VDC 1:29 Forward 90: 7.6 VDC 1:41 Back 90: 7.6 VDC 1:52 Forward 100: 7.9 VDC 2:02 Back 100: 7.9 VDC At the bottom of the pico scope window the cycle time, duty cycle, high and low pulse width measurements are shown for both encoder signals. Oscilloscope is the PicoScope 3000 Series with 2 Channels. To download and install Pico Scope software on Linux refer to the documentation . Summary of installation instructions Add repository to the updater 1 sudo bash -c 'echo \"deb https://labs.picotech.com/debian/ picoscope main\" >/etc/apt/sources.list.d/picoscope.list' Import public key 1 wget -O - https://labs.picotech.com/debian/dists/picoscope/Release.gpg.key | sudo apt-key add - Update package manager cache 1 sudo apt-get update Install PicoScope 1 sudo apt-get install picoscope","title":"Wheel Encoder Measurements"},{"location":"components.html","text":"Part list and assembly of the robot platform and the components. Category Hardware Part Number Data Sheet & Info Accessories Case for Raspberry Pi 4 B Slim acrylic case for Raspberry Pi 4, stackable, rainbow/transparent BerryBase Micro SD Card SanDisk 64GB Class 10 SanDisk , Ubuntu 18.04 Image Robot Car Kit 2WD robot05 Instructions manual Power bank Intenso Powerbank S10000 Intenso Actuator (Deprecated) Gearbox motor DC Gearbox motor - \"TT Motor\" - 200RPM - 3 to 6VDC Adafruit DG01E-E Motor with encoder DG01E-E Hobby motor with quadrature encoder Sparkfun Board Raspberry Pi 4 B Raspberry Pi 4 B - 4 GB OEM Website Cables Jumper - Female to Female Jumper - Male to Male Micro USB - USB Cable Camera extension cable I2C 4 pin cable Electronics Fan Fan 30x30x7mm 5V DC with Dupont connector BerryBase I2C motor driver Grove - I2C Motor Driver Seeed Studio I2C Hub Grove - I2C Hub Seeed Studio Human Machine Interface OLED Display Grove OLED Display 0.96\" Seeed Studio LED Ring NeoPixel Ring 12x5050 RGB LED Adafruit Sensors Camera module Raspberry Pi - camera module v2.1 Raspberry Pi Ultrasonic ranger Grove - Ultrasonic Ranger Seeed Studio IMU Adafruit 9-DOF Absolute Orientation IMU Fusion Breakout - BNO055 Adafruit Odometry Joy-IT - LM393 Speed Sensor with H206 slot-type opto interrupter Joy-IT Order list Part Store Raspberry Pi 4 B (4 Gb) Amazon.com , Amazon.de SanDisk 64 GB SD Card Class 10 Amazon.com , Amazon.de Robot Smart Chassis Kit Amazon.com , Amazon.de SLAMTEC RPLidar A2M8 (12 m) Amazon.com , Amazon.de Grove Ultrasonic Ranger Amazon.com , Amazon.de Raspi Camera Module V2, 8 MP, 1080p Amazon.com , Amazon.de Grove Motor Driver seeedstudio.com , Amazon.de I2C Hub seeedstudio.com , Amazon.de Additional (Optional) Equipment Part Store PicoScope 3000 Series Oscilloscope 2CH Amazon.de VOLTCRAFT PPS-16005 Amazon.de Board - Raspberry Pi 4 B The main processing unit of the robot is a Raspberry Pi 4 B with 4 GB of RAM. Raspberry Pi 4 B - 4 GB RAM variant. Accessories and Electronics Case and Cooling To protect the Rasbperry Pi 4 B we choose a case that provides access to all its ports. The following images show a stackable acrylic case in rainbow colors. Stackable Rainbow Case for Raspberry Pi 4 B. With this case it is possible to install four heatsinks and apply a fan as cooling equipment for the electronics of the Raspberry Pi 4 B such as its ARM processor. Heatsinks and cooling fan for Raspberry Pi 4 B. SD Card The Raspberry Pi requires a medium to boot from. For this we will use a micro sd card because it is lightweight and easy to flash new operating systems. SanDisk Micro SD Card Class 10. Although a micro sd card won't last that long compared to an hard disk drive (HDD) or solid state disk (SSD) it is well suited for testing. Because sd cards are slower when reading and writing data you should make sure to choose a micro sd card with high performance ratings. For the Raspberry Pi a Class 10 micro sd card is recommended. Regarding speed, the Pi has a limited bus speed of approximately 20 MB/s ( source ) Robot Base The Robot Car Kit 2WD from Joy-IT (article no.: robot05) is used as the base for the autonomous mobile robot. Parts of the 2WD Robot Car Kit 05 from Joy-IT. The acrylic chassis has many holes which allow to mount a mounting plate that can hold different development boards. It allows also to mount a Raspberry Pi 4 B, which will be used in this project. Two wheels, hence 2WD, are included in the kit which can be attached to the motors that are provided too. A third caster wheel is provided which allows the robot to spin on the spot. This means the robot can be described by a holonomic model. The motors operate in a range between 3 to 6 Volts DC and make it possible to mount a punched disk for speed measurements. With that punched disk and additional speed sensors it is possible to implement odometry in ROS. To power the motors a battery compartment is available together with a switch to turn the robot on or off. Power Supplies As mentioned the robot will be equipped with a 5V/2.1A USB-C powerbank to supply the Raspberry Pi 4 B with 5 V. Power bank with 10.000 mAh from Intenso. To power the motors the provided battery compartment will be used, which holds four AA batteries \\(4 \\cdot 1.5\\text{V} = 6\\text{V}\\) . I2C Hub The Raspberry Pi provides just two I2C ports, which is why we will use a I2C hub. With the four port I2C hub from Grove it is possible to connect three I2C devices to a single I2C port of the Raspberry Pi Grove I2C Hub. Breadboard and GPIO Extension Cable Optional but helpful for testing is a breadboard and a GPIO extension cable suitable for the Raspberry Pi 4 B. Breadboard with GPIO extension cable. Sensors Sensors are used to sense the environment and to collect information of the current state. For this 2WD robot the sensors are categorized into perception and localization which are explained in the following two sections. Perception Perception sensors of the 2WD robot will be used to avoid collisions using ultrasonic rangers. Another use case is to detect and follow objects using a camera. Ultrasonic Ranger To avoid obstacles the robot will carry a Grove - Ultrasonic Ranger at the front. Grove Ultrasonic Ranger for obstacle avoidance. It is a non-contact distance measurement module which works at 40KHz and can be interfaced via a single GPIO . For example physical pin 11 of the Raspberry Pi connected to the SIG pin on the sensor can provide the PWM communication. Parameter Value/Range Operating voltage 3.2~5.2V Operating current 8mA Ultrasonic frequency 40kHz Measuring range 2-350cm Resolution 1cm Output PWM Size 50mm X 25mm X 16mm Weight 13g Measurement angle 15 degree Working temperature -10~60 degree C Trigger signal 10uS TTL Echo signal TTL The code that will be used to wrap this sensor as a ROS node can be found in the Grove Raspberry Pi repository on GitHub. As an alternative we could use the HC SR04 . Camera RPi Camera v2. Localization Inertial Measurement Unit An intertial measurement unit (IMU) measures the acceleration and orientation through gyroscopes directly. Other states such as the velocity can then be calculated. For this the Adafruit 9-DOF Absolute Orientation IMU Fusion Breakout - BNO055 is used. 9-DOF Absolute Orientation IMU Fusion Breakout - BNO055 from Adafruit. Odometry For the used odometry sensor see the section below Motor and Wheel Encoder Alternative Optical Sensor To estimate the change in position over time ( odometry ) the robot will utilize an optical speed sensor . Specifically the Joy-IT Speed Sensor which combines a LM393 ( datasheet ) comperator and a H206 slot-type opto interrupter. LM393 Speed Sensor from Joy-IT. Technical Specifications: Dimensions: 32 x 14 x 7mm Operating voltage: 3.3V to 5V (we will use 3.3V) Two outputs: Digital (D0) and Analog (A0) References: https://dronebotworkshop.com/robot-car-with-speed-sensors/ Actuators Grove - I2C Motor Driver V1.3 Control To drive the two motors of the car kit we use the Grove - I2C Motor Driver V1.3 from Seeed Studio. Grove - I2C Motor Driver. Motor and Wheel Encoder The DG01E-E Hobby Motor has a quadrature encoder built in, which makes it easy to assemble the robot and saves space because of no additional (optical or magnetic) wheel encoders. DG01D-E Motor with wheel encoders. Alternative Brushed Gear Motor Brushed Gearbox Motor DC Gearbox motor - \"TT Motor\" - 200RPM - 3 to 6VDC. Human Machine Interface (HMI) The human machine interface is the layer between the user and the robot. OLED Display To update the user with status messages the robot has a 0.96 inch oled (organic ligth emitting diode) display. The oled display used is the Grove I2C 0.96 inch OLED display from Seeed Studio. Grove - I2C 0.96 inch OLED Display. The display is connected to the RPi via I2C on the physical pins 27 (scl) and 28 (sda), refere to the pinout . Library","title":"Components"},{"location":"components.html#board-raspberry-pi-4-b","text":"The main processing unit of the robot is a Raspberry Pi 4 B with 4 GB of RAM. Raspberry Pi 4 B - 4 GB RAM variant.","title":"Board - Raspberry Pi 4 B"},{"location":"components.html#accessories-and-electronics","text":"","title":"Accessories and Electronics"},{"location":"components.html#case-and-cooling","text":"To protect the Rasbperry Pi 4 B we choose a case that provides access to all its ports. The following images show a stackable acrylic case in rainbow colors. Stackable Rainbow Case for Raspberry Pi 4 B. With this case it is possible to install four heatsinks and apply a fan as cooling equipment for the electronics of the Raspberry Pi 4 B such as its ARM processor. Heatsinks and cooling fan for Raspberry Pi 4 B.","title":"Case and Cooling"},{"location":"components.html#sd-card","text":"The Raspberry Pi requires a medium to boot from. For this we will use a micro sd card because it is lightweight and easy to flash new operating systems. SanDisk Micro SD Card Class 10. Although a micro sd card won't last that long compared to an hard disk drive (HDD) or solid state disk (SSD) it is well suited for testing. Because sd cards are slower when reading and writing data you should make sure to choose a micro sd card with high performance ratings. For the Raspberry Pi a Class 10 micro sd card is recommended. Regarding speed, the Pi has a limited bus speed of approximately 20 MB/s ( source )","title":"SD Card"},{"location":"components.html#robot-base","text":"The Robot Car Kit 2WD from Joy-IT (article no.: robot05) is used as the base for the autonomous mobile robot. Parts of the 2WD Robot Car Kit 05 from Joy-IT. The acrylic chassis has many holes which allow to mount a mounting plate that can hold different development boards. It allows also to mount a Raspberry Pi 4 B, which will be used in this project. Two wheels, hence 2WD, are included in the kit which can be attached to the motors that are provided too. A third caster wheel is provided which allows the robot to spin on the spot. This means the robot can be described by a holonomic model. The motors operate in a range between 3 to 6 Volts DC and make it possible to mount a punched disk for speed measurements. With that punched disk and additional speed sensors it is possible to implement odometry in ROS. To power the motors a battery compartment is available together with a switch to turn the robot on or off.","title":"Robot Base"},{"location":"components.html#power-supplies","text":"As mentioned the robot will be equipped with a 5V/2.1A USB-C powerbank to supply the Raspberry Pi 4 B with 5 V. Power bank with 10.000 mAh from Intenso. To power the motors the provided battery compartment will be used, which holds four AA batteries \\(4 \\cdot 1.5\\text{V} = 6\\text{V}\\) .","title":"Power Supplies"},{"location":"components.html#i2c-hub","text":"The Raspberry Pi provides just two I2C ports, which is why we will use a I2C hub. With the four port I2C hub from Grove it is possible to connect three I2C devices to a single I2C port of the Raspberry Pi Grove I2C Hub.","title":"I2C Hub"},{"location":"components.html#breadboard-and-gpio-extension-cable","text":"Optional but helpful for testing is a breadboard and a GPIO extension cable suitable for the Raspberry Pi 4 B. Breadboard with GPIO extension cable.","title":"Breadboard and GPIO Extension Cable"},{"location":"components.html#sensors","text":"Sensors are used to sense the environment and to collect information of the current state. For this 2WD robot the sensors are categorized into perception and localization which are explained in the following two sections.","title":"Sensors"},{"location":"components.html#perception","text":"Perception sensors of the 2WD robot will be used to avoid collisions using ultrasonic rangers. Another use case is to detect and follow objects using a camera.","title":"Perception"},{"location":"components.html#ultrasonic-ranger","text":"To avoid obstacles the robot will carry a Grove - Ultrasonic Ranger at the front. Grove Ultrasonic Ranger for obstacle avoidance. It is a non-contact distance measurement module which works at 40KHz and can be interfaced via a single GPIO . For example physical pin 11 of the Raspberry Pi connected to the SIG pin on the sensor can provide the PWM communication. Parameter Value/Range Operating voltage 3.2~5.2V Operating current 8mA Ultrasonic frequency 40kHz Measuring range 2-350cm Resolution 1cm Output PWM Size 50mm X 25mm X 16mm Weight 13g Measurement angle 15 degree Working temperature -10~60 degree C Trigger signal 10uS TTL Echo signal TTL The code that will be used to wrap this sensor as a ROS node can be found in the Grove Raspberry Pi repository on GitHub. As an alternative we could use the HC SR04 .","title":"Ultrasonic Ranger"},{"location":"components.html#camera","text":"RPi Camera v2.","title":"Camera"},{"location":"components.html#localization","text":"","title":"Localization"},{"location":"components.html#inertial-measurement-unit","text":"An intertial measurement unit (IMU) measures the acceleration and orientation through gyroscopes directly. Other states such as the velocity can then be calculated. For this the Adafruit 9-DOF Absolute Orientation IMU Fusion Breakout - BNO055 is used. 9-DOF Absolute Orientation IMU Fusion Breakout - BNO055 from Adafruit.","title":"Inertial Measurement Unit"},{"location":"components.html#odometry","text":"For the used odometry sensor see the section below Motor and Wheel Encoder Alternative Optical Sensor To estimate the change in position over time ( odometry ) the robot will utilize an optical speed sensor . Specifically the Joy-IT Speed Sensor which combines a LM393 ( datasheet ) comperator and a H206 slot-type opto interrupter. LM393 Speed Sensor from Joy-IT. Technical Specifications: Dimensions: 32 x 14 x 7mm Operating voltage: 3.3V to 5V (we will use 3.3V) Two outputs: Digital (D0) and Analog (A0) References: https://dronebotworkshop.com/robot-car-with-speed-sensors/","title":"Odometry"},{"location":"components.html#actuators","text":"Grove - I2C Motor Driver V1.3","title":"Actuators"},{"location":"components.html#control","text":"To drive the two motors of the car kit we use the Grove - I2C Motor Driver V1.3 from Seeed Studio. Grove - I2C Motor Driver.","title":"Control"},{"location":"components.html#motor-and-wheel-encoder","text":"The DG01E-E Hobby Motor has a quadrature encoder built in, which makes it easy to assemble the robot and saves space because of no additional (optical or magnetic) wheel encoders. DG01D-E Motor with wheel encoders. Alternative Brushed Gear Motor","title":"Motor and Wheel Encoder"},{"location":"components.html#brushed-gearbox-motor","text":"DC Gearbox motor - \"TT Motor\" - 200RPM - 3 to 6VDC.","title":"Brushed Gearbox Motor"},{"location":"components.html#human-machine-interface-hmi","text":"The human machine interface is the layer between the user and the robot.","title":"Human Machine Interface (HMI)"},{"location":"components.html#oled-display","text":"To update the user with status messages the robot has a 0.96 inch oled (organic ligth emitting diode) display. The oled display used is the Grove I2C 0.96 inch OLED display from Seeed Studio. Grove - I2C 0.96 inch OLED Display. The display is connected to the RPi via I2C on the physical pins 27 (scl) and 28 (sda), refere to the pinout . Library","title":"OLED Display"},{"location":"diffbot_base.html","text":"DiffBot Base Package This package contains the so called hardware interface of DiffBot which represents the real hardware in software to work with ROS Control . ROS Control Overview. In the simpleste case all that is needed in this package is to write a class that inherits from hardware_interface::RobotHW and provide a launch file. The launch file will Load the robot description from diffbot_description to the paramter server Run the hardware interface of this package diffbot_base Load the controller configuration yaml from the diffbot_control package to the parameter server Load the controllers with the controller manager diffbot_base Package The diffbot_base package is created with catkin-tools : 1 2 3 4 5 6 7 fjp@diffbot:/home/fjp/catkin_ws/src$ catkin create pkg diffbot_base --catkin-deps diff_drive_controller hardware_interface roscpp sensor_msgs rosparam_shortcuts Creating package \"diffbot_base\" in \"/home/fjp/catkin_ws/src\"... Created file diffbot_base/package.xml Created file diffbot_base/CMakeLists.txt Created folder diffbot_base/include/diffbot_base Created folder diffbot_base/src Successfully created package files in /home/fjp/catkin_ws/src/diffbot_base. To work with this package the specified dependencies must be installed either using the available Ubuntu/Debian packages for ROS Noetic or they have to be built from source first. The following table lists the dependencies that we have to install because they are not already part of the ROS Noetic desktop full installation. Refer to the section ROS Noetic Setup for how this was done. Dependency Source Ubuntu/Debian Package rosparam_shortcuts https://github.com/PickNikRobotics/rosparam_shortcuts ros-noetic-rosparam-shortcuts hardware_interface https://github.com/ros-controls/ros_control ros-noetic-ros-control diff_drive_controller https://github.com/ros-controls/ros_controllers ros-noetic-ros-controllers To install a package from source clone (using git) or download the source files from where they are located (commonly hosted on GitHub) into the src folder of a ros catkin workspace and execute the catkin build command. Also make sure to source the workspace after building new packages with source devel/setup.bash . 1 2 3 4 cd /homw/fjp/git/diffbot/ros/ # Navigate to the workspace catkin build # Build all the packages in the workspace ls build # Show the resulting build space ls devel # Show the resulting devel space Make sure to clone/download the source files suitable for the ROS distribtion you are using. If the sources are not available for the distribution you are working with, it is worth to try building anyway. Chances are that the package you want to use is suitable for multiple ROS distros. For example if a package states in its docs, that it is only available for kinetic it is possible that it will work with a ROS noetic install. Hardware Interface See the include and src folders of this package and the details on the hardware interface implementation. For more details on the hardware interface also refer to the section ROS Integration: Control , it gives more details and also this overview article about ROS Control . The hardware interface provides an interface between the real robot hardware and the controllers provided by ROS Control (or even custom controllers). DiffBot works with the diff_drive_controller that is configured in the diffbot_control package, which is also relevant for the simulation in Gazebo. Remember that the simulation uses the gazebo_ros_control package to communicate with the diff_drive_controller . For the real robot hardware, ROS Control uses an instance of type hardware_interface::RobotHW that is passed to the controller_manager to handle the resources, meaning that the actuated robot joints are not in use by multiple controllers that might be loaded. The skeleton of DiffBot's hardware interface looks like following, where the constructor is used to read loaded configuration values from the robot's description from the ROS parameter server: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 namespace diffbot_base { DiffBotHWInterface :: DiffBotHWInterface ( ros :: NodeHandle & nh , urdf :: Model * urdf_model ) : name_ ( \"hardware_interface\" ) , nh_ ( nh ) { // Initialization of the robot's resources (joints, sensors, actuators) and // interfaces can be done here or inside init(). // E.g. parse the URDF for joint names & interfaces, then initialize them // Check if the URDF model needs to be loaded if ( urdf_model == NULL ) loadURDF ( nh , \"robot_description\" ); else urdf_model_ = urdf_model ; // Load rosparams ros :: NodeHandle rpnh ( nh_ , name_ ); std :: size_t error = 0 ; // Code API of rosparam_shortcuts: // http://docs.ros.org/en/noetic/api/rosparam_shortcuts/html/namespacerosparam__shortcuts.html#aa6536fe0130903960b1de4872df68d5d error += ! rosparam_shortcuts :: get ( name_ , rpnh , \"joints\" , joint_names_ ); error += ! rosparam_shortcuts :: get ( name_ , nh_ , \"mobile_base_controller/wheel_radius\" , wheel_radius_ ); error += ! rosparam_shortcuts :: get ( name_ , nh_ , \"mobile_base_controller/linear/x/max_velocity\" , max_velocity_ ); rosparam_shortcuts :: shutdownIfError ( name_ , error ); wheel_diameter_ = 2.0 * wheel_radius_ ; //max_velocity_ = 0.2; // m/s // ros_control RobotHW needs velocity in rad/s but in the config its given in m/s max_velocity_ = linearToAngular ( max_velocity_ ); // Setup publisher for the motor driver pub_left_motor_value_ = nh_ . advertise < std_msgs :: Int32 > ( \"motor_left\" , 1 ); pub_right_motor_value_ = nh_ . advertise < std_msgs :: Int32 > ( \"motor_right\" , 1 ); // Setup subscriber for the wheel encoders sub_left_encoder_ticks_ = nh_ . subscribe ( \"ticks_left\" , 1 , & DiffBotHWInterface :: leftEncoderTicksCallback , this ); sub_right_encoder_ticks_ = nh_ . subscribe ( \"ticks_right\" , 1 , & DiffBotHWInterface :: rightEncoderTicksCallback , this ); // Initialize the hardware interface init ( nh_ , nh_ ); } bool DiffBotHWInterface :: init ( ros :: NodeHandle & root_nh , ros :: NodeHandle & robot_hw_nh ) { ROS_INFO ( \"Initializing DiffBot Hardware Interface ...\" ); num_joints_ = joint_names_ . size (); ROS_INFO ( \"Number of joints: %d\" , ( int ) num_joints_ ); std :: array < std :: string , NUM_JOINTS > motor_names = { \"left_motor\" , \"right_motor\" }; for ( unsigned int i = 0 ; i < num_joints_ ; i ++ ) { // Create a JointStateHandle for each joint and register them with the // JointStateInterface. hardware_interface :: JointStateHandle joint_state_handle ( joint_names_ [ i ], & joint_positions_ [ i ], & joint_velocities_ [ i ], & joint_efforts_ [ i ]); joint_state_interface_ . registerHandle ( joint_state_handle ); // Create a JointHandle (read and write) for each controllable joint // using the read-only joint handles within the JointStateInterface and // register them with the JointVelocityInterface. hardware_interface :: JointHandle joint_handle ( joint_state_handle , & joint_velocity_commands_ [ i ]); velocity_joint_interface_ . registerHandle ( joint_handle ); // Initialize joint states with zero values joint_positions_ [ i ] = 0.0 ; joint_velocities_ [ i ] = 0.0 ; joint_efforts_ [ i ] = 0.0 ; // unused with diff_drive_controller joint_velocity_commands_ [ i ] = 0.0 ; // Initialize the pid controllers for the motors using the robot namespace std :: string pid_namespace = \"pid/\" + motor_names [ i ]; ROS_INFO_STREAM ( \"pid namespace: \" << pid_namespace ); ros :: NodeHandle nh ( root_nh , pid_namespace ); // TODO implement builder pattern to initialize values otherwise it is hard to see which parameter is what. pids_ [ i ]. init ( nh , 0.0 , 10.0 , 1.0 , 1.0 , 0.0 , 0.0 , false , - max_velocity_ , max_velocity_ ); pids_ [ i ]. setOutputLimits ( - max_velocity_ , max_velocity_ ); } // Register the JointStateInterface containing the read only joints // with this robot's hardware_interface::RobotHW. registerInterface ( & joint_state_interface_ ); // Register the JointVelocityInterface containing the read/write joints // with this robot's hardware_interface::RobotHW. registerInterface ( & velocity_joint_interface_ ); ROS_INFO ( \"... Done Initializing DiffBot Hardware Interface\" ); return true ; } // The read method is part of the control loop cycle (read, update, write) and is used to // populate the robot state from the robot's hardware resources (joints, sensors, actuators). // This method should be called before controller_manager::ControllerManager::update() and write. void DiffBotHWInterface :: read ( const ros :: Time & time , const ros :: Duration & period ) { ros :: Duration elapsed_time = period ; // Read from robot hw (motor encoders) // Fill joint_state_* members with read values double wheel_angles [ 2 ]; double wheel_angle_deltas [ 2 ]; for ( std :: size_t i = 0 ; i < num_joints_ ; ++ i ) { wheel_angles [ i ] = ticksToAngle ( encoder_ticks_ [ i ]); //double wheel_angle_normalized = normalizeAngle(wheel_angle); wheel_angle_deltas [ i ] = wheel_angles [ i ] - joint_positions_ [ i ]; joint_positions_ [ i ] += wheel_angle_deltas [ i ]; joint_velocities_ [ i ] = wheel_angle_deltas [ i ] / period . toSec (); joint_efforts_ [ i ] = 0.0 ; // unused with diff_drive_controller } } // The write method is part of the control loop cycle (read, update, write) and is used to // send out commands to the robot's hardware resources (joints, actuators). // This method should be called after read and controller_manager::ControllerManager::update. void DiffBotHWInterface :: write ( const ros :: Time & time , const ros :: Duration & period ) { ros :: Duration elapsed_time = period ; // Write to robot hw // joint velocity commands from ros_control's RobotHW are in rad/s // Convert the velocity command to a percentage value for the motor // This maps the velocity to a percentage value which is used to apply // a percentage of the highest possible battery voltage to each motor. std_msgs :: Int32 left_motor ; std_msgs :: Int32 right_motor ; double output_left = pids_ [ 0 ]( joint_velocities_ [ 0 ], joint_velocity_commands_ [ 0 ], period ); double output_right = pids_ [ 1 ]( joint_velocities_ [ 1 ], joint_velocity_commands_ [ 1 ], period ); left_motor . data = output_left / max_velocity_ * 100.0 ; right_motor . data = output_right / max_velocity_ * 100.0 ; // Publish the PID computed motor commands to the left and right motors pub_left_motor_value_ . publish ( left_motor ); pub_right_motor_value_ . publish ( right_motor ); } // Process updates from encoders using a subscriber void DiffBotHWInterface :: leftEncoderTicksCallback ( const std_msgs :: Int32 :: ConstPtr & msg ) { encoder_ticks_ [ 0 ] = msg -> data ; ROS_DEBUG_STREAM_THROTTLE ( 1 , \"Left encoder ticks: \" << msg -> data ); } void DiffBotHWInterface :: rightEncoderTicksCallback ( const std_msgs :: Int32 :: ConstPtr & msg ) { encoder_ticks_ [ 1 ] = msg -> data ; ROS_DEBUG_STREAM_THROTTLE ( 1 , \"Right encoder ticks: \" << msg -> data ); } double DiffBotHWInterface :: ticksToAngle ( const int & ticks ) const { // Convert number of encoder ticks to angle in radians double angle = ( double ) ticks * ( 2.0 * M_PI / 542.0 ); ROS_DEBUG_STREAM_THROTTLE ( 1 , ticks << \" ticks correspond to an angle of \" << angle ); return angle ; } }; The functions above are designed to give the controller manager (and the controllers inside the controller manager) access to the joint state of custom robot, and to command it. When the controller manager runs, the controllers will read from the pos, vel and eff variables of the custom robot hardware interface, and the controller will write the desired command into the cmd variable. It's mandatory to make sure the pos, vel and eff variables always have the latest joint state available, and to make sure that whatever is written into the cmd variable gets executed by the robot. This can be done by implementing hardware_interface::RobotHW::read() and a hardware_interface::RobotHW::write() methods. The main node that will be executed uses the controller_manager to operate the so called control loop. In the case of DiffBot a simple example looks like the following, refer to the diffbot_base.cpp for the complete implementation: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 #include <ros/ros.h> #include <diffbot_base/diffbot_hw_interface.h> #include <controller_manager/controller_manager.h> int main ( int argc , char ** argv ) { // Initialize the ROS node ros :: init ( argc , argv , \"diffbot_hw_interface\" ); ros :: NodeHandle nh ; // Create an instance of your robot so that this instance knows about all // the resources that are available. diffbot_base :: DiffBotHWInterface diffBot ( nh ); // Create an instance of the controller manager and pass it the robot, // so that it can handle its resources. controller_manager :: ControllerManager cm ( & diffBot ); // Setup a separate thread that will be used to service ROS callbacks. // NOTE: We run the ROS loop in a separate thread as external calls such // as service callbacks to load controllers can block the (main) control loop ros :: AsyncSpinner spinner ( 1 ); spinner . start (); // Setup for the control loop. ros :: Time prev_time = ros :: Time :: now (); ros :: Rate rate ( 10.0 ); // 10 Hz rate // Blocks until shutdown signal recieved while ( ros :: ok ()) { // Basic bookkeeping to get the system time in order to compute the control period. const ros :: Time time = ros :: Time :: now (); const ros :: Duration period = time - prev_time ; // Execution of the actual control loop. diffBot . read ( time , period ); // If needed, its possible to define transmissions in software by calling the // transmission_interface::ActuatorToJointPositionInterface::propagate() // after reading the joint states. cm . update ( time , period ); // In case of software transmissions, use // transmission_interface::JointToActuatorEffortHandle::propagate() // to convert from the joint space to the actuator space. diffBot . write ( time , period ); // All these steps keep getting repeated with the specified rate. rate . sleep (); } return 0 ; } As we can see, the basic steps are to initialize the node, instantiate the hardware interface, pass it to a new controller manager and run the control loop that does the following: Read joint states from the real robot hardware Update the diff_drive_controller with read values and compute the joint velocities using the target cmd_vel Write the computed values You may be wondering why the read values aren't returned from the diffbot.read() method and nothing is passed to the diffbot.write() . This is because the RobotHW::init() method, shown in the first code snippet, is used to register the actuated joint names (described in the diffbot_description ) to the joint_position , joint_velocity and joint_effort member variables of the custom robot hardware interface. The class that registers the variables of the controller with the hardware interface member variables and thereby gives read access to all joint values without conflicting with other controllers, is the hardware_interface::JointStateInterface . ROS Control uses the hardware_interface::VelocityJointInterface (part of the joint_command_interface.h ) that registers the command member variable of the controller with the hardware interface to provide it the command that should be written to the actuators. When the controller manager runs, the controllers will read from the joint_position , joint_velocity and joint_effort variables of the custom robot hardware interface, and the controller will write the desired command into the joint_velocity_command variable. It's mandatory to make sure the position, velocity and effort (effort is not needed in the case of the diff_drive_controller ) variables always have the latest joint state available, and to make sure that whatever is written into the joint_velocity_command variable gets executed by the robot. As mentioned this can be done by implementing hardware_interface::RobotHW::read() and a hardware_interface::RobotHW::write() methods. In the control loop the overriden hardware_interface::RobotHW::read() method of DiffBot is used to read the joint states. The diff_drive_controller works with a VelocityInterface which is why the joint_position , defined in rad, and joint_velocity , defined in rad/s, are calculated from the encoder ticks. PID Controller Note the two PID controllers inside the hardware interface, where each PID is passed the error between velocity measured by the encoders and the target velocity computed by the diff_drive_controller for a specific wheel joint. The diff_drive_controller doesn't have a PID controller integrated, and doesn't take care if the wheels of the robot are actually turning. As mentioned above, ROS Control expects that the commands sent by the controller are actually implemented on the real robot hardware and that the joint states are always up to date. This means that the diff_drive_controller just uses the twist_msg on the cmd_vel topic for example from the rqt_robot_steering and converts it to a velocity command for the motors. It doesn't take the actual velocity of the motors into account. See the code of diff_drive_controller where the joint_command_velocity is calculated. This is why a PID controller is needed to avoid situations like the following where the robot moves not straigth although it is commanded to do so: The PID used here inherits from the ROS Control control_toolbox::Pid that provides Dynamic Reconfigure out of the box to tune the proportional, integral and derivative gains. The behaviour when using only the P, I and D gains is that the output can overshoot and even change between positive and negative motor percent values because of a P gain that is too high. To avoid this, a feed forward gain can help to reach the setpoint faster. To add this feed forward gain to the dynamic reconfigure parameters it is necessary to add a new parameter configuration file in this package inside a cfg folder. For more details on ROS dynamic reconfigure see the official tutorials . With the use of the PID controller the robot is able to drive straight: In case of using inexpensive motors like the DG01D-E of DiffBot, you have to take inaccurate driving behaviour into account. The straight driving behaviour can be improved with motors that start spinning at the same voltage levels. To find suitable motors do a voltage sweep test by slightly increasing the voltage and note the voltage level where each motor starts to rotate. Such a test was done on DiffBot's motors. Using six DG01D-E motors the following values were recorded (sorted by increasing voltage): Motor Voltage (V) 01 2.5 02 2.8 - 3.0 03 3.1 04 3.2 05 3.2 06 3.3 In the videos above, motors numbered 01 and 03 were used coincidencely and I wasn't aware of the remarkable differences in voltage levels. Using the motors 04 and 05 improved the driving behaviour significantly. To deal with significant differences in the motors it would also help to tune the two PIDs individually, which is not shown in the video above . Make also sure that the motor driver outputs the same voltage level on both channels when the robot is commanded to move straight. The used Grove i2c motor driver was tested to do this. Another problem of not driving straight can be weight distribution or the orientation of the caster wheel. A good test to check the accuracy is to fix two meters of adhesive tape on the floor in a straight line. Then, place the robot on one end oriented in the direction to the other end. Now command it to move straight along the line and stop it when it reaches the end of the tape. Record the lateral displacement from the tape. Measuring a value below 10 cm is considered precise for these motors. Launch File To run a single controller_manager, the one from the diffbot_base package defined inside difbot_base.cpp use the launch file from diffbot_base/launch/diffbot.launch : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 <!-- https://github.com/ros-controls/ros_controllers/tree/kinetic-devel/diff_drive_controller/test --> <launch> <!-- Load DiffBot model --> <param name= \"robot_description\" command= \"$(find xacro)/xacro '$(find diffbot_description)/urdf/diffbot.xacro'\" /> <node name= \"diffbot_base\" pkg= \"diffbot_base\" type= \"diffbot_base\" /> <!-- Load controller config to the parameter server --> <rosparam command= \"load\" file= \"$(find diffbot_control)/config/diffbot_control.yaml\" /> <!-- Load the controllers --> <node name= \"controller_spawner\" pkg= \"controller_manager\" type= \"spawner\" respawn= \"false\" output= \"screen\" ns= \"diffbot\" args= \"joint_state_controller mobile_base_controller\" /> </launch> This will load the DiffBot robot description onto the parameter server which is required for the hardware interface that gets created inside the next node diffbot_base . It creates the hardware interface and instantiates a new controller manager in the diffbot_base.cpp . Finally the spawner from the controller_manager package is used to initialize and start the controllers defined in the diffbot_control/config/diffbot_control.yaml . The last step in this launch file is required to get the controllers initialized and started. Another way would be to use controller_manager::ControllerManager::loadControllers() inside the diffbot_base.cpp . After launching this launch file on DiffBot (Raspberry Pi) with 1 roslaunch diffbot_base diffbot.launch the following parameters are stored on the parameter server: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 $ rosparam list /diffbot/hardware_interface/joints /diffbot/joint_state_controller/extra_joints /diffbot/joint_state_controller/publish_rate /diffbot/joint_state_controller/type /diffbot/mobile_base_controller/base_frame_id /diffbot/mobile_base_controller/left_wheel /diffbot/mobile_base_controller/pose_covariance_diagonal /diffbot/mobile_base_controller/publish_rate /diffbot/mobile_base_controller/right_wheel /diffbot/mobile_base_controller/twist_covariance_diagonal /diffbot/mobile_base_controller/type /diffbot/mobile_base_controller/wheel_radius /diffbot/mobile_base_controller/wheel_separation /robot_description /rosdistro /roslaunch/uris/host_tensorbook__46157 /roslaunch/uris/host_ubuntu__33729 /rosversion /run_id Additional Requirements Because the hardware interface subscribes to the encoders, that are connected to the Teensy MCU, and publishes to the motors via the motr driver node, another launch will be required to run these additional nodes. See the diffbot_bringup package for this setup. Simulation To have a simulation showing DiffBot, the second step is to use the diffbot_gazebo/launch/diffbot_base.launch on the work pc: 1 $ roslaunch diffbot_gazebo diffbot_base.launch This will launch the gazebo simulation, which can make use of the running controllers inside the controller manager too: ROS Control with Gazebo Overview. After launching the Gazebo simulation the controllers got uninitialized. (It is assumed that the gazebo_ros_control plugin that gets launched). Because of this the controllers have to be initialized and started again. For this the diffbot_base/launch/controllers.launch should be used. This launch file is just loading and starting all controllers again. Note that using the spawner from the controller_manager package, like in the diffbot_base/launch/diffbot.launch results in an error. (TODO this needs some more testing).","title":"Base Hardware Interface"},{"location":"diffbot_base.html#diffbot-base-package","text":"This package contains the so called hardware interface of DiffBot which represents the real hardware in software to work with ROS Control . ROS Control Overview. In the simpleste case all that is needed in this package is to write a class that inherits from hardware_interface::RobotHW and provide a launch file. The launch file will Load the robot description from diffbot_description to the paramter server Run the hardware interface of this package diffbot_base Load the controller configuration yaml from the diffbot_control package to the parameter server Load the controllers with the controller manager","title":"DiffBot Base Package"},{"location":"diffbot_base.html#diffbot_base-package","text":"The diffbot_base package is created with catkin-tools : 1 2 3 4 5 6 7 fjp@diffbot:/home/fjp/catkin_ws/src$ catkin create pkg diffbot_base --catkin-deps diff_drive_controller hardware_interface roscpp sensor_msgs rosparam_shortcuts Creating package \"diffbot_base\" in \"/home/fjp/catkin_ws/src\"... Created file diffbot_base/package.xml Created file diffbot_base/CMakeLists.txt Created folder diffbot_base/include/diffbot_base Created folder diffbot_base/src Successfully created package files in /home/fjp/catkin_ws/src/diffbot_base. To work with this package the specified dependencies must be installed either using the available Ubuntu/Debian packages for ROS Noetic or they have to be built from source first. The following table lists the dependencies that we have to install because they are not already part of the ROS Noetic desktop full installation. Refer to the section ROS Noetic Setup for how this was done. Dependency Source Ubuntu/Debian Package rosparam_shortcuts https://github.com/PickNikRobotics/rosparam_shortcuts ros-noetic-rosparam-shortcuts hardware_interface https://github.com/ros-controls/ros_control ros-noetic-ros-control diff_drive_controller https://github.com/ros-controls/ros_controllers ros-noetic-ros-controllers To install a package from source clone (using git) or download the source files from where they are located (commonly hosted on GitHub) into the src folder of a ros catkin workspace and execute the catkin build command. Also make sure to source the workspace after building new packages with source devel/setup.bash . 1 2 3 4 cd /homw/fjp/git/diffbot/ros/ # Navigate to the workspace catkin build # Build all the packages in the workspace ls build # Show the resulting build space ls devel # Show the resulting devel space Make sure to clone/download the source files suitable for the ROS distribtion you are using. If the sources are not available for the distribution you are working with, it is worth to try building anyway. Chances are that the package you want to use is suitable for multiple ROS distros. For example if a package states in its docs, that it is only available for kinetic it is possible that it will work with a ROS noetic install.","title":"diffbot_base Package"},{"location":"diffbot_base.html#hardware-interface","text":"See the include and src folders of this package and the details on the hardware interface implementation. For more details on the hardware interface also refer to the section ROS Integration: Control , it gives more details and also this overview article about ROS Control . The hardware interface provides an interface between the real robot hardware and the controllers provided by ROS Control (or even custom controllers). DiffBot works with the diff_drive_controller that is configured in the diffbot_control package, which is also relevant for the simulation in Gazebo. Remember that the simulation uses the gazebo_ros_control package to communicate with the diff_drive_controller . For the real robot hardware, ROS Control uses an instance of type hardware_interface::RobotHW that is passed to the controller_manager to handle the resources, meaning that the actuated robot joints are not in use by multiple controllers that might be loaded. The skeleton of DiffBot's hardware interface looks like following, where the constructor is used to read loaded configuration values from the robot's description from the ROS parameter server: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 namespace diffbot_base { DiffBotHWInterface :: DiffBotHWInterface ( ros :: NodeHandle & nh , urdf :: Model * urdf_model ) : name_ ( \"hardware_interface\" ) , nh_ ( nh ) { // Initialization of the robot's resources (joints, sensors, actuators) and // interfaces can be done here or inside init(). // E.g. parse the URDF for joint names & interfaces, then initialize them // Check if the URDF model needs to be loaded if ( urdf_model == NULL ) loadURDF ( nh , \"robot_description\" ); else urdf_model_ = urdf_model ; // Load rosparams ros :: NodeHandle rpnh ( nh_ , name_ ); std :: size_t error = 0 ; // Code API of rosparam_shortcuts: // http://docs.ros.org/en/noetic/api/rosparam_shortcuts/html/namespacerosparam__shortcuts.html#aa6536fe0130903960b1de4872df68d5d error += ! rosparam_shortcuts :: get ( name_ , rpnh , \"joints\" , joint_names_ ); error += ! rosparam_shortcuts :: get ( name_ , nh_ , \"mobile_base_controller/wheel_radius\" , wheel_radius_ ); error += ! rosparam_shortcuts :: get ( name_ , nh_ , \"mobile_base_controller/linear/x/max_velocity\" , max_velocity_ ); rosparam_shortcuts :: shutdownIfError ( name_ , error ); wheel_diameter_ = 2.0 * wheel_radius_ ; //max_velocity_ = 0.2; // m/s // ros_control RobotHW needs velocity in rad/s but in the config its given in m/s max_velocity_ = linearToAngular ( max_velocity_ ); // Setup publisher for the motor driver pub_left_motor_value_ = nh_ . advertise < std_msgs :: Int32 > ( \"motor_left\" , 1 ); pub_right_motor_value_ = nh_ . advertise < std_msgs :: Int32 > ( \"motor_right\" , 1 ); // Setup subscriber for the wheel encoders sub_left_encoder_ticks_ = nh_ . subscribe ( \"ticks_left\" , 1 , & DiffBotHWInterface :: leftEncoderTicksCallback , this ); sub_right_encoder_ticks_ = nh_ . subscribe ( \"ticks_right\" , 1 , & DiffBotHWInterface :: rightEncoderTicksCallback , this ); // Initialize the hardware interface init ( nh_ , nh_ ); } bool DiffBotHWInterface :: init ( ros :: NodeHandle & root_nh , ros :: NodeHandle & robot_hw_nh ) { ROS_INFO ( \"Initializing DiffBot Hardware Interface ...\" ); num_joints_ = joint_names_ . size (); ROS_INFO ( \"Number of joints: %d\" , ( int ) num_joints_ ); std :: array < std :: string , NUM_JOINTS > motor_names = { \"left_motor\" , \"right_motor\" }; for ( unsigned int i = 0 ; i < num_joints_ ; i ++ ) { // Create a JointStateHandle for each joint and register them with the // JointStateInterface. hardware_interface :: JointStateHandle joint_state_handle ( joint_names_ [ i ], & joint_positions_ [ i ], & joint_velocities_ [ i ], & joint_efforts_ [ i ]); joint_state_interface_ . registerHandle ( joint_state_handle ); // Create a JointHandle (read and write) for each controllable joint // using the read-only joint handles within the JointStateInterface and // register them with the JointVelocityInterface. hardware_interface :: JointHandle joint_handle ( joint_state_handle , & joint_velocity_commands_ [ i ]); velocity_joint_interface_ . registerHandle ( joint_handle ); // Initialize joint states with zero values joint_positions_ [ i ] = 0.0 ; joint_velocities_ [ i ] = 0.0 ; joint_efforts_ [ i ] = 0.0 ; // unused with diff_drive_controller joint_velocity_commands_ [ i ] = 0.0 ; // Initialize the pid controllers for the motors using the robot namespace std :: string pid_namespace = \"pid/\" + motor_names [ i ]; ROS_INFO_STREAM ( \"pid namespace: \" << pid_namespace ); ros :: NodeHandle nh ( root_nh , pid_namespace ); // TODO implement builder pattern to initialize values otherwise it is hard to see which parameter is what. pids_ [ i ]. init ( nh , 0.0 , 10.0 , 1.0 , 1.0 , 0.0 , 0.0 , false , - max_velocity_ , max_velocity_ ); pids_ [ i ]. setOutputLimits ( - max_velocity_ , max_velocity_ ); } // Register the JointStateInterface containing the read only joints // with this robot's hardware_interface::RobotHW. registerInterface ( & joint_state_interface_ ); // Register the JointVelocityInterface containing the read/write joints // with this robot's hardware_interface::RobotHW. registerInterface ( & velocity_joint_interface_ ); ROS_INFO ( \"... Done Initializing DiffBot Hardware Interface\" ); return true ; } // The read method is part of the control loop cycle (read, update, write) and is used to // populate the robot state from the robot's hardware resources (joints, sensors, actuators). // This method should be called before controller_manager::ControllerManager::update() and write. void DiffBotHWInterface :: read ( const ros :: Time & time , const ros :: Duration & period ) { ros :: Duration elapsed_time = period ; // Read from robot hw (motor encoders) // Fill joint_state_* members with read values double wheel_angles [ 2 ]; double wheel_angle_deltas [ 2 ]; for ( std :: size_t i = 0 ; i < num_joints_ ; ++ i ) { wheel_angles [ i ] = ticksToAngle ( encoder_ticks_ [ i ]); //double wheel_angle_normalized = normalizeAngle(wheel_angle); wheel_angle_deltas [ i ] = wheel_angles [ i ] - joint_positions_ [ i ]; joint_positions_ [ i ] += wheel_angle_deltas [ i ]; joint_velocities_ [ i ] = wheel_angle_deltas [ i ] / period . toSec (); joint_efforts_ [ i ] = 0.0 ; // unused with diff_drive_controller } } // The write method is part of the control loop cycle (read, update, write) and is used to // send out commands to the robot's hardware resources (joints, actuators). // This method should be called after read and controller_manager::ControllerManager::update. void DiffBotHWInterface :: write ( const ros :: Time & time , const ros :: Duration & period ) { ros :: Duration elapsed_time = period ; // Write to robot hw // joint velocity commands from ros_control's RobotHW are in rad/s // Convert the velocity command to a percentage value for the motor // This maps the velocity to a percentage value which is used to apply // a percentage of the highest possible battery voltage to each motor. std_msgs :: Int32 left_motor ; std_msgs :: Int32 right_motor ; double output_left = pids_ [ 0 ]( joint_velocities_ [ 0 ], joint_velocity_commands_ [ 0 ], period ); double output_right = pids_ [ 1 ]( joint_velocities_ [ 1 ], joint_velocity_commands_ [ 1 ], period ); left_motor . data = output_left / max_velocity_ * 100.0 ; right_motor . data = output_right / max_velocity_ * 100.0 ; // Publish the PID computed motor commands to the left and right motors pub_left_motor_value_ . publish ( left_motor ); pub_right_motor_value_ . publish ( right_motor ); } // Process updates from encoders using a subscriber void DiffBotHWInterface :: leftEncoderTicksCallback ( const std_msgs :: Int32 :: ConstPtr & msg ) { encoder_ticks_ [ 0 ] = msg -> data ; ROS_DEBUG_STREAM_THROTTLE ( 1 , \"Left encoder ticks: \" << msg -> data ); } void DiffBotHWInterface :: rightEncoderTicksCallback ( const std_msgs :: Int32 :: ConstPtr & msg ) { encoder_ticks_ [ 1 ] = msg -> data ; ROS_DEBUG_STREAM_THROTTLE ( 1 , \"Right encoder ticks: \" << msg -> data ); } double DiffBotHWInterface :: ticksToAngle ( const int & ticks ) const { // Convert number of encoder ticks to angle in radians double angle = ( double ) ticks * ( 2.0 * M_PI / 542.0 ); ROS_DEBUG_STREAM_THROTTLE ( 1 , ticks << \" ticks correspond to an angle of \" << angle ); return angle ; } }; The functions above are designed to give the controller manager (and the controllers inside the controller manager) access to the joint state of custom robot, and to command it. When the controller manager runs, the controllers will read from the pos, vel and eff variables of the custom robot hardware interface, and the controller will write the desired command into the cmd variable. It's mandatory to make sure the pos, vel and eff variables always have the latest joint state available, and to make sure that whatever is written into the cmd variable gets executed by the robot. This can be done by implementing hardware_interface::RobotHW::read() and a hardware_interface::RobotHW::write() methods. The main node that will be executed uses the controller_manager to operate the so called control loop. In the case of DiffBot a simple example looks like the following, refer to the diffbot_base.cpp for the complete implementation: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 #include <ros/ros.h> #include <diffbot_base/diffbot_hw_interface.h> #include <controller_manager/controller_manager.h> int main ( int argc , char ** argv ) { // Initialize the ROS node ros :: init ( argc , argv , \"diffbot_hw_interface\" ); ros :: NodeHandle nh ; // Create an instance of your robot so that this instance knows about all // the resources that are available. diffbot_base :: DiffBotHWInterface diffBot ( nh ); // Create an instance of the controller manager and pass it the robot, // so that it can handle its resources. controller_manager :: ControllerManager cm ( & diffBot ); // Setup a separate thread that will be used to service ROS callbacks. // NOTE: We run the ROS loop in a separate thread as external calls such // as service callbacks to load controllers can block the (main) control loop ros :: AsyncSpinner spinner ( 1 ); spinner . start (); // Setup for the control loop. ros :: Time prev_time = ros :: Time :: now (); ros :: Rate rate ( 10.0 ); // 10 Hz rate // Blocks until shutdown signal recieved while ( ros :: ok ()) { // Basic bookkeeping to get the system time in order to compute the control period. const ros :: Time time = ros :: Time :: now (); const ros :: Duration period = time - prev_time ; // Execution of the actual control loop. diffBot . read ( time , period ); // If needed, its possible to define transmissions in software by calling the // transmission_interface::ActuatorToJointPositionInterface::propagate() // after reading the joint states. cm . update ( time , period ); // In case of software transmissions, use // transmission_interface::JointToActuatorEffortHandle::propagate() // to convert from the joint space to the actuator space. diffBot . write ( time , period ); // All these steps keep getting repeated with the specified rate. rate . sleep (); } return 0 ; } As we can see, the basic steps are to initialize the node, instantiate the hardware interface, pass it to a new controller manager and run the control loop that does the following: Read joint states from the real robot hardware Update the diff_drive_controller with read values and compute the joint velocities using the target cmd_vel Write the computed values You may be wondering why the read values aren't returned from the diffbot.read() method and nothing is passed to the diffbot.write() . This is because the RobotHW::init() method, shown in the first code snippet, is used to register the actuated joint names (described in the diffbot_description ) to the joint_position , joint_velocity and joint_effort member variables of the custom robot hardware interface. The class that registers the variables of the controller with the hardware interface member variables and thereby gives read access to all joint values without conflicting with other controllers, is the hardware_interface::JointStateInterface . ROS Control uses the hardware_interface::VelocityJointInterface (part of the joint_command_interface.h ) that registers the command member variable of the controller with the hardware interface to provide it the command that should be written to the actuators. When the controller manager runs, the controllers will read from the joint_position , joint_velocity and joint_effort variables of the custom robot hardware interface, and the controller will write the desired command into the joint_velocity_command variable. It's mandatory to make sure the position, velocity and effort (effort is not needed in the case of the diff_drive_controller ) variables always have the latest joint state available, and to make sure that whatever is written into the joint_velocity_command variable gets executed by the robot. As mentioned this can be done by implementing hardware_interface::RobotHW::read() and a hardware_interface::RobotHW::write() methods. In the control loop the overriden hardware_interface::RobotHW::read() method of DiffBot is used to read the joint states. The diff_drive_controller works with a VelocityInterface which is why the joint_position , defined in rad, and joint_velocity , defined in rad/s, are calculated from the encoder ticks.","title":"Hardware Interface"},{"location":"diffbot_base.html#pid-controller","text":"Note the two PID controllers inside the hardware interface, where each PID is passed the error between velocity measured by the encoders and the target velocity computed by the diff_drive_controller for a specific wheel joint. The diff_drive_controller doesn't have a PID controller integrated, and doesn't take care if the wheels of the robot are actually turning. As mentioned above, ROS Control expects that the commands sent by the controller are actually implemented on the real robot hardware and that the joint states are always up to date. This means that the diff_drive_controller just uses the twist_msg on the cmd_vel topic for example from the rqt_robot_steering and converts it to a velocity command for the motors. It doesn't take the actual velocity of the motors into account. See the code of diff_drive_controller where the joint_command_velocity is calculated. This is why a PID controller is needed to avoid situations like the following where the robot moves not straigth although it is commanded to do so: The PID used here inherits from the ROS Control control_toolbox::Pid that provides Dynamic Reconfigure out of the box to tune the proportional, integral and derivative gains. The behaviour when using only the P, I and D gains is that the output can overshoot and even change between positive and negative motor percent values because of a P gain that is too high. To avoid this, a feed forward gain can help to reach the setpoint faster. To add this feed forward gain to the dynamic reconfigure parameters it is necessary to add a new parameter configuration file in this package inside a cfg folder. For more details on ROS dynamic reconfigure see the official tutorials . With the use of the PID controller the robot is able to drive straight: In case of using inexpensive motors like the DG01D-E of DiffBot, you have to take inaccurate driving behaviour into account. The straight driving behaviour can be improved with motors that start spinning at the same voltage levels. To find suitable motors do a voltage sweep test by slightly increasing the voltage and note the voltage level where each motor starts to rotate. Such a test was done on DiffBot's motors. Using six DG01D-E motors the following values were recorded (sorted by increasing voltage): Motor Voltage (V) 01 2.5 02 2.8 - 3.0 03 3.1 04 3.2 05 3.2 06 3.3 In the videos above, motors numbered 01 and 03 were used coincidencely and I wasn't aware of the remarkable differences in voltage levels. Using the motors 04 and 05 improved the driving behaviour significantly. To deal with significant differences in the motors it would also help to tune the two PIDs individually, which is not shown in the video above . Make also sure that the motor driver outputs the same voltage level on both channels when the robot is commanded to move straight. The used Grove i2c motor driver was tested to do this. Another problem of not driving straight can be weight distribution or the orientation of the caster wheel. A good test to check the accuracy is to fix two meters of adhesive tape on the floor in a straight line. Then, place the robot on one end oriented in the direction to the other end. Now command it to move straight along the line and stop it when it reaches the end of the tape. Record the lateral displacement from the tape. Measuring a value below 10 cm is considered precise for these motors.","title":"PID Controller"},{"location":"diffbot_base.html#launch-file","text":"To run a single controller_manager, the one from the diffbot_base package defined inside difbot_base.cpp use the launch file from diffbot_base/launch/diffbot.launch : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 <!-- https://github.com/ros-controls/ros_controllers/tree/kinetic-devel/diff_drive_controller/test --> <launch> <!-- Load DiffBot model --> <param name= \"robot_description\" command= \"$(find xacro)/xacro '$(find diffbot_description)/urdf/diffbot.xacro'\" /> <node name= \"diffbot_base\" pkg= \"diffbot_base\" type= \"diffbot_base\" /> <!-- Load controller config to the parameter server --> <rosparam command= \"load\" file= \"$(find diffbot_control)/config/diffbot_control.yaml\" /> <!-- Load the controllers --> <node name= \"controller_spawner\" pkg= \"controller_manager\" type= \"spawner\" respawn= \"false\" output= \"screen\" ns= \"diffbot\" args= \"joint_state_controller mobile_base_controller\" /> </launch> This will load the DiffBot robot description onto the parameter server which is required for the hardware interface that gets created inside the next node diffbot_base . It creates the hardware interface and instantiates a new controller manager in the diffbot_base.cpp . Finally the spawner from the controller_manager package is used to initialize and start the controllers defined in the diffbot_control/config/diffbot_control.yaml . The last step in this launch file is required to get the controllers initialized and started. Another way would be to use controller_manager::ControllerManager::loadControllers() inside the diffbot_base.cpp . After launching this launch file on DiffBot (Raspberry Pi) with 1 roslaunch diffbot_base diffbot.launch the following parameters are stored on the parameter server: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 $ rosparam list /diffbot/hardware_interface/joints /diffbot/joint_state_controller/extra_joints /diffbot/joint_state_controller/publish_rate /diffbot/joint_state_controller/type /diffbot/mobile_base_controller/base_frame_id /diffbot/mobile_base_controller/left_wheel /diffbot/mobile_base_controller/pose_covariance_diagonal /diffbot/mobile_base_controller/publish_rate /diffbot/mobile_base_controller/right_wheel /diffbot/mobile_base_controller/twist_covariance_diagonal /diffbot/mobile_base_controller/type /diffbot/mobile_base_controller/wheel_radius /diffbot/mobile_base_controller/wheel_separation /robot_description /rosdistro /roslaunch/uris/host_tensorbook__46157 /roslaunch/uris/host_ubuntu__33729 /rosversion /run_id","title":"Launch File"},{"location":"diffbot_base.html#additional-requirements","text":"Because the hardware interface subscribes to the encoders, that are connected to the Teensy MCU, and publishes to the motors via the motr driver node, another launch will be required to run these additional nodes. See the diffbot_bringup package for this setup.","title":"Additional Requirements"},{"location":"diffbot_base.html#simulation","text":"To have a simulation showing DiffBot, the second step is to use the diffbot_gazebo/launch/diffbot_base.launch on the work pc: 1 $ roslaunch diffbot_gazebo diffbot_base.launch This will launch the gazebo simulation, which can make use of the running controllers inside the controller manager too: ROS Control with Gazebo Overview. After launching the Gazebo simulation the controllers got uninitialized. (It is assumed that the gazebo_ros_control plugin that gets launched). Because of this the controllers have to be initialized and started again. For this the diffbot_base/launch/controllers.launch should be used. This launch file is just loading and starting all controllers again. Note that using the spawner from the controller_manager package, like in the diffbot_base/launch/diffbot.launch results in an error. (TODO this needs some more testing).","title":"Simulation"},{"location":"diffbot_bringup.html","text":"DiffBot Bring Up Package The bringup package is used to initialize the real hardware of DiffBot and to actually drive the robot around. First the package is created using catkin-tools : 1 2 3 4 5 fjp@diffbot:~/git/diffbot/ros/src$ catkin create pkg diffbot_bringup Creating package \"diffbot_bringup\" in \"/home/fjp/git/diffbot/ros/src\"... Created file diffbot_bringup/package.xml Created file diffbot_bringup/CMakeLists.txt Successfully created package files in /home/fjp/git/diffbot/ros/src/diffbot_bringup. The package provides a launch folder which includes minimal.launch and bringup.launch . The minimal.launch is used to load DiffBot's robot descripton and the controller configuration onto the ROS parameter server using the launch file from the diffbot_base package . It will also setup the ROS controller manager with DiffBot's hardware interface . For the motor driver the node motor_driver.py from the grove_motor_driver package is started. And for the encoders rosserial communicates with the Teensy microcontroller to publish the encoder ticks. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 <launch> <!-- Including the following launch file from diffbot_base package will --> <!-- Load the robot description onto the parameter server --> <!-- Run the controller manager with DiffBot's hardware interface --> <!-- Load the controller config onto the parameter server --> <include file= \"$(find diffbot_base)/launch/diffbot.launch\" > <!-- arg name=\"model\" value=\"$(arg model)\" /--> </include> <!-- Motors --> <!-- --> <node name= \"motor_driver\" pkg= \"grove_motor_driver\" type= \"motor_driver.py\" respawn= \"false\" output= \"screen\" ns= \"diffbot\" /> <!-- Encoders --> <!-- Run rosserial to connect with the Teensy 3.2 board connected to the motor encoders --> <node name= \"rosserial_teensy\" pkg= \"rosserial_python\" type= \"serial_node.py\" respawn= \"false\" output= \"screen\" ns= \"diffbot\" args= \"_port:=/dev/ttyACM0 _baud:=115200\" /> </launch> As mentioned, the ROS controller used for DiffBot is the diff_drive_controller . This controller publishes a transform message (see its published topics ), via the /tf topic, between the odom frame and the frame configured in the controller's configuration specified by the base_frame_id . In the case of DiffBot this is the base_footprint , a conventional link, defined in REP-120 , for mobile robots that specifies the robot's footprint. Because this is the only transform published by diff_drive_controller another node is needed to publish rest of the link transformations. It is the well known robot_state_publisher , which uses the joint states published by the ROS controller joint_state_controller (not to be confused with joint_state_publisher - it is not used here, see this answer for the difference) to create the transforms between the links. To do this the bringup.launch includes the minimal.launch and then runs the robot_state_publisher : 1 2 3 4 5 6 7 8 9 10 11 12 <launch> <include file= \"$(find diffbot_bringup)/launch/minimal.launch\" > <!-- arg name=\"model\" value=\"$(arg model)\" /--> </include> <!-- Starting robot state publish which will publish tf --> <!-- This is needed to publish transforms between all links --> <!-- diff_drive_controller publishes only a single transfrom between odom and base_footprint --> <!-- The robot_state_publisher reads the joint states published by ros control's joint_state_controller --> <node name= \"robot_state_publisher\" pkg= \"robot_state_publisher\" type= \"robot_state_publisher\" output= \"screen\" ns= \"diffbot\" /> </launch>","title":"Hardware Bringup"},{"location":"diffbot_bringup.html#diffbot-bring-up-package","text":"The bringup package is used to initialize the real hardware of DiffBot and to actually drive the robot around. First the package is created using catkin-tools : 1 2 3 4 5 fjp@diffbot:~/git/diffbot/ros/src$ catkin create pkg diffbot_bringup Creating package \"diffbot_bringup\" in \"/home/fjp/git/diffbot/ros/src\"... Created file diffbot_bringup/package.xml Created file diffbot_bringup/CMakeLists.txt Successfully created package files in /home/fjp/git/diffbot/ros/src/diffbot_bringup. The package provides a launch folder which includes minimal.launch and bringup.launch . The minimal.launch is used to load DiffBot's robot descripton and the controller configuration onto the ROS parameter server using the launch file from the diffbot_base package . It will also setup the ROS controller manager with DiffBot's hardware interface . For the motor driver the node motor_driver.py from the grove_motor_driver package is started. And for the encoders rosserial communicates with the Teensy microcontroller to publish the encoder ticks. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 <launch> <!-- Including the following launch file from diffbot_base package will --> <!-- Load the robot description onto the parameter server --> <!-- Run the controller manager with DiffBot's hardware interface --> <!-- Load the controller config onto the parameter server --> <include file= \"$(find diffbot_base)/launch/diffbot.launch\" > <!-- arg name=\"model\" value=\"$(arg model)\" /--> </include> <!-- Motors --> <!-- --> <node name= \"motor_driver\" pkg= \"grove_motor_driver\" type= \"motor_driver.py\" respawn= \"false\" output= \"screen\" ns= \"diffbot\" /> <!-- Encoders --> <!-- Run rosserial to connect with the Teensy 3.2 board connected to the motor encoders --> <node name= \"rosserial_teensy\" pkg= \"rosserial_python\" type= \"serial_node.py\" respawn= \"false\" output= \"screen\" ns= \"diffbot\" args= \"_port:=/dev/ttyACM0 _baud:=115200\" /> </launch> As mentioned, the ROS controller used for DiffBot is the diff_drive_controller . This controller publishes a transform message (see its published topics ), via the /tf topic, between the odom frame and the frame configured in the controller's configuration specified by the base_frame_id . In the case of DiffBot this is the base_footprint , a conventional link, defined in REP-120 , for mobile robots that specifies the robot's footprint. Because this is the only transform published by diff_drive_controller another node is needed to publish rest of the link transformations. It is the well known robot_state_publisher , which uses the joint states published by the ROS controller joint_state_controller (not to be confused with joint_state_publisher - it is not used here, see this answer for the difference) to create the transforms between the links. To do this the bringup.launch includes the minimal.launch and then runs the robot_state_publisher : 1 2 3 4 5 6 7 8 9 10 11 12 <launch> <include file= \"$(find diffbot_bringup)/launch/minimal.launch\" > <!-- arg name=\"model\" value=\"$(arg model)\" /--> </include> <!-- Starting robot state publish which will publish tf --> <!-- This is needed to publish transforms between all links --> <!-- diff_drive_controller publishes only a single transfrom between odom and base_footprint --> <!-- The robot_state_publisher reads the joint states published by ros control's joint_state_controller --> <node name= \"robot_state_publisher\" pkg= \"robot_state_publisher\" type= \"robot_state_publisher\" output= \"screen\" ns= \"diffbot\" /> </launch>","title":"DiffBot Bring Up Package"},{"location":"diffbot_control.html","text":"DiffBot Control Package As described in the ROS Integration and Gazebo Simulation sections, DiffBot makes use of ROS Control repositories. Specifically the diff_drive_controller package from the ros_controllers meta package. To leverage ROS Control for the simulation with Gazebo the robot description and the controller configuration (usually a MYROBOT_control.yaml file) is required. For the real hardware its required to implement a class derived from hardware_interface::RobotHW . The convention to control a robot (in simulation and in the real world) is to have a package named MYROBOT_control . In case of DiffBot its called diffbot_control and created with catkin create pkg PKG_NAME [--catkin-deps [DEP [DEP ...]]] : 1 2 3 4 5 6 7 catkin create pkg diffbot_control --catkin-deps diff_drive_controller roscpp sensor_msgs Creating package \"diffbot_control\" in \"/home/fjp/git/diffbot/ros/src\"... Created file diffbot_control/CMakeLists.txt Created file diffbot_control/package.xml Created folder diffbot_control/include/diffbot_control Created folder diffbot_control/src Successfully created package files in /home/fjp/git/diffbot/ros/src/diffbot_control. To work with this package the specified dependencies must be installed either using the available Ubuntu/Debian packages for ROS Noetic or have to be built from source first. The following table lists the dependencies that we have to install because they are not already part of the ROS Noetic desktop full installation. Refer to the section ROS Noetic Setup for how this was done. Dependency Source Ubuntu/Debian Package diff_drive_controller https://github.com/ros-controls/ros_controllers ros-noetic-ros-controllers To install a package from source clone (using git) or download the source files from where they are located (commonly hosted on GitHub) into the src folder of a ros catkin workspace and execute the catkin build command. Also make sure to source the workspace after building new packages with source devel/setup.bash . 1 2 3 4 cd /home/fjp/git/diffbot/ros/ # Navigate to the workspace catkin build # Build all the packages in the workspace ls build # Show the resulting build space ls devel # Show the resulting devel space Make sure to clone/download the source files suitable for the ROS distribtion you are using. If the sources are not available for the distribution you are working with, it is worth to try building anyway. Chances are that the package you want to use is suitable for multiple ROS distros. For example if a package states in its docs, that it is only available for kinetic it is possible that it will work with a ROS noetic install. ROS Control in Gazebo Two great resources to get the diff_drive_controller working inside Gazebo is the Gazebo ROS Control Tutorial of rrbot and the R2D2 ROS URDF Tutorial , especially the last section, The Wheels on the Droid Go Round and Round . To spawn DiffBot inside Gazebo, RViz and control it with the rqt_robot_steering plugin, launch the diffbot.launch inside the diffbot_control package: 1 roslaunch diffbot_control diffbot.launch This launch file makes use of diffbot_gazebo/launch/diffbot.launch , diffbot_control/launch/diffbot_control.launch to run gazebo and the diff_drive_controller . It also opens RViz with the configuration stored in diffbot_control/rviz/diffbot.rviz . The following video shows the result of launching. Note the video may be outdated when you read this and the model has improved. ROS Control on the Real Hardware As mentioned above the its required to implement a class derived from hardware_interface::RobotHW . Let's call it DiffBotHW and create it inside the diffbot_control/src folder.","title":"Control"},{"location":"diffbot_control.html#diffbot-control-package","text":"As described in the ROS Integration and Gazebo Simulation sections, DiffBot makes use of ROS Control repositories. Specifically the diff_drive_controller package from the ros_controllers meta package. To leverage ROS Control for the simulation with Gazebo the robot description and the controller configuration (usually a MYROBOT_control.yaml file) is required. For the real hardware its required to implement a class derived from hardware_interface::RobotHW . The convention to control a robot (in simulation and in the real world) is to have a package named MYROBOT_control . In case of DiffBot its called diffbot_control and created with catkin create pkg PKG_NAME [--catkin-deps [DEP [DEP ...]]] : 1 2 3 4 5 6 7 catkin create pkg diffbot_control --catkin-deps diff_drive_controller roscpp sensor_msgs Creating package \"diffbot_control\" in \"/home/fjp/git/diffbot/ros/src\"... Created file diffbot_control/CMakeLists.txt Created file diffbot_control/package.xml Created folder diffbot_control/include/diffbot_control Created folder diffbot_control/src Successfully created package files in /home/fjp/git/diffbot/ros/src/diffbot_control. To work with this package the specified dependencies must be installed either using the available Ubuntu/Debian packages for ROS Noetic or have to be built from source first. The following table lists the dependencies that we have to install because they are not already part of the ROS Noetic desktop full installation. Refer to the section ROS Noetic Setup for how this was done. Dependency Source Ubuntu/Debian Package diff_drive_controller https://github.com/ros-controls/ros_controllers ros-noetic-ros-controllers To install a package from source clone (using git) or download the source files from where they are located (commonly hosted on GitHub) into the src folder of a ros catkin workspace and execute the catkin build command. Also make sure to source the workspace after building new packages with source devel/setup.bash . 1 2 3 4 cd /home/fjp/git/diffbot/ros/ # Navigate to the workspace catkin build # Build all the packages in the workspace ls build # Show the resulting build space ls devel # Show the resulting devel space Make sure to clone/download the source files suitable for the ROS distribtion you are using. If the sources are not available for the distribution you are working with, it is worth to try building anyway. Chances are that the package you want to use is suitable for multiple ROS distros. For example if a package states in its docs, that it is only available for kinetic it is possible that it will work with a ROS noetic install.","title":"DiffBot Control Package"},{"location":"diffbot_control.html#ros-control-in-gazebo","text":"Two great resources to get the diff_drive_controller working inside Gazebo is the Gazebo ROS Control Tutorial of rrbot and the R2D2 ROS URDF Tutorial , especially the last section, The Wheels on the Droid Go Round and Round . To spawn DiffBot inside Gazebo, RViz and control it with the rqt_robot_steering plugin, launch the diffbot.launch inside the diffbot_control package: 1 roslaunch diffbot_control diffbot.launch This launch file makes use of diffbot_gazebo/launch/diffbot.launch , diffbot_control/launch/diffbot_control.launch to run gazebo and the diff_drive_controller . It also opens RViz with the configuration stored in diffbot_control/rviz/diffbot.rviz . The following video shows the result of launching. Note the video may be outdated when you read this and the model has improved.","title":"ROS Control in Gazebo"},{"location":"diffbot_control.html#ros-control-on-the-real-hardware","text":"As mentioned above the its required to implement a class derived from hardware_interface::RobotHW . Let's call it DiffBotHW and create it inside the diffbot_control/src folder.","title":"ROS Control on the Real Hardware"},{"location":"diffbot_gazebo.html","text":"Simulate DiffBot in Gazebo As described in the Creating your own Gazebo ROS Package , it is common in ROS to create a package that contains all the world files and launch files used with Gazebo. These files are located in a ROS package named /MYROBOT_gazebo . For DiffBot the package is named diffbot_gazebo . Another example that follows best pratices is rrbot which can be found in the gazebo_ros_demos repository. 1 2 3 4 5 fjp@ubuntu:~/git/diffbot/ros/src$ catkin create pkg diffbot_gazebo Creating package \"diffbot_gazebo\" in \"/home/fjp/git/diffbot/ros/src\"... Created file diffbot_gazebo/package.xml Created file diffbot_gazebo/CMakeLists.txt Successfully created package files in /home/fjp/git/diffbot/ros/src/diffbot_gazebo. The diffbot_gazebo package contains a launch file to lauch a world in Gazebo and spawn the robot model, which is defined in the previously created diffbot_description package. For the launch files the convention is to have a folder named launch and for Gazebo world files a folder named world inside a package. 1 fjp@ubuntu:~/git/diffbot/ros/src/diffbot_gazebo$ mkdir launch world Inside the launch folder is the diffbot.launch . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 <launch> <!-- these are the arguments you can pass this launch file, for example paused:=true --> <arg name= \"paused\" default= \"false\" /> <arg name= \"use_sim_time\" default= \"true\" /> <arg name= \"gui\" default= \"true\" /> <arg name= \"headless\" default= \"false\" /> <arg name= \"debug\" default= \"false\" /> <!-- We resume the logic in empty_world.launch, changing only the name of the world to be launched --> <include file= \"$(find gazebo_ros)/launch/empty_world.launch\" > <arg name= \"world_name\" value= \"$(find diffbot_gazebo)/worlds/diffbot.world\" /> <arg name= \"debug\" value= \"$(arg debug)\" /> <arg name= \"gui\" value= \"$(arg gui)\" /> <arg name= \"paused\" value= \"$(arg paused)\" /> <arg name= \"use_sim_time\" value= \"$(arg use_sim_time)\" /> <arg name= \"headless\" value= \"$(arg headless)\" /> </include> </launch> In the world folder of the diffbot_gazebo package is the diffbot.world file: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 <?xml version=\"1.0\" ?> <sdf version= \"1.4\" > <world name= \"default\" > <include> <uri> model://ground_plane </uri> </include> <include> <uri> model://sun </uri> </include> <include> <uri> model://gas_station </uri> <name> gas_station </name> <pose> -2.0 7.0 0 0 0 0 </pose> </include> </world> </sdf> With these files build the catkin workspace and source it to make the new diffbot_gazebo package visible to roslaunch : 1 2 catkin build source devel/setup.zsh Then its possible to launch the diffbot.launch with: 1 roslaunch diffbot_gazebo diffbot.launch This will lead to the following output: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 roslaunch diffbot_gazebo diffbot.launch ... logging to /home/fjp/.ros/log/6be90ef2-fdd8-11ea-9cb3-317fd602d5f2/roslaunch-tensorbook-393333.log Checking log directory for disk usage. This may take a while. Press Ctrl-C to interrupt Done checking log file disk usage. Usage is <1GB. started roslaunch server http://tensorbook:32837/ SUMMARY ======== PARAMETERS * /gazebo/enable_ros_network: True * /rosdistro: noetic * /rosversion: 1.15.8 * /use_sim_time: True NODES / gazebo (gazebo_ros/gzserver) gazebo_gui (gazebo_ros/gzclient) ROS_MASTER_URI=http://localhost:11311 process[gazebo-1]: started with pid [393352] process[gazebo_gui-2]: started with pid [393357] [ INFO] [1600950165.494721382]: Finished loading Gazebo ROS API Plugin. [ INFO] [1600950165.495515766]: waitForService: Service [/gazebo/set_physics_properties] has not been advertised, waiting... [ INFO] [1600950165.649461740]: Finished loading Gazebo ROS API Plugin. [ INFO] [1600950165.650277038]: waitForService: Service [/gazebo_gui/set_physics_properties] has not been advertised, waiting... [ INFO] [1600950166.640929113]: waitForService: Service [/gazebo/set_physics_properties] is now available. [ INFO] [1600950166.659917502, 0.007000000]: Physics dynamic reconfigure ready. Also, the Gazebo simulator will open a new window with the objects defined in diffbot.world except for the Gas station because it is a model that has to be downloaded first, which is happening in the background. TODO: gas station is not showing this way. Empty world of DiffBot. To get the Gas station or other available models it is possible to clone the gazebo models repository into your /home/your_username/.gazebo folder, e.g.: 1 /home/fjp/.gazeb$ git clone osrf/gazebo_models Then add this path inside Gazebo to insert these models into your world file. Using ROS launch to Spawn URDF Robots According to the Gazebo roslaunch tutorial the recommended way to spawn a robot into Gazebo is to use a launch file. Therefore, edit the diffbot.launch inside the diffbot_gazebo package by adding the following inside the <launch> </launch tag: 1 2 3 4 5 6 7 <!-- Load the URDF into the ROS Parameter Server --> <param name=\"robot_description\" command=\"$(find xacro)/xacro --inorder '$(find diffbot_description)/urdf/diffbot.xacro'\" /> <!-- Run a python script to the send a service call to gazebo_ros to spawn a URDF robot --> <node name=\"urdf_spawner\" pkg=\"gazebo_ros\" type=\"spawn_model\" respawn=\"false\" output=\"screen\" args=\"-urdf -model diffbot -param robot_description\"/> See also the complete diffbot.launch file. This will open Gazebo simulator and show the DiffBot model: Empty world including DiffBot. Moving the Robot Note that the robot cannot be moved without having either a Gazebo plugin loaded or making use of ROS Control and its Gazebo plugin gazebo_ros_control , see also the Gazebo ROS Control Tutorial . Using the ROS Control and its Gazebo plugin is done in case of DiffBot. An alternative would be to use the existing differential_drive_controller Gazebo plugin without having to rely on ROS Control. The next section explains the diffbot_control package in more detail and how to setup the diff_drive_controller from the ros_controllers package. Adding Sensors To add sensors to a robot model make use of link and joint tags to define the desired location and shape, possibly using meshes. For the simulation of these sensor there exist common Gazebo plugins that can be used. See, the [Tutorial: Using Gazebo plugins with ROS] for existing plugins and more details how to use them. For a full list of plugins see also gazebo_ros_pkgs which is a package or interface for using ROS with the Gazebo simulator. Camera This section follows Gazebo tutorial Adding a Camera . Laser (Lidar) This section follows Gazebo tutorial Adding a Laser GPU . Ultrasonic Ranger See the source of the gazebo_ros_range plugin. Inertial Measurement Unit (IMU) This section follows Gazebo tutorial [Adding an IMU](http://gazebosim.org/tutorials?tut=ros_gzplugins#IMU(GazeboRosImu). Note use GazeboRosImuSensor? Troubleshooting A quick way to verify if the conversion from xacro to urdf to sdf is working is the following ( source: Tutorial URDF in Gazebo ): First convert the xacro model to a urdf model with the xacro command: 1 xacro src/diffbot_description/urdf/diffbot.xacro -o diffbot.urdf This will output the urdf into a file named diffbot.urdf in the current working directory. Then use the gz command to create a sdf: 1 2 # gazebo3 and above gz sdf -p MODEL.urdf DiffBot sdf. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 <sdf version= '1.7' > <model name= 'diffbot' > <link name= 'base_footprint' > <inertial> <pose> -0.012273 0 0.040818 0 -0 0 </pose> <mass> 5.5 </mass> <inertia> <ixx> 0.0387035 </ixx> <ixy> 0 </ixy> <ixz> 0.000552273 </ixz> <iyy> 0.0188626 </iyy> <iyz> 0 </iyz> <izz> 0.0561591 </izz> </inertia> </inertial> <collision name= 'base_footprint_collision' > <pose> 0 0 0 0 -0 0 </pose> <geometry> <box> <size> 0.001 0.001 0.001 </size> </box> </geometry> <surface> <contact> <ode/> </contact> <friction> <ode/> </friction> </surface> </collision> <collision name= 'base_footprint_fixed_joint_lump__base_link_collision_1' > <pose> 0 0 0.04 0 -0 0 </pose> <geometry> <box> <size> 0.3 0.15 0.02 </size> </box> </geometry> <surface> <contact> <ode/> </contact> <friction> <ode/> </friction> </surface> </collision> <collision name= 'base_footprint_fixed_joint_lump__caster_link_collision_2' > <pose> -0.135 0 0.029 0 -0 0 </pose> <geometry> <sphere> <radius> 0.025 </radius> </sphere> </geometry> <surface> <contact> <ode/> </contact> <friction> <ode/> </friction> </surface> </collision> <visual name= 'base_footprint_fixed_joint_lump__base_link_visual' > <pose> 0 0 0.04 0 -0 0 </pose> <geometry> <box> <size> 0.3 0.15 0.02 </size> </box> </geometry> <material> <script> <name> Gazebo/White </name> <uri> file://media/materials/scripts/gazebo.material </uri> </script> </material> </visual> <visual name= 'base_footprint_fixed_joint_lump__caster_link_visual_1' > <pose> -0.115 0 0.029 0 -0 0 </pose> <geometry> <sphere> <radius> 0.025 </radius> </sphere> </geometry> </visual> <velocity_decay/> <velocity_decay/> <gravity> 1 </gravity> <velocity_decay/> </link> <joint name= 'front_left_wheel_joint' type= 'revolute' > <pose relative_to= 'base_footprint' > 0.105 -0.085 0.04 0 -0 0 </pose> <parent> base_footprint </parent> <child> front_left_wheel </child> <axis> <xyz> 0 1 0 </xyz> <limit> <lower> -1e+16 </lower> <upper> 1e+16 </upper> </limit> <dynamics> <spring_reference> 0 </spring_reference> <spring_stiffness> 0 </spring_stiffness> </dynamics> </axis> </joint> <link name= 'front_left_wheel' > <pose relative_to= 'front_left_wheel_joint' > 0 0 0 0 -0 0 </pose> <inertial> <pose> 0 0 0 0 -0 0 </pose> <mass> 2.5 </mass> <inertia> <ixx> 0.00108333 </ixx> <ixy> 0 </ixy> <ixz> 0 </ixz> <iyy> 0.00108333 </iyy> <iyz> 0 </iyz> <izz> 0.002 </izz> </inertia> </inertial> <collision name= 'front_left_wheel_collision' > <pose> 0 0 0 1.5708 -0 0 </pose> <geometry> <cylinder> <length> 0.02 </length> <radius> 0.04 </radius> </cylinder> </geometry> <surface> <contact> <ode> <kp> 1e+07 </kp> <kd> 1 </kd> </ode> </contact> <friction> <ode> <mu> 1 </mu> <mu2> 1 </mu2> <fdir1> 1 0 0 </fdir1> </ode> </friction> </surface> </collision> <visual name= 'front_left_wheel_visual' > <pose> 0 0 0 1.5708 -0 0 </pose> <geometry> <cylinder> <length> 0.02 </length> <radius> 0.04 </radius> </cylinder> </geometry> <material> <script> <name> Gazebo/Grey </name> <uri> file://media/materials/scripts/gazebo.material </uri> </script> </material> </visual> <gravity> 1 </gravity> <velocity_decay/> </link> <joint name= 'front_right_wheel_joint' type= 'revolute' > <pose relative_to= 'base_footprint' > 0.105 0.085 0.04 0 -0 0 </pose> <parent> base_footprint </parent> <child> front_right_wheel </child> <axis> <xyz> 0 1 0 </xyz> <limit> <lower> -1e+16 </lower> <upper> 1e+16 </upper> </limit> <dynamics> <spring_reference> 0 </spring_reference> <spring_stiffness> 0 </spring_stiffness> </dynamics> </axis> </joint> <link name= 'front_right_wheel' > <pose relative_to= 'front_right_wheel_joint' > 0 0 0 0 -0 0 </pose> <inertial> <pose> 0 0 0 0 -0 0 </pose> <mass> 2.5 </mass> <inertia> <ixx> 0.00108333 </ixx> <ixy> 0 </ixy> <ixz> 0 </ixz> <iyy> 0.00108333 </iyy> <iyz> 0 </iyz> <izz> 0.002 </izz> </inertia> </inertial> <collision name= 'front_right_wheel_collision' > <pose> 0 0 0 1.5708 -0 0 </pose> <geometry> <cylinder> <length> 0.02 </length> <radius> 0.04 </radius> </cylinder> </geometry> <surface> <contact> <ode> <kp> 1e+07 </kp> <kd> 1 </kd> </ode> </contact> <friction> <ode> <mu> 1 </mu> <mu2> 1 </mu2> <fdir1> 1 0 0 </fdir1> </ode> </friction> </surface> </collision> <visual name= 'front_right_wheel_visual' > <pose> 0 0 0 1.5708 -0 0 </pose> <geometry> <cylinder> <length> 0.02 </length> <radius> 0.04 </radius> </cylinder> </geometry> <material> <script> <name> Gazebo/Grey </name> <uri> file://media/materials/scripts/gazebo.material </uri> </script> </material> </visual> <gravity> 1 </gravity> <velocity_decay/> </link> <plugin name= 'gazebo_ros_control' filename= 'libgazebo_ros_control.so' > <robotNamespace> /diffbot </robotNamespace> <robotSimType> gazebo_ros_control/DefaultRobotHWSim </robotSimType> </plugin> <static> 0 </static> <plugin name= 'differential_drive_controller' filename= 'libgazebo_ros_diff_drive.so' > <legacyMode> 1 </legacyMode> <rosDebugLevel> Debug </rosDebugLevel> <publishWheelTF> 0 </publishWheelTF> <robotNamespace> / </robotNamespace> <publishTf> 1 </publishTf> <publishWheelJointState> 0 </publishWheelJointState> <alwaysOn> 1 </alwaysOn> <updateRate> 100.0 </updateRate> <leftJoint> front_left_wheel_joint </leftJoint> <rightJoint> front_right_wheel_joint </rightJoint> <wheelSeparation> 0.3 </wheelSeparation> <wheelDiameter> 0.08 </wheelDiameter> <broadcastTF> 1 </broadcastTF> <wheelTorque> 30 </wheelTorque> <wheelAcceleration> 1.8 </wheelAcceleration> <commandTopic> cmd_vel </commandTopic> <odometryFrame> odom </odometryFrame> <odometryTopic> odom </odometryTopic> <robotBaseFrame> base_footprint </robotBaseFrame> </plugin> </model> </sdf> In case the output looks like the following, there are most certainly missing <inertial> tags in the <link> tag. For the Gazebo simulator the <inertial> must be present, in order to simulate the dynamics of the robot. See also http://wiki.ros.org/urdf/XML/link and the Gazebo tutorials on URDF . 1 2 3 <sdf version= '1.7' > <model name= 'diffbot' /> </sdf>","title":"Simulation"},{"location":"diffbot_gazebo.html#simulate-diffbot-in-gazebo","text":"As described in the Creating your own Gazebo ROS Package , it is common in ROS to create a package that contains all the world files and launch files used with Gazebo. These files are located in a ROS package named /MYROBOT_gazebo . For DiffBot the package is named diffbot_gazebo . Another example that follows best pratices is rrbot which can be found in the gazebo_ros_demos repository. 1 2 3 4 5 fjp@ubuntu:~/git/diffbot/ros/src$ catkin create pkg diffbot_gazebo Creating package \"diffbot_gazebo\" in \"/home/fjp/git/diffbot/ros/src\"... Created file diffbot_gazebo/package.xml Created file diffbot_gazebo/CMakeLists.txt Successfully created package files in /home/fjp/git/diffbot/ros/src/diffbot_gazebo. The diffbot_gazebo package contains a launch file to lauch a world in Gazebo and spawn the robot model, which is defined in the previously created diffbot_description package. For the launch files the convention is to have a folder named launch and for Gazebo world files a folder named world inside a package. 1 fjp@ubuntu:~/git/diffbot/ros/src/diffbot_gazebo$ mkdir launch world Inside the launch folder is the diffbot.launch . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 <launch> <!-- these are the arguments you can pass this launch file, for example paused:=true --> <arg name= \"paused\" default= \"false\" /> <arg name= \"use_sim_time\" default= \"true\" /> <arg name= \"gui\" default= \"true\" /> <arg name= \"headless\" default= \"false\" /> <arg name= \"debug\" default= \"false\" /> <!-- We resume the logic in empty_world.launch, changing only the name of the world to be launched --> <include file= \"$(find gazebo_ros)/launch/empty_world.launch\" > <arg name= \"world_name\" value= \"$(find diffbot_gazebo)/worlds/diffbot.world\" /> <arg name= \"debug\" value= \"$(arg debug)\" /> <arg name= \"gui\" value= \"$(arg gui)\" /> <arg name= \"paused\" value= \"$(arg paused)\" /> <arg name= \"use_sim_time\" value= \"$(arg use_sim_time)\" /> <arg name= \"headless\" value= \"$(arg headless)\" /> </include> </launch> In the world folder of the diffbot_gazebo package is the diffbot.world file: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 <?xml version=\"1.0\" ?> <sdf version= \"1.4\" > <world name= \"default\" > <include> <uri> model://ground_plane </uri> </include> <include> <uri> model://sun </uri> </include> <include> <uri> model://gas_station </uri> <name> gas_station </name> <pose> -2.0 7.0 0 0 0 0 </pose> </include> </world> </sdf> With these files build the catkin workspace and source it to make the new diffbot_gazebo package visible to roslaunch : 1 2 catkin build source devel/setup.zsh Then its possible to launch the diffbot.launch with: 1 roslaunch diffbot_gazebo diffbot.launch This will lead to the following output: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 roslaunch diffbot_gazebo diffbot.launch ... logging to /home/fjp/.ros/log/6be90ef2-fdd8-11ea-9cb3-317fd602d5f2/roslaunch-tensorbook-393333.log Checking log directory for disk usage. This may take a while. Press Ctrl-C to interrupt Done checking log file disk usage. Usage is <1GB. started roslaunch server http://tensorbook:32837/ SUMMARY ======== PARAMETERS * /gazebo/enable_ros_network: True * /rosdistro: noetic * /rosversion: 1.15.8 * /use_sim_time: True NODES / gazebo (gazebo_ros/gzserver) gazebo_gui (gazebo_ros/gzclient) ROS_MASTER_URI=http://localhost:11311 process[gazebo-1]: started with pid [393352] process[gazebo_gui-2]: started with pid [393357] [ INFO] [1600950165.494721382]: Finished loading Gazebo ROS API Plugin. [ INFO] [1600950165.495515766]: waitForService: Service [/gazebo/set_physics_properties] has not been advertised, waiting... [ INFO] [1600950165.649461740]: Finished loading Gazebo ROS API Plugin. [ INFO] [1600950165.650277038]: waitForService: Service [/gazebo_gui/set_physics_properties] has not been advertised, waiting... [ INFO] [1600950166.640929113]: waitForService: Service [/gazebo/set_physics_properties] is now available. [ INFO] [1600950166.659917502, 0.007000000]: Physics dynamic reconfigure ready. Also, the Gazebo simulator will open a new window with the objects defined in diffbot.world except for the Gas station because it is a model that has to be downloaded first, which is happening in the background. TODO: gas station is not showing this way. Empty world of DiffBot. To get the Gas station or other available models it is possible to clone the gazebo models repository into your /home/your_username/.gazebo folder, e.g.: 1 /home/fjp/.gazeb$ git clone osrf/gazebo_models Then add this path inside Gazebo to insert these models into your world file.","title":"Simulate DiffBot in Gazebo"},{"location":"diffbot_gazebo.html#using-ros-launch-to-spawn-urdf-robots","text":"According to the Gazebo roslaunch tutorial the recommended way to spawn a robot into Gazebo is to use a launch file. Therefore, edit the diffbot.launch inside the diffbot_gazebo package by adding the following inside the <launch> </launch tag: 1 2 3 4 5 6 7 <!-- Load the URDF into the ROS Parameter Server --> <param name=\"robot_description\" command=\"$(find xacro)/xacro --inorder '$(find diffbot_description)/urdf/diffbot.xacro'\" /> <!-- Run a python script to the send a service call to gazebo_ros to spawn a URDF robot --> <node name=\"urdf_spawner\" pkg=\"gazebo_ros\" type=\"spawn_model\" respawn=\"false\" output=\"screen\" args=\"-urdf -model diffbot -param robot_description\"/> See also the complete diffbot.launch file. This will open Gazebo simulator and show the DiffBot model: Empty world including DiffBot.","title":"Using ROS launch to Spawn URDF Robots"},{"location":"diffbot_gazebo.html#moving-the-robot","text":"Note that the robot cannot be moved without having either a Gazebo plugin loaded or making use of ROS Control and its Gazebo plugin gazebo_ros_control , see also the Gazebo ROS Control Tutorial . Using the ROS Control and its Gazebo plugin is done in case of DiffBot. An alternative would be to use the existing differential_drive_controller Gazebo plugin without having to rely on ROS Control. The next section explains the diffbot_control package in more detail and how to setup the diff_drive_controller from the ros_controllers package.","title":"Moving the Robot"},{"location":"diffbot_gazebo.html#adding-sensors","text":"To add sensors to a robot model make use of link and joint tags to define the desired location and shape, possibly using meshes. For the simulation of these sensor there exist common Gazebo plugins that can be used. See, the [Tutorial: Using Gazebo plugins with ROS] for existing plugins and more details how to use them. For a full list of plugins see also gazebo_ros_pkgs which is a package or interface for using ROS with the Gazebo simulator.","title":"Adding Sensors"},{"location":"diffbot_gazebo.html#camera","text":"This section follows Gazebo tutorial Adding a Camera .","title":"Camera"},{"location":"diffbot_gazebo.html#laser-lidar","text":"This section follows Gazebo tutorial Adding a Laser GPU .","title":"Laser (Lidar)"},{"location":"diffbot_gazebo.html#ultrasonic-ranger","text":"See the source of the gazebo_ros_range plugin.","title":"Ultrasonic Ranger"},{"location":"diffbot_gazebo.html#inertial-measurement-unit-imu","text":"This section follows Gazebo tutorial [Adding an IMU](http://gazebosim.org/tutorials?tut=ros_gzplugins#IMU(GazeboRosImu). Note use GazeboRosImuSensor?","title":"Inertial Measurement Unit (IMU)"},{"location":"diffbot_gazebo.html#troubleshooting","text":"A quick way to verify if the conversion from xacro to urdf to sdf is working is the following ( source: Tutorial URDF in Gazebo ): First convert the xacro model to a urdf model with the xacro command: 1 xacro src/diffbot_description/urdf/diffbot.xacro -o diffbot.urdf This will output the urdf into a file named diffbot.urdf in the current working directory. Then use the gz command to create a sdf: 1 2 # gazebo3 and above gz sdf -p MODEL.urdf DiffBot sdf. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 <sdf version= '1.7' > <model name= 'diffbot' > <link name= 'base_footprint' > <inertial> <pose> -0.012273 0 0.040818 0 -0 0 </pose> <mass> 5.5 </mass> <inertia> <ixx> 0.0387035 </ixx> <ixy> 0 </ixy> <ixz> 0.000552273 </ixz> <iyy> 0.0188626 </iyy> <iyz> 0 </iyz> <izz> 0.0561591 </izz> </inertia> </inertial> <collision name= 'base_footprint_collision' > <pose> 0 0 0 0 -0 0 </pose> <geometry> <box> <size> 0.001 0.001 0.001 </size> </box> </geometry> <surface> <contact> <ode/> </contact> <friction> <ode/> </friction> </surface> </collision> <collision name= 'base_footprint_fixed_joint_lump__base_link_collision_1' > <pose> 0 0 0.04 0 -0 0 </pose> <geometry> <box> <size> 0.3 0.15 0.02 </size> </box> </geometry> <surface> <contact> <ode/> </contact> <friction> <ode/> </friction> </surface> </collision> <collision name= 'base_footprint_fixed_joint_lump__caster_link_collision_2' > <pose> -0.135 0 0.029 0 -0 0 </pose> <geometry> <sphere> <radius> 0.025 </radius> </sphere> </geometry> <surface> <contact> <ode/> </contact> <friction> <ode/> </friction> </surface> </collision> <visual name= 'base_footprint_fixed_joint_lump__base_link_visual' > <pose> 0 0 0.04 0 -0 0 </pose> <geometry> <box> <size> 0.3 0.15 0.02 </size> </box> </geometry> <material> <script> <name> Gazebo/White </name> <uri> file://media/materials/scripts/gazebo.material </uri> </script> </material> </visual> <visual name= 'base_footprint_fixed_joint_lump__caster_link_visual_1' > <pose> -0.115 0 0.029 0 -0 0 </pose> <geometry> <sphere> <radius> 0.025 </radius> </sphere> </geometry> </visual> <velocity_decay/> <velocity_decay/> <gravity> 1 </gravity> <velocity_decay/> </link> <joint name= 'front_left_wheel_joint' type= 'revolute' > <pose relative_to= 'base_footprint' > 0.105 -0.085 0.04 0 -0 0 </pose> <parent> base_footprint </parent> <child> front_left_wheel </child> <axis> <xyz> 0 1 0 </xyz> <limit> <lower> -1e+16 </lower> <upper> 1e+16 </upper> </limit> <dynamics> <spring_reference> 0 </spring_reference> <spring_stiffness> 0 </spring_stiffness> </dynamics> </axis> </joint> <link name= 'front_left_wheel' > <pose relative_to= 'front_left_wheel_joint' > 0 0 0 0 -0 0 </pose> <inertial> <pose> 0 0 0 0 -0 0 </pose> <mass> 2.5 </mass> <inertia> <ixx> 0.00108333 </ixx> <ixy> 0 </ixy> <ixz> 0 </ixz> <iyy> 0.00108333 </iyy> <iyz> 0 </iyz> <izz> 0.002 </izz> </inertia> </inertial> <collision name= 'front_left_wheel_collision' > <pose> 0 0 0 1.5708 -0 0 </pose> <geometry> <cylinder> <length> 0.02 </length> <radius> 0.04 </radius> </cylinder> </geometry> <surface> <contact> <ode> <kp> 1e+07 </kp> <kd> 1 </kd> </ode> </contact> <friction> <ode> <mu> 1 </mu> <mu2> 1 </mu2> <fdir1> 1 0 0 </fdir1> </ode> </friction> </surface> </collision> <visual name= 'front_left_wheel_visual' > <pose> 0 0 0 1.5708 -0 0 </pose> <geometry> <cylinder> <length> 0.02 </length> <radius> 0.04 </radius> </cylinder> </geometry> <material> <script> <name> Gazebo/Grey </name> <uri> file://media/materials/scripts/gazebo.material </uri> </script> </material> </visual> <gravity> 1 </gravity> <velocity_decay/> </link> <joint name= 'front_right_wheel_joint' type= 'revolute' > <pose relative_to= 'base_footprint' > 0.105 0.085 0.04 0 -0 0 </pose> <parent> base_footprint </parent> <child> front_right_wheel </child> <axis> <xyz> 0 1 0 </xyz> <limit> <lower> -1e+16 </lower> <upper> 1e+16 </upper> </limit> <dynamics> <spring_reference> 0 </spring_reference> <spring_stiffness> 0 </spring_stiffness> </dynamics> </axis> </joint> <link name= 'front_right_wheel' > <pose relative_to= 'front_right_wheel_joint' > 0 0 0 0 -0 0 </pose> <inertial> <pose> 0 0 0 0 -0 0 </pose> <mass> 2.5 </mass> <inertia> <ixx> 0.00108333 </ixx> <ixy> 0 </ixy> <ixz> 0 </ixz> <iyy> 0.00108333 </iyy> <iyz> 0 </iyz> <izz> 0.002 </izz> </inertia> </inertial> <collision name= 'front_right_wheel_collision' > <pose> 0 0 0 1.5708 -0 0 </pose> <geometry> <cylinder> <length> 0.02 </length> <radius> 0.04 </radius> </cylinder> </geometry> <surface> <contact> <ode> <kp> 1e+07 </kp> <kd> 1 </kd> </ode> </contact> <friction> <ode> <mu> 1 </mu> <mu2> 1 </mu2> <fdir1> 1 0 0 </fdir1> </ode> </friction> </surface> </collision> <visual name= 'front_right_wheel_visual' > <pose> 0 0 0 1.5708 -0 0 </pose> <geometry> <cylinder> <length> 0.02 </length> <radius> 0.04 </radius> </cylinder> </geometry> <material> <script> <name> Gazebo/Grey </name> <uri> file://media/materials/scripts/gazebo.material </uri> </script> </material> </visual> <gravity> 1 </gravity> <velocity_decay/> </link> <plugin name= 'gazebo_ros_control' filename= 'libgazebo_ros_control.so' > <robotNamespace> /diffbot </robotNamespace> <robotSimType> gazebo_ros_control/DefaultRobotHWSim </robotSimType> </plugin> <static> 0 </static> <plugin name= 'differential_drive_controller' filename= 'libgazebo_ros_diff_drive.so' > <legacyMode> 1 </legacyMode> <rosDebugLevel> Debug </rosDebugLevel> <publishWheelTF> 0 </publishWheelTF> <robotNamespace> / </robotNamespace> <publishTf> 1 </publishTf> <publishWheelJointState> 0 </publishWheelJointState> <alwaysOn> 1 </alwaysOn> <updateRate> 100.0 </updateRate> <leftJoint> front_left_wheel_joint </leftJoint> <rightJoint> front_right_wheel_joint </rightJoint> <wheelSeparation> 0.3 </wheelSeparation> <wheelDiameter> 0.08 </wheelDiameter> <broadcastTF> 1 </broadcastTF> <wheelTorque> 30 </wheelTorque> <wheelAcceleration> 1.8 </wheelAcceleration> <commandTopic> cmd_vel </commandTopic> <odometryFrame> odom </odometryFrame> <odometryTopic> odom </odometryTopic> <robotBaseFrame> base_footprint </robotBaseFrame> </plugin> </model> </sdf> In case the output looks like the following, there are most certainly missing <inertial> tags in the <link> tag. For the Gazebo simulator the <inertial> must be present, in order to simulate the dynamics of the robot. See also http://wiki.ros.org/urdf/XML/link and the Gazebo tutorials on URDF . 1 2 3 <sdf version= '1.7' > <model name= 'diffbot' /> </sdf>","title":"Troubleshooting"},{"location":"diffbot_mbf.html","text":"DiffBot Move Base Flex As described in the move_base_flex ROS wiki : Move Base Flex (MBF) is a backwards-compatible replacement for move_base. MBF can use existing plugins for move_base, and provides an enhanced version of the planner, controller and recovery plugin ROS interfaces. It exposes action servers for planning, controlling and recovering, providing detailed information of the current state and the plugin\u2019s feedback. An external executive logic can use MBF and its actions to perform smart and flexible navigation strategies. Furthermore, MBF enables the use of other map representations, e.g. meshes or grid_map This package is a meta package and refers to the Move Base Flex stack packages.The abstract core of MBF \u2013 without any binding to a map representation \u2013 is represented by the mbf_abstract_nav and the mbf_abstract_core . For navigation on costmaps see mbf_costmap_nav and mbf_costmap_core . This diffbot_mbf package was created using catkin-tools using the following command: 1 2 3 4 5 catkin create pkg diffbot_mbf Creating package \"diffbot_mbf\" in \"/home/fjp/git/ros_ws/src/diffbot\"... Created file diffbot_mbf/package.xml Created file diffbot_mbf/CMakeLists.txt Successfully created package files in /home/fjp/git/ros_ws/src/diffbot/diffbot_mbf. Additionally the following Ubuntu packages are required dependencies of move_base_flex : 1 sudo apt install ros-noetic-mbf-costmap-nav Another working example for turtlebot3 can be found in the turtlebot3_mbf package.","title":"Move Base Flex"},{"location":"diffbot_mbf.html#diffbot-move-base-flex","text":"As described in the move_base_flex ROS wiki : Move Base Flex (MBF) is a backwards-compatible replacement for move_base. MBF can use existing plugins for move_base, and provides an enhanced version of the planner, controller and recovery plugin ROS interfaces. It exposes action servers for planning, controlling and recovering, providing detailed information of the current state and the plugin\u2019s feedback. An external executive logic can use MBF and its actions to perform smart and flexible navigation strategies. Furthermore, MBF enables the use of other map representations, e.g. meshes or grid_map This package is a meta package and refers to the Move Base Flex stack packages.The abstract core of MBF \u2013 without any binding to a map representation \u2013 is represented by the mbf_abstract_nav and the mbf_abstract_core . For navigation on costmaps see mbf_costmap_nav and mbf_costmap_core . This diffbot_mbf package was created using catkin-tools using the following command: 1 2 3 4 5 catkin create pkg diffbot_mbf Creating package \"diffbot_mbf\" in \"/home/fjp/git/ros_ws/src/diffbot\"... Created file diffbot_mbf/package.xml Created file diffbot_mbf/CMakeLists.txt Successfully created package files in /home/fjp/git/ros_ws/src/diffbot/diffbot_mbf. Additionally the following Ubuntu packages are required dependencies of move_base_flex : 1 sudo apt install ros-noetic-mbf-costmap-nav Another working example for turtlebot3 can be found in the turtlebot3_mbf package.","title":"DiffBot Move Base Flex"},{"location":"diffbot_msgs.html","text":"DiffBot Messages Package As mentioned before, the nodes in ROS communicate with each other by publishing messages to topics . ROS provides the std_msgs package that includes ROS' common message types to represent primitive data types (see the ROS msg specification for primitive types) and other basic message constructs, such as multiarrays. Note howerver, the following from the std_msgs documentation : Quote The types in std_msgs do not convey semantic meaning about their contents: every message simply has a field called \"data\". Therefore, while the messages in this package can be useful for quick prototyping, they are NOT intended for \"long-term\" usage. For ease of documentation and collaboration, we recommend that existing messages be used, or new messages created, that provide meaningful field name(s). Diffbot Messages Therefore, we create a package that contains message definitions specific to DiffBot. The following command uses catkin-tools to create the diffbot_msgs package: 1 2 3 4 5 6 ros_ws/src$ catkin create pkg diffbot_msgs --catkin-deps message_generation std_msgs Creating package \"diffbot_msgs\" in \"/home/fjp/git/ros_ws/src\"... WARNING: Packages with messages or services should depend on both message_generation and message_runtime Created file diffbot_msgs/package.xml Created file diffbot_msgs/CMakeLists.txt Successfully created package files in /home/fjp/git/ros_ws/src/diffbot_msgs. Note The following is based on ROS Tutorials Creating Msg And Srv . In this tutorial you can find the required configurations for the package.xml and CMakeLists.txt . Encoders Currently there is no encoder message definition in ROS (see the sensor_msgs package) which is why a dedicated message is created for the encoders. For this, a simple msg description file, named Encoders.msg is created in the msg/ subdirectory of this diffbot_msgs package: 1 2 3 4 5 6 7 8 # This is a message to hold number of ticks from Encoders Header header # Use an array of size two of type int32 for the two encoders. # int32 is used instead of int64 because it is not supporte by Arduino/Teensy. # An overflow is also unlikely with the encoders of the DG01D-E # motor with its encoder because of its low encoder resolution int32[2] ticks The message includes the message type Header (see also Header msg ) which includes common metadata fileds such as timestamp that is automatically set by ROS client libraries . Having this encoder message description gives semantic meaning to the encoder messages and for example avoids having two separate int32 publishers for each encoder. Combining the encoder message into a single one alleviates additional timing problems. There exists also the common_msgs meta package for common, generic robot-specific message types. From the common_msgs DiffBot uses for example the nav_msgs for navigation with the navigation stack . Other relevant message definitions are the sensor_msgs/Imu and sensor_msgs/LaserScan , where both are definitions from the sensor_msgs package. Wheel Commands To command a joint velocity for each wheel diffbot_msgs provides the WheelCmd.msg . This specifies the Header and a float64 array for the angular wheel joint velocities. 1 2 3 4 5 6 # This is a message that holds commanded angular joint velocity Header header # Use an array of type float32 for the two wheel joint velocities. # float32 is used instead of float64 because it is not supporte by Arduino/Teensy. float32[] velocities Using rosmsg After building the package and its messages using catkin build let's make sure that ROS can see it using the rosmsg show command. 1 2 3 4 5 6 $ rosmsg show diffbot_msgs/Encoders std_msgs/Header header uint32 seq time stamp string frame_id int32[2] encoders Tip When using the a ros command such as rosmsg make use of the Tab key to auto complete the message name. ROSSerial The generated messages in this packages are used on the Teensy microcontroller, which is using rosserial . Integrating these messages requires the following steps. Generate rosserial libraries in a temporary folder using the make_libraries script: 1 rosrun rosserial_client make_libraries ~/Arduino/tmp/ This will generate all messages for ALL installed packages, but in our case only the diffbot_msgs package is needed to avoid missing includes. Copy the generated ~/Arduino/tmp/diffbot_msgs message folder to the src folder of the rosserial Arduino library. When rosserial was installed with the Arduino Library Manager, the location is ~/Arduino/libraries/Rosserial_Arduino_Library/ . Usage The new messages, specific to DiffBot, can be used by including the generated header, for example #include <diffbot_msgs/Encoders.h> . References Tutorials Arduino IDE Setup , specifically Install ros_lib into the Arduino Environment rosserial limitations : float64 is not supported on Arduino.","title":"Messages"},{"location":"diffbot_msgs.html#diffbot-messages-package","text":"As mentioned before, the nodes in ROS communicate with each other by publishing messages to topics . ROS provides the std_msgs package that includes ROS' common message types to represent primitive data types (see the ROS msg specification for primitive types) and other basic message constructs, such as multiarrays. Note howerver, the following from the std_msgs documentation : Quote The types in std_msgs do not convey semantic meaning about their contents: every message simply has a field called \"data\". Therefore, while the messages in this package can be useful for quick prototyping, they are NOT intended for \"long-term\" usage. For ease of documentation and collaboration, we recommend that existing messages be used, or new messages created, that provide meaningful field name(s).","title":"DiffBot Messages Package"},{"location":"diffbot_msgs.html#diffbot-messages","text":"Therefore, we create a package that contains message definitions specific to DiffBot. The following command uses catkin-tools to create the diffbot_msgs package: 1 2 3 4 5 6 ros_ws/src$ catkin create pkg diffbot_msgs --catkin-deps message_generation std_msgs Creating package \"diffbot_msgs\" in \"/home/fjp/git/ros_ws/src\"... WARNING: Packages with messages or services should depend on both message_generation and message_runtime Created file diffbot_msgs/package.xml Created file diffbot_msgs/CMakeLists.txt Successfully created package files in /home/fjp/git/ros_ws/src/diffbot_msgs. Note The following is based on ROS Tutorials Creating Msg And Srv . In this tutorial you can find the required configurations for the package.xml and CMakeLists.txt .","title":"Diffbot Messages"},{"location":"diffbot_msgs.html#encoders","text":"Currently there is no encoder message definition in ROS (see the sensor_msgs package) which is why a dedicated message is created for the encoders. For this, a simple msg description file, named Encoders.msg is created in the msg/ subdirectory of this diffbot_msgs package: 1 2 3 4 5 6 7 8 # This is a message to hold number of ticks from Encoders Header header # Use an array of size two of type int32 for the two encoders. # int32 is used instead of int64 because it is not supporte by Arduino/Teensy. # An overflow is also unlikely with the encoders of the DG01D-E # motor with its encoder because of its low encoder resolution int32[2] ticks The message includes the message type Header (see also Header msg ) which includes common metadata fileds such as timestamp that is automatically set by ROS client libraries . Having this encoder message description gives semantic meaning to the encoder messages and for example avoids having two separate int32 publishers for each encoder. Combining the encoder message into a single one alleviates additional timing problems. There exists also the common_msgs meta package for common, generic robot-specific message types. From the common_msgs DiffBot uses for example the nav_msgs for navigation with the navigation stack . Other relevant message definitions are the sensor_msgs/Imu and sensor_msgs/LaserScan , where both are definitions from the sensor_msgs package.","title":"Encoders"},{"location":"diffbot_msgs.html#wheel-commands","text":"To command a joint velocity for each wheel diffbot_msgs provides the WheelCmd.msg . This specifies the Header and a float64 array for the angular wheel joint velocities. 1 2 3 4 5 6 # This is a message that holds commanded angular joint velocity Header header # Use an array of type float32 for the two wheel joint velocities. # float32 is used instead of float64 because it is not supporte by Arduino/Teensy. float32[] velocities","title":"Wheel Commands"},{"location":"diffbot_msgs.html#using-rosmsg","text":"After building the package and its messages using catkin build let's make sure that ROS can see it using the rosmsg show command. 1 2 3 4 5 6 $ rosmsg show diffbot_msgs/Encoders std_msgs/Header header uint32 seq time stamp string frame_id int32[2] encoders Tip When using the a ros command such as rosmsg make use of the Tab key to auto complete the message name.","title":"Using rosmsg"},{"location":"diffbot_msgs.html#rosserial","text":"The generated messages in this packages are used on the Teensy microcontroller, which is using rosserial . Integrating these messages requires the following steps. Generate rosserial libraries in a temporary folder using the make_libraries script: 1 rosrun rosserial_client make_libraries ~/Arduino/tmp/ This will generate all messages for ALL installed packages, but in our case only the diffbot_msgs package is needed to avoid missing includes. Copy the generated ~/Arduino/tmp/diffbot_msgs message folder to the src folder of the rosserial Arduino library. When rosserial was installed with the Arduino Library Manager, the location is ~/Arduino/libraries/Rosserial_Arduino_Library/ .","title":"ROSSerial"},{"location":"diffbot_msgs.html#usage","text":"The new messages, specific to DiffBot, can be used by including the generated header, for example #include <diffbot_msgs/Encoders.h> .","title":"Usage"},{"location":"diffbot_msgs.html#references","text":"Tutorials Arduino IDE Setup , specifically Install ros_lib into the Arduino Environment rosserial limitations : float64 is not supported on Arduino.","title":"References"},{"location":"diffbot_navigation.html","text":"DiffBot Navigation Package Navigation Stack Overview. 1 2 3 4 5 fjp@diffbot:~/catkin_ws/src/diffbot$ catkin create pkg diffbot_navigation --catkin-deps amcl map_server move_base diffbot_bringup Creating package \"diffbot_navigation\" in \"/home/fjp/git/ros_ws/src/diffbot\"... Created file diffbot_navigation/package.xml Created file diffbot_navigation/CMakeLists.txt Successfully created package files in /home/fjp/git/ros_ws/src/diffbot/diffbot_navigation. We also need the following ROS packages that can be installed from the ROS Ubuntu packages: 1 $ sudo apt install ros-noetic-dwa-local-planner ros-noetic-amcl ros-noetic-map-server ros-noetic-move-base After this we create the required launch files and parameter configurations. These will be used for the simulation and the real robot. First we focus on the simulation in Gazebo. Launch files All launch files are in the folder named launch of the diffbot_navigation package. Inside the move_base.launch it is important to remap the following topics: 1 2 3 4 5 6 7 <!-- Arguments --> <arg name= \"cmd_vel_topic\" default= \"/diffbot/mobile_base_controller/cmd_vel\" /> <arg name= \"odom_topic\" default= \"/diffbot/mobile_base_controller/odom\" /> ... <!-- remappings of move_base node --> <remap from= \"cmd_vel\" to= \"$(arg cmd_vel_topic)\" /> <remap from= \"odom\" to= \"$(arg odom_topic)\" /> Parameter Configuration The parameters for the navigation package go into the config (for some robots named param ) folder. Most of them can be changed during runtime using dynamic reconfigure with the rqt_reconfigure gui . Setup and Configuration of the Navigation Stack on a Robot amcl : amcl is a probabilistic localization system for a robot moving in 2D. It implements the adaptive (or KLD-sampling) Monte Carlo localization approach (as described by Dieter Fox), which uses a particle filter to track the pose of a robot against a known map. map_server : provides the map_server ROS Node, which offers map data as a ROS Service. It also provides the map_saver command-line utility, which allows dynamically generated maps to be saved to file. move_base : The move_base package provides an implementation of an action (see the actionlib package) that, given a goal in the world, will attempt to reach it with a mobile base. The move_base node links together a global and local planner to accomplish its global navigation task. It supports any global planner adhering to the nav_core::BaseGlobalPlanner interface specified in the nav_core package and any local planner adhering to the nav_core::BaseLocalPlanner interface specified in the nav_core package. The move_base node also maintains two costmaps, one for the global planner, and one for a local planner (see the costmap_2d package) that are used to accomplish navigation tasks. gmapping : This package contains a ROS wrapper for OpenSlam's Gmapping. The gmapping package provides laser-based SLAM (Simultaneous Localization and Mapping), as a ROS node called slam_gmapping. Using slam_gmapping, you can create a 2-D occupancy grid map (like a building floorplan) from laser and pose data collected by a mobile robot. ROS cartographer slam_toolbox Examples - TurtleBot3 Navigation Navigation in Gazebo with available Map To navigate the robot in the simulation run the following command but make sure to first download the turtlebot3_world to your ~/.gazebo/models/ folder. This is required because the turtlebot3_world.world file references the turtlebot3_world model. 1 roslaunch diffbot_navigation diffbot.launch world_name:='$(find diffbot_gazebo)/worlds/turtlebot3_world.world' This will spawn DiffBot inside the turtlebot3 world inside Gazebo and visualize the elements of the navigation stack in RViz. Navigation demo of DiffBot (click to view the demo on Youtube). To navigate the robot using the default DWA planner in the known map, coming from the running map_server , you can use the 2D Nav Goal in RViz . Just select the navigation arrow to where the robot should move as shown in the animation above. The DWA local planner is working for differential drive robots, like DiffBot. For other robots such as non-holonomic robots or other types of mobile robots (also differential drive robots) other planners can be used. See for example teb_local_planner . Resources Global Planners: - global_planner Local Planners: - Difference between DWA and Base Local Planner - Difference between DWA and TEB Local Planner","title":"Navigation"},{"location":"diffbot_navigation.html#diffbot-navigation-package","text":"Navigation Stack Overview. 1 2 3 4 5 fjp@diffbot:~/catkin_ws/src/diffbot$ catkin create pkg diffbot_navigation --catkin-deps amcl map_server move_base diffbot_bringup Creating package \"diffbot_navigation\" in \"/home/fjp/git/ros_ws/src/diffbot\"... Created file diffbot_navigation/package.xml Created file diffbot_navigation/CMakeLists.txt Successfully created package files in /home/fjp/git/ros_ws/src/diffbot/diffbot_navigation. We also need the following ROS packages that can be installed from the ROS Ubuntu packages: 1 $ sudo apt install ros-noetic-dwa-local-planner ros-noetic-amcl ros-noetic-map-server ros-noetic-move-base After this we create the required launch files and parameter configurations. These will be used for the simulation and the real robot. First we focus on the simulation in Gazebo.","title":"DiffBot Navigation Package"},{"location":"diffbot_navigation.html#launch-files","text":"All launch files are in the folder named launch of the diffbot_navigation package. Inside the move_base.launch it is important to remap the following topics: 1 2 3 4 5 6 7 <!-- Arguments --> <arg name= \"cmd_vel_topic\" default= \"/diffbot/mobile_base_controller/cmd_vel\" /> <arg name= \"odom_topic\" default= \"/diffbot/mobile_base_controller/odom\" /> ... <!-- remappings of move_base node --> <remap from= \"cmd_vel\" to= \"$(arg cmd_vel_topic)\" /> <remap from= \"odom\" to= \"$(arg odom_topic)\" />","title":"Launch files"},{"location":"diffbot_navigation.html#parameter-configuration","text":"The parameters for the navigation package go into the config (for some robots named param ) folder. Most of them can be changed during runtime using dynamic reconfigure with the rqt_reconfigure gui . Setup and Configuration of the Navigation Stack on a Robot amcl : amcl is a probabilistic localization system for a robot moving in 2D. It implements the adaptive (or KLD-sampling) Monte Carlo localization approach (as described by Dieter Fox), which uses a particle filter to track the pose of a robot against a known map. map_server : provides the map_server ROS Node, which offers map data as a ROS Service. It also provides the map_saver command-line utility, which allows dynamically generated maps to be saved to file. move_base : The move_base package provides an implementation of an action (see the actionlib package) that, given a goal in the world, will attempt to reach it with a mobile base. The move_base node links together a global and local planner to accomplish its global navigation task. It supports any global planner adhering to the nav_core::BaseGlobalPlanner interface specified in the nav_core package and any local planner adhering to the nav_core::BaseLocalPlanner interface specified in the nav_core package. The move_base node also maintains two costmaps, one for the global planner, and one for a local planner (see the costmap_2d package) that are used to accomplish navigation tasks. gmapping : This package contains a ROS wrapper for OpenSlam's Gmapping. The gmapping package provides laser-based SLAM (Simultaneous Localization and Mapping), as a ROS node called slam_gmapping. Using slam_gmapping, you can create a 2-D occupancy grid map (like a building floorplan) from laser and pose data collected by a mobile robot. ROS cartographer slam_toolbox Examples - TurtleBot3 Navigation","title":"Parameter Configuration"},{"location":"diffbot_navigation.html#navigation-in-gazebo-with-available-map","text":"To navigate the robot in the simulation run the following command but make sure to first download the turtlebot3_world to your ~/.gazebo/models/ folder. This is required because the turtlebot3_world.world file references the turtlebot3_world model. 1 roslaunch diffbot_navigation diffbot.launch world_name:='$(find diffbot_gazebo)/worlds/turtlebot3_world.world' This will spawn DiffBot inside the turtlebot3 world inside Gazebo and visualize the elements of the navigation stack in RViz. Navigation demo of DiffBot (click to view the demo on Youtube). To navigate the robot using the default DWA planner in the known map, coming from the running map_server , you can use the 2D Nav Goal in RViz . Just select the navigation arrow to where the robot should move as shown in the animation above. The DWA local planner is working for differential drive robots, like DiffBot. For other robots such as non-holonomic robots or other types of mobile robots (also differential drive robots) other planners can be used. See for example teb_local_planner .","title":"Navigation in Gazebo with available Map"},{"location":"diffbot_navigation.html#resources","text":"Global Planners: - global_planner Local Planners: - Difference between DWA and Base Local Planner - Difference between DWA and TEB Local Planner","title":"Resources"},{"location":"diffbot_perception.html","text":"Perception To provide perception capabilities to your mobile robot you need a sensor like a RGB or RGB-D camera as hardware and software like OpenCV or PCL . Quote OpenCV (Open Source Computer Vision Library) is a library of programming functions mainly aimed at real-time computer vision. Originally developed by Intel, it was later supported by Willow Garage then Itseez (which was later acquired by Intel). The library is cross-platform and free for use under the open-source Apache 2 License. Starting with 2011, OpenCV features GPU acceleration for real-time operations. Quote The Point Cloud Library (PCL) is a standalone, large scale, open project for 2D/3D image and point cloud processing. PCL is released under the terms of the BSD license, and thus free for commercial and research use. When working with OpenCV and ROS we have to install required dependency using ros-noetic-vision-opencv . This will install additional dependencies like OpenCV ( libopencv-dv ) and ros-noetic-cv-bridge . There is no need to install OpenCV from source or the Ubuntu binaries (deb package). If you want to use OpenCV without ROS you should consider installing it from source using CMake . This will allow you to configure what features should be installed, e.g., example code. Otherwise you can install a pre compiled binary libopencv-dev . Similar to work with PCL and ROS you need to install the dependency ros-noetic-perception-pcl , which is the interface between ROS and PCL. To work with PCL without ROS you should consider installing it from source using CMake . This will allow you to configure what features should be installed, e.g., example code. Otherwise you can install a pre compiled binary libpcl-dev . perception_pcl is a meta package for PCL (Point Cloud Library) ROS interface stack. PCL-ROS is the preferred bridge for 3D applications involving n-D Point Clouds and 3D geometry processing in ROS.","title":"Diffbot perception"},{"location":"diffbot_perception.html#perception","text":"To provide perception capabilities to your mobile robot you need a sensor like a RGB or RGB-D camera as hardware and software like OpenCV or PCL . Quote OpenCV (Open Source Computer Vision Library) is a library of programming functions mainly aimed at real-time computer vision. Originally developed by Intel, it was later supported by Willow Garage then Itseez (which was later acquired by Intel). The library is cross-platform and free for use under the open-source Apache 2 License. Starting with 2011, OpenCV features GPU acceleration for real-time operations. Quote The Point Cloud Library (PCL) is a standalone, large scale, open project for 2D/3D image and point cloud processing. PCL is released under the terms of the BSD license, and thus free for commercial and research use. When working with OpenCV and ROS we have to install required dependency using ros-noetic-vision-opencv . This will install additional dependencies like OpenCV ( libopencv-dv ) and ros-noetic-cv-bridge . There is no need to install OpenCV from source or the Ubuntu binaries (deb package). If you want to use OpenCV without ROS you should consider installing it from source using CMake . This will allow you to configure what features should be installed, e.g., example code. Otherwise you can install a pre compiled binary libopencv-dev . Similar to work with PCL and ROS you need to install the dependency ros-noetic-perception-pcl , which is the interface between ROS and PCL. To work with PCL without ROS you should consider installing it from source using CMake . This will allow you to configure what features should be installed, e.g., example code. Otherwise you can install a pre compiled binary libpcl-dev . perception_pcl is a meta package for PCL (Point Cloud Library) ROS interface stack. PCL-ROS is the preferred bridge for 3D applications involving n-D Point Clouds and 3D geometry processing in ROS.","title":"Perception"},{"location":"diffbot_robot.html","text":"DiffBot Robot The diffbot_robot package is a ROS metapackage that references all related packages to DiffBot. A metapackage can be created with: 1 2 3 4 fjp@ubuntu:ros_ws/src/$ catkin_create_pkg diffbot_robot --meta Created file diffbot_robot/package.xml Created file diffbot_robot/CMakeLists.txt Successfully created files in /home/fjp/git/ros_ws/src/diffbot/diffbot_robot. Please adjust the values in package.xml. To release a package see the bloom page and the listed tutorials there. Specifically the following ones: To index the package follow the Indexing Your ROS Repository for Documentation Generation . Release a package using bloom, see First Time Release tutorial .","title":"Robot Package"},{"location":"diffbot_robot.html#diffbot-robot","text":"The diffbot_robot package is a ROS metapackage that references all related packages to DiffBot. A metapackage can be created with: 1 2 3 4 fjp@ubuntu:ros_ws/src/$ catkin_create_pkg diffbot_robot --meta Created file diffbot_robot/package.xml Created file diffbot_robot/CMakeLists.txt Successfully created files in /home/fjp/git/ros_ws/src/diffbot/diffbot_robot. Please adjust the values in package.xml. To release a package see the bloom page and the listed tutorials there. Specifically the following ones: To index the package follow the Indexing Your ROS Repository for Documentation Generation . Release a package using bloom, see First Time Release tutorial .","title":"DiffBot Robot"},{"location":"diffbot_slam.html","text":"DiffBot Slam Package This package contains launch files and configurations for different simultaneous localization and mapping (SLAM) algorithms to map the environment of the robot in 2D, although some of these algorithms can be used to map in 3D. 1 2 3 4 5 fjp@diffbot:~/catkin_ws/src/diffbot$ catkin create pkg diffbot_slam --catkin-deps diffbot_navigation gmapping Creating package \"diffbot_slam\" in \"/home/fjp/git/ros_ws/src/diffbot\"... Created file diffbot_slam/package.xml Created file diffbot_slam/CMakeLists.txt Successfully created package files in /home/fjp/git/ros_ws/src/diffbot/diffbot_slam. Additional runtime dependencies are: cartographer_ros , hector_slam , frontier_exploration and explore_lite . These are added to this workspace using vcstool (TODO). As you can see this package has lots of dependencies to test different slam implementations and frontier exploration approaches. To run this package these dependencies need to be installed and are set as exec_depend in the package.xml . Currently only gmapping provides a ROS Noetic Ubuntu package that can be installed directly with: 1 sudo apt install ros-noetic-gmapping In case you want to try more advanced SLAM algorithms, such as karto_slam or cartographer_ros you need the following Ubuntu package dependencies. Alternatively you can install from source by building the cloned git repository in your catkin workspace. Take the required installation size into account. For example karto_slam needs approximately 125MB because it will also install ros-noetic-open-karto . 1 sudo apt install ros-noetic-slam-karto SLAM SLAM stands for Simultaneous Localization and Mapping sometimes refered to as Concurrent Localization and Mappping (CLAM). The SLAM algorithm combines localization and mapping, where a robot has access only to its own movement and sensory data. The robot must build a map while simultaneously localizing itself relative to the map. See also this blog post on FastSLAM . To use the following slam algorithms, we need a mobile robot that provides odometry data and is equipped with a horizontally-mounted, fixed, laser range-finder. Viewed on a higher level, every specific slam node of these algorithms will attempt to transform each incoming scan into the odom (odometry) tf frame. Therefore the node will subscribe to the laser /scan and the /tf topics. Transforms are necessary to relate frames for laser, base, and odometry. The only exception is hector_slam which doesn't require odometry for mapping. The following SLAM implementations are offered using the launch files explained in the next section. It is suggested to start with gmapping which is used by default. gmapping : This package contains a ROS wrapper for OpenSlam's Gmapping . The gmapping package provides laser-based SLAM (Simultaneous Localization and Mapping), as a ROS node called slam_gmapping . Using slam_gmapping , you can create a 2-D occupancy grid map (like a building floorplan) from laser and pose data collected by a mobile robot. cartographer : Cartographer is a system that provides real-time simultaneous localization and mapping (SLAM) in 2D and 3D across multiple platforms and sensor configurations. See the documentation for an algorithm walkthrough . karto : This package pulls in the Karto mapping library, and provides a ROS wrapper for using it. Karto is considered more accurate than, for example gmapping (note: for ROS noetic, see slam_karto ) and became open source in 2010 . hector_slam : metapackage that installs hector_mapping and related packages. The hector_mapping is a SLAM approach that can be used without odometry as well as on platforms that exhibit roll/pitch motion (of the sensor, the platform or both), such as drones. It leverages the high update rate of modern LIDAR systems like the Hokuyo UTM-30LX and provides 2D pose estimates at scan rate of the sensors (40Hz for the UTM-30LX). While the system does not provide explicit loop closing ability, it is sufficiently accurate for many real world scenarios. The system has successfully been used on Unmanned Ground Robots, Unmanned Surface Vehicles, Handheld Mapping Devices and logged data from quadrotor UAVs. Unlike gmapping which uses a particle filter , karto , cartographer and hector_slam are all graph-based SLAM algorithms . The least accurate SLAM algorithm is gmapping but it works fine for smaller maps. Use other algorithms, such as karto if you operate your robot in larger environments or you want more accuracy. Another interesing package is slam_toolbox which provides ROS1 and ROS2 support and is based on the easy to use karto algorithm. karto is the basis for many companies because it provides an excellent scan matcher and can operate in large environments. Additionally, slam_toolbox provides tools to edit a generated map and even create a high quality map using stored data (offline). The cartographer package is currently supported by OpenRobotics and not by Google where it was originally developed. It is currently also not setup correctly for DiffBot. Using it will result in errors. Launch files This package provides a main launch file named diffbot_slam.launch which accepts an argument slam_method . Depending on its value, different launch files will be included that execute the specified SLAM algorithm using its configuration in the config folder. As mentioned above, every ROS slam package requries messages from the laser-range finder topic. Usually this topic is named /scan . To distinguish possible multiple lidars, the topic of DiffBot resides in its namespace /diffbot/scan . Therefore, its necessary to remap the /scan topic to /diffbot/scan . The following shows how this was done for the gmapping launch file. Inside this package in the launch/gmapping.launch it is important to map the scan topic to laser scanner topic published by Diffbot. Remappings are done in the node tag . Here, for the gmapping.launch in the gmapping node: 1 2 3 4 5 6 7 8 9 10 11 <launch> <!-- Arguments --> <arg name= \"scan_topic\" default= \"diffbot/scan\" /> ... <!-- Gmapping --> <node pkg= \"gmapping\" type= \"slam_gmapping\" name= \"diffbot_slam_gmapping\" output= \"screen\" > ... <!-- remapping of gmapping node --> <remap from= \"scan\" to= \"$(arg scan_topic)\" /> </node> </launch> Parameter Configurations Most of the configrations are the same as turtlebot3_slam/config . For detailed description of what each parameter does, please check the individual package documentation of the different SLAM methods. Gazebo Simulation Tests To test SLAM in the Gazebo simulator run the following two launch files in separate terminals. First run the simulation with: 1 roslaunch diffbot_gazebo diffbot.launch world_name:='$(find diffbot_gazebo)/worlds/turtlebot3_world.world' and in a second terminal execute the SLAM algorithm: 1 roslaunch diffbot_slam diffbot_slam.launch slam_method:=gmapping Here you can choose between different algorithms by changing the value of the slam_method argument. Possible values are gmapping (the default), karto , hector and cartographer . The ROS node graph will look like the following: ROS Node graph after launching Gazebo and gmapping. In the figure we can see that gmapping subscribes and publishes to tf . It requires the transformation from <the frame attached to incoming scans> to the base_link , which is usually a fixed value, broadcast periodically by the robot_state_publisher . Aditionally, it requires the transform from base_link to odom . This is provided by the odometry system (e.g., the driver for the mobile base). In the case of DiffBot the odometry system consists of EKF fusion data from the motor encoders and the IMU. The provided tf transforms are map to odom that describes the current estimate of the robot's pose within the map frame. You can read more about the required and provided transforms in the documentation. Field Tests In case you get inaccurate maps follow the official ROS troubleshooting guide for navigation . Frontier Exploration The so far described mapping approaches require manually steering the robot in the unknown environment. Frontier exploration is an approach to move a mobile robot on its own to new frontiers to extend its map into new territory until the entire environment has been explored. The ROS wiki provides a good tutorial using Husky robot how to use the frontier_exploration package. A lightweight alternative is the explore_lite package. Other SLAM Packages (for 3D Mapping) hdl_graph_slam : Open source ROS package for real-time 6DOF SLAM using a 3D LIDAR. It is based on 3D Graph SLAM with NDT scan matching-based odometry estimation and loop detection. This method is useful for outdoor. RTAB-Map : stands for Real-Time Appearance-Based Mapping and is a RGB-D SLAM approach based on a global loop closure detector with real-time constraints. This package can be used to generate a 3D point clouds of the environment and/or to create a 2D occupancy grid map for navigation. To do this it requires only a stereo or RGB-D camera for visual odometry. Additional wheel odometry is not required but can improve the result. Loam Velodyne : Laser Odometry and Mapping (Loam) is a realtime method for state estimation and mapping using a 3D lidar, see also the forked Github repository for loam_velodyne . Note that this is not supported officially anymore because it became closed source. ORB-SLAM2 : Real-Time SLAM for Monocular, Stereo and RGB-D Cameras, with Loop Detection and Relocalization Capabilities. See orb_slam2_ros for the ROS wrapper. slam_toolbox : This package provides a sped up improved slam karto with updated SDK and visualization and modification toolsets. It is a ROS drop in replacement to gmapping, cartographer, karto, hector, etc. This package supports ROS1 and ROS2 and is suitable for use in commercial products because it can map large environments. And it provides tools to edit the generated maps. See also the related ROSCon 2019 video . References slam_toolbox , Slam Toolbox ROSCon 2019 pdf Papers: A Tutorial on Graph-Based SLAM cartographer Real-Time Loop Closure in 2D LIDAR SLAM hector_slam A flexible and scalable SLAM system with full 3D motion estimation . A practical introduction to to pose graph slam The Normal Distributions Transform: A New Approach to Laser Scan Matching RTAB-Map as an Open-Source Lidar and Visual SLAM Library for Large-Scale and Long-Term Online Operation LOAM: Lidar Odometry and Mapping in Real-time ORB-SLAM2: an Open-Source SLAM System for Monocular, Stereo and RGB-D Cameras","title":"SLAM"},{"location":"diffbot_slam.html#diffbot-slam-package","text":"This package contains launch files and configurations for different simultaneous localization and mapping (SLAM) algorithms to map the environment of the robot in 2D, although some of these algorithms can be used to map in 3D. 1 2 3 4 5 fjp@diffbot:~/catkin_ws/src/diffbot$ catkin create pkg diffbot_slam --catkin-deps diffbot_navigation gmapping Creating package \"diffbot_slam\" in \"/home/fjp/git/ros_ws/src/diffbot\"... Created file diffbot_slam/package.xml Created file diffbot_slam/CMakeLists.txt Successfully created package files in /home/fjp/git/ros_ws/src/diffbot/diffbot_slam. Additional runtime dependencies are: cartographer_ros , hector_slam , frontier_exploration and explore_lite . These are added to this workspace using vcstool (TODO). As you can see this package has lots of dependencies to test different slam implementations and frontier exploration approaches. To run this package these dependencies need to be installed and are set as exec_depend in the package.xml . Currently only gmapping provides a ROS Noetic Ubuntu package that can be installed directly with: 1 sudo apt install ros-noetic-gmapping In case you want to try more advanced SLAM algorithms, such as karto_slam or cartographer_ros you need the following Ubuntu package dependencies. Alternatively you can install from source by building the cloned git repository in your catkin workspace. Take the required installation size into account. For example karto_slam needs approximately 125MB because it will also install ros-noetic-open-karto . 1 sudo apt install ros-noetic-slam-karto","title":"DiffBot Slam Package"},{"location":"diffbot_slam.html#slam","text":"SLAM stands for Simultaneous Localization and Mapping sometimes refered to as Concurrent Localization and Mappping (CLAM). The SLAM algorithm combines localization and mapping, where a robot has access only to its own movement and sensory data. The robot must build a map while simultaneously localizing itself relative to the map. See also this blog post on FastSLAM . To use the following slam algorithms, we need a mobile robot that provides odometry data and is equipped with a horizontally-mounted, fixed, laser range-finder. Viewed on a higher level, every specific slam node of these algorithms will attempt to transform each incoming scan into the odom (odometry) tf frame. Therefore the node will subscribe to the laser /scan and the /tf topics. Transforms are necessary to relate frames for laser, base, and odometry. The only exception is hector_slam which doesn't require odometry for mapping. The following SLAM implementations are offered using the launch files explained in the next section. It is suggested to start with gmapping which is used by default. gmapping : This package contains a ROS wrapper for OpenSlam's Gmapping . The gmapping package provides laser-based SLAM (Simultaneous Localization and Mapping), as a ROS node called slam_gmapping . Using slam_gmapping , you can create a 2-D occupancy grid map (like a building floorplan) from laser and pose data collected by a mobile robot. cartographer : Cartographer is a system that provides real-time simultaneous localization and mapping (SLAM) in 2D and 3D across multiple platforms and sensor configurations. See the documentation for an algorithm walkthrough . karto : This package pulls in the Karto mapping library, and provides a ROS wrapper for using it. Karto is considered more accurate than, for example gmapping (note: for ROS noetic, see slam_karto ) and became open source in 2010 . hector_slam : metapackage that installs hector_mapping and related packages. The hector_mapping is a SLAM approach that can be used without odometry as well as on platforms that exhibit roll/pitch motion (of the sensor, the platform or both), such as drones. It leverages the high update rate of modern LIDAR systems like the Hokuyo UTM-30LX and provides 2D pose estimates at scan rate of the sensors (40Hz for the UTM-30LX). While the system does not provide explicit loop closing ability, it is sufficiently accurate for many real world scenarios. The system has successfully been used on Unmanned Ground Robots, Unmanned Surface Vehicles, Handheld Mapping Devices and logged data from quadrotor UAVs. Unlike gmapping which uses a particle filter , karto , cartographer and hector_slam are all graph-based SLAM algorithms . The least accurate SLAM algorithm is gmapping but it works fine for smaller maps. Use other algorithms, such as karto if you operate your robot in larger environments or you want more accuracy. Another interesing package is slam_toolbox which provides ROS1 and ROS2 support and is based on the easy to use karto algorithm. karto is the basis for many companies because it provides an excellent scan matcher and can operate in large environments. Additionally, slam_toolbox provides tools to edit a generated map and even create a high quality map using stored data (offline). The cartographer package is currently supported by OpenRobotics and not by Google where it was originally developed. It is currently also not setup correctly for DiffBot. Using it will result in errors.","title":"SLAM"},{"location":"diffbot_slam.html#launch-files","text":"This package provides a main launch file named diffbot_slam.launch which accepts an argument slam_method . Depending on its value, different launch files will be included that execute the specified SLAM algorithm using its configuration in the config folder. As mentioned above, every ROS slam package requries messages from the laser-range finder topic. Usually this topic is named /scan . To distinguish possible multiple lidars, the topic of DiffBot resides in its namespace /diffbot/scan . Therefore, its necessary to remap the /scan topic to /diffbot/scan . The following shows how this was done for the gmapping launch file. Inside this package in the launch/gmapping.launch it is important to map the scan topic to laser scanner topic published by Diffbot. Remappings are done in the node tag . Here, for the gmapping.launch in the gmapping node: 1 2 3 4 5 6 7 8 9 10 11 <launch> <!-- Arguments --> <arg name= \"scan_topic\" default= \"diffbot/scan\" /> ... <!-- Gmapping --> <node pkg= \"gmapping\" type= \"slam_gmapping\" name= \"diffbot_slam_gmapping\" output= \"screen\" > ... <!-- remapping of gmapping node --> <remap from= \"scan\" to= \"$(arg scan_topic)\" /> </node> </launch>","title":"Launch files"},{"location":"diffbot_slam.html#parameter-configurations","text":"Most of the configrations are the same as turtlebot3_slam/config . For detailed description of what each parameter does, please check the individual package documentation of the different SLAM methods.","title":"Parameter Configurations"},{"location":"diffbot_slam.html#gazebo-simulation-tests","text":"To test SLAM in the Gazebo simulator run the following two launch files in separate terminals. First run the simulation with: 1 roslaunch diffbot_gazebo diffbot.launch world_name:='$(find diffbot_gazebo)/worlds/turtlebot3_world.world' and in a second terminal execute the SLAM algorithm: 1 roslaunch diffbot_slam diffbot_slam.launch slam_method:=gmapping Here you can choose between different algorithms by changing the value of the slam_method argument. Possible values are gmapping (the default), karto , hector and cartographer . The ROS node graph will look like the following: ROS Node graph after launching Gazebo and gmapping. In the figure we can see that gmapping subscribes and publishes to tf . It requires the transformation from <the frame attached to incoming scans> to the base_link , which is usually a fixed value, broadcast periodically by the robot_state_publisher . Aditionally, it requires the transform from base_link to odom . This is provided by the odometry system (e.g., the driver for the mobile base). In the case of DiffBot the odometry system consists of EKF fusion data from the motor encoders and the IMU. The provided tf transforms are map to odom that describes the current estimate of the robot's pose within the map frame. You can read more about the required and provided transforms in the documentation.","title":"Gazebo Simulation Tests"},{"location":"diffbot_slam.html#field-tests","text":"In case you get inaccurate maps follow the official ROS troubleshooting guide for navigation .","title":"Field Tests"},{"location":"diffbot_slam.html#frontier-exploration","text":"The so far described mapping approaches require manually steering the robot in the unknown environment. Frontier exploration is an approach to move a mobile robot on its own to new frontiers to extend its map into new territory until the entire environment has been explored. The ROS wiki provides a good tutorial using Husky robot how to use the frontier_exploration package. A lightweight alternative is the explore_lite package.","title":"Frontier Exploration"},{"location":"diffbot_slam.html#other-slam-packages-for-3d-mapping","text":"hdl_graph_slam : Open source ROS package for real-time 6DOF SLAM using a 3D LIDAR. It is based on 3D Graph SLAM with NDT scan matching-based odometry estimation and loop detection. This method is useful for outdoor. RTAB-Map : stands for Real-Time Appearance-Based Mapping and is a RGB-D SLAM approach based on a global loop closure detector with real-time constraints. This package can be used to generate a 3D point clouds of the environment and/or to create a 2D occupancy grid map for navigation. To do this it requires only a stereo or RGB-D camera for visual odometry. Additional wheel odometry is not required but can improve the result. Loam Velodyne : Laser Odometry and Mapping (Loam) is a realtime method for state estimation and mapping using a 3D lidar, see also the forked Github repository for loam_velodyne . Note that this is not supported officially anymore because it became closed source. ORB-SLAM2 : Real-Time SLAM for Monocular, Stereo and RGB-D Cameras, with Loop Detection and Relocalization Capabilities. See orb_slam2_ros for the ROS wrapper. slam_toolbox : This package provides a sped up improved slam karto with updated SDK and visualization and modification toolsets. It is a ROS drop in replacement to gmapping, cartographer, karto, hector, etc. This package supports ROS1 and ROS2 and is suitable for use in commercial products because it can map large environments. And it provides tools to edit the generated maps. See also the related ROSCon 2019 video .","title":"Other SLAM Packages (for 3D Mapping)"},{"location":"diffbot_slam.html#references","text":"slam_toolbox , Slam Toolbox ROSCon 2019 pdf Papers: A Tutorial on Graph-Based SLAM cartographer Real-Time Loop Closure in 2D LIDAR SLAM hector_slam A flexible and scalable SLAM system with full 3D motion estimation . A practical introduction to to pose graph slam The Normal Distributions Transform: A New Approach to Laser Scan Matching RTAB-Map as an Open-Source Lidar and Visual SLAM Library for Large-Scale and Long-Term Online Operation LOAM: Lidar Odometry and Mapping in Real-time ORB-SLAM2: an Open-Source SLAM System for Monocular, Stereo and RGB-D Cameras","title":"References"},{"location":"git-setup.html","text":"git setup Install git on Ubuntu via the following command: fjp@ubuntu : ~/git/2wd-robot $ sudo apt install git Set your username and email address that you use on github (when using github to host your repository): fjp@ubuntu : ~ $ git config --global user.name \"fjp\" fjp@ubuntu : ~ $ git config --global user.email \"franz.pucher@gmail.com\" To store your password credentials when pushing and pulling to the remote repository use the following commands: fjp@ubuntu : ~/git/2wd-robot $ git config --global credential.helper store fjp@ubuntu : ~/git/2wd-robot $ git push Username for 'https://github.com': fjp Password for 'https://fjp@github.com': Everything up-to-date fjp@ubuntu : ~/git/2wd-robot $ git push Everything up-to-date","title":"Git Setup"},{"location":"git-setup.html#git-setup","text":"Install git on Ubuntu via the following command: fjp@ubuntu : ~/git/2wd-robot $ sudo apt install git Set your username and email address that you use on github (when using github to host your repository): fjp@ubuntu : ~ $ git config --global user.name \"fjp\" fjp@ubuntu : ~ $ git config --global user.email \"franz.pucher@gmail.com\" To store your password credentials when pushing and pulling to the remote repository use the following commands: fjp@ubuntu : ~/git/2wd-robot $ git config --global credential.helper store fjp@ubuntu : ~/git/2wd-robot $ git push Username for 'https://github.com': fjp Password for 'https://fjp@github.com': Everything up-to-date fjp@ubuntu : ~/git/2wd-robot $ git push Everything up-to-date","title":"git setup"},{"location":"grove_motor_driver.html","text":"Grove - I2C Motor Driver V1.3 The package for the Grove I2C Motor Driver V1.3 from Seeed Studio is created with catkin create pkg PKG_NAME [--catkin-deps [DEP [DEP ...]]] : 1 2 3 4 5 6 7 fjp@ubuntu:~/git/diffbot/ros/src$ catkin create pkg grove_motor_driver --catkin-deps rospy roscpp geometry_msgs Creating package \"grove_motor_driver\" in \"/home/fjp/git/diffbot/ros/src\"... Created file grove_motor_driver/CMakeLists.txt Created file grove_motor_driver/package.xml Created folder grove_motor_driver/include/grove_motor_driver Created folder grove_motor_driver/src Successfully created package files in /home/fjp/git/diffbot/ros/src/grove_motor_driver. The package depends on the two ROS client libraries rospy and roscpp . To control the two motors the package will use the geometry_msgs/Twist message. The interface to the motor driver is done with the Python library from DexterInd which is a rewerite of the Seeed Studio Arduino library . This library requires the following two python libraries RPi.GPIO smbus SMBus (System Management Bus) is a subset from the I2C protocol These libraries should be installed with pip3 , Python's package manager: 1 2 pip3 install RPi.GPIO pip3 install smbus Note that this will install these packages system wide. This is ok because they are installed on the Raspberry Pi which is dedicated to operate for this purpose. For a development environment it is best practice to use a python virtual environment like venv and install the packages inside it. Connection Afterwards the I2C port of the motor should be connected to the I2C port 1 of the Raspberry Pi 4 B. Don't forget to remove the jumper on the motor driver board which would provide power to the Pi. However, it is also not required to connect VCC and GND of the I2C connector. Only the SDA (data) and SCL (clock) wires are required. Make sure to set the address with the dip switches on the motor driver to 0x0f because this is the default address used in the library files. To test the physical I2C connection use i2cdetect described in Hardware Interfaces : The output should list 0f in the address table: 1 2 3 4 5 6 7 8 9 10 $ i2cdetect -y 1 0 1 2 3 4 5 6 7 8 9 a b c d e f 00: -- -- -- -- -- -- -- -- -- -- -- -- 0f 10: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 20: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 30: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 40: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 50: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 60: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 70: -- -- -- -- -- -- -- -- Test Motor Driver Test the motor driver by running one of the python files: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 fjp@ubuntu:~/git/diffbot/ros/src/grove_motor_driver/src$ python3 motor_example.py Forward Back Stop Speed: 0 Speed: 1 Speed: 2 Speed: 3 Speed: 4 Speed: 5 Speed: 6 Speed: 7 Speed: 8 Speed: 9 Speed: 10 Speed: 11 Speed: 12 ... Speed: 25 Speed: 26 Speed: 27 Speed: 28 Speed: 29 Speed: 30 Speed: 31 Speed: 32 Speed: 33 ... Speed: 55 Speed: 56 Speed: 57 Speed: 58 Speed: 59 Speed: 60 Speed: 61 Speed: 62 Speed: 63 Speed: 64 Speed: 65 Speed: 66 ... Speed: 75 Speed: 76 Speed: 77 Speed: 78 Speed: 79 Speed: 80 Speed: 81 Speed: 82 Speed: 83 Speed: 84 Speed: 85 Speed: 86 Speed: 87 Speed: 88 ... Speed: 97 Speed: 98 Speed: 99 Stop Troubleshooting If you get errors like the following, make sure the I2C cables from the motor driver to the Raspberry Pi are connected (see Hardware Interfaces for more infos) and use the RESET button on the motor driver. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 fjp@ubuntu:~/git/2wd-robot/ros/src/control$ sudo python grove_i2c_motor_driver.py Traceback (most recent call last): File \"grove_i2c_motor_driver.py\", line 68, in <module> m.MotorSpeedSetAB(100,100) File \"grove_i2c_motor_driver.py\", line 57, in MotorSpeedSetAB bus.write_i2c_block_data(self.I2CMotorDriverAdd, self.MotorSpeedSet, [MotorSpeedA,MotorSpeedB]) IOError: [Errno 110] Connection timed out fjp@ubuntu:~/git/2wd-robot/ros/src/control$ sudo python grove_i2c_motor_driver.py Traceback (most recent call last): File \"grove_i2c_motor_driver.py\", line 68, in <module> m.MotorSpeedSetAB(100,100) File \"grove_i2c_motor_driver.py\", line 57, in MotorSpeedSetAB bus.write_i2c_block_data(self.I2CMotorDriverAdd, self.MotorSpeedSet, [MotorSpeedA,MotorSpeedB]) IOError: [Errno 121] Remote I/O error Try pressing the RESET button and release it right before executing one of the scripts. You should also be able to detect the motor driver with i2cdetect -y 1 : 1 2 3 4 5 6 7 8 9 10 fjp@ubuntu:~/git/2wd-robot/ros/src/control/src$ sudo i2cdetect -y 1 0 1 2 3 4 5 6 7 8 9 a b c d e f 00: -- -- -- -- -- -- -- -- -- -- -- -- 0f 10: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 20: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 30: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 40: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 50: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 60: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 70: -- -- -- -- -- -- -- -- As you can see the address of the motor driver is detected at 0x0f . In case of the following output, where every address of the I2C bus seems to be taken, it is most likely that the SDA (data) and SCL (clock) signal cables are switched: 1 2 3 4 5 6 7 8 9 10 i2cdetect -y 1 0 1 2 3 4 5 6 7 8 9 a b c d e f 00: 03 04 05 06 07 08 09 0a 0b 0c 0d 0e 0f 10: 10 11 12 13 14 15 16 17 18 19 1a 1b 1c 1d 1e 1f 20: 20 21 22 23 24 25 26 27 28 29 2a 2b 2c 2d 2e 2f 30: 30 31 32 33 34 35 36 37 38 39 3a 3b 3c 3d 3e 3f 40: 40 41 42 43 44 45 46 47 48 49 4a 4b 4c 4d 4e 4f 50: 50 51 52 53 54 55 56 57 58 59 5a 5b 5c 5d 5e 5f 60: 60 61 62 63 64 65 66 67 68 69 6a 6b 6c 6d 6e 6f 70: 70 71 72 73 74 75 76 77 To solve this make sure the SDA and SCL cables are connected to the right pins on the Raspberry Pi. See Hardware Interfaces for more infos. Another solution is to restart the Raspberry Pi while making sure that the motor driver is powerd on by connecting it to the battery pack. ROS Node for Motor Driver To use the available library of the Grove I2C motor driver in ROS we need to create a wrapper node, called motor_driver . The node subscribes to the topic /2wd_robot/cmd_vel which is of type Twist message from the geometry_msgs header. To send commands to the motor the . These topics can be published with nodes from the navigation stack or with rostopic pub for test purposes. https://en.wikipedia.org/wiki/Differential_wheeled_robot http://wiki.ros.org/differential_drive After the node has been implemented, we need to build the workspace with catkin build : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 fjp@ubuntu:~/git/2wd-robot/ros$ catkin build ---------------------------------------------------------------- Profile: default Extending: [cached] /opt/ros/melodic Workspace: /home/fjp/git/2wd-robot/ros ---------------------------------------------------------------- Build Space: [exists] /home/fjp/git/2wd-robot/ros/build Devel Space: [exists] /home/fjp/git/2wd-robot/ros/devel Install Space: [unused] /home/fjp/git/2wd-robot/ros/install Log Space: [exists] /home/fjp/git/2wd-robot/ros/logs Source Space: [exists] /home/fjp/git/2wd-robot/ros/src DESTDIR: [unused] None ---------------------------------------------------------------- Devel Space Layout: linked Install Space Layout: None ---------------------------------------------------------------- Additional CMake Args: None Additional Make Args: None Additional catkin Make Args: None Internal Make Job Server: True Cache Job Environments: False ---------------------------------------------------------------- Whitelisted Packages: None Blacklisted Packages: None ---------------------------------------------------------------- Workspace configuration appears valid. ---------------------------------------------------------------- [build] Found '2' packages in 0.0 seconds. [build] Updating package table. Starting >>> grove_motor_driver Starting >>> grove_ultrasonic_ranger Finished <<< grove_motor_driver [ 1.0 seconds ] Finished <<< grove_ultrasonic_ranger [ 1.0 seconds ] [build] Summary: All 2 packages succeeded! [build] Ignored: None. [build] Warnings: None. [build] Abandoned: None. [build] Failed: None. [build] Runtime: 2.0 seconds total. As the final note of the build output suggests, we have to source the setup.bash files in the devel space. 1 fjp@ubuntu:~/git/2wd-robot/ros$ source devel/setup.bash To make the ranger node executable we have to modify the ranger.py file: 1 fjp@ubuntu:~/git/2wd-robot/ros/src/grove_ultrasonic_ranger/src$ sudo chmod a+x ranger.py","title":"Motor Driver"},{"location":"grove_motor_driver.html#grove-i2c-motor-driver-v13","text":"The package for the Grove I2C Motor Driver V1.3 from Seeed Studio is created with catkin create pkg PKG_NAME [--catkin-deps [DEP [DEP ...]]] : 1 2 3 4 5 6 7 fjp@ubuntu:~/git/diffbot/ros/src$ catkin create pkg grove_motor_driver --catkin-deps rospy roscpp geometry_msgs Creating package \"grove_motor_driver\" in \"/home/fjp/git/diffbot/ros/src\"... Created file grove_motor_driver/CMakeLists.txt Created file grove_motor_driver/package.xml Created folder grove_motor_driver/include/grove_motor_driver Created folder grove_motor_driver/src Successfully created package files in /home/fjp/git/diffbot/ros/src/grove_motor_driver. The package depends on the two ROS client libraries rospy and roscpp . To control the two motors the package will use the geometry_msgs/Twist message. The interface to the motor driver is done with the Python library from DexterInd which is a rewerite of the Seeed Studio Arduino library . This library requires the following two python libraries RPi.GPIO smbus SMBus (System Management Bus) is a subset from the I2C protocol These libraries should be installed with pip3 , Python's package manager: 1 2 pip3 install RPi.GPIO pip3 install smbus Note that this will install these packages system wide. This is ok because they are installed on the Raspberry Pi which is dedicated to operate for this purpose. For a development environment it is best practice to use a python virtual environment like venv and install the packages inside it.","title":"Grove - I2C Motor Driver V1.3"},{"location":"grove_motor_driver.html#connection","text":"Afterwards the I2C port of the motor should be connected to the I2C port 1 of the Raspberry Pi 4 B. Don't forget to remove the jumper on the motor driver board which would provide power to the Pi. However, it is also not required to connect VCC and GND of the I2C connector. Only the SDA (data) and SCL (clock) wires are required. Make sure to set the address with the dip switches on the motor driver to 0x0f because this is the default address used in the library files. To test the physical I2C connection use i2cdetect described in Hardware Interfaces : The output should list 0f in the address table: 1 2 3 4 5 6 7 8 9 10 $ i2cdetect -y 1 0 1 2 3 4 5 6 7 8 9 a b c d e f 00: -- -- -- -- -- -- -- -- -- -- -- -- 0f 10: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 20: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 30: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 40: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 50: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 60: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 70: -- -- -- -- -- -- -- --","title":"Connection"},{"location":"grove_motor_driver.html#test-motor-driver","text":"Test the motor driver by running one of the python files: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 fjp@ubuntu:~/git/diffbot/ros/src/grove_motor_driver/src$ python3 motor_example.py Forward Back Stop Speed: 0 Speed: 1 Speed: 2 Speed: 3 Speed: 4 Speed: 5 Speed: 6 Speed: 7 Speed: 8 Speed: 9 Speed: 10 Speed: 11 Speed: 12 ... Speed: 25 Speed: 26 Speed: 27 Speed: 28 Speed: 29 Speed: 30 Speed: 31 Speed: 32 Speed: 33 ... Speed: 55 Speed: 56 Speed: 57 Speed: 58 Speed: 59 Speed: 60 Speed: 61 Speed: 62 Speed: 63 Speed: 64 Speed: 65 Speed: 66 ... Speed: 75 Speed: 76 Speed: 77 Speed: 78 Speed: 79 Speed: 80 Speed: 81 Speed: 82 Speed: 83 Speed: 84 Speed: 85 Speed: 86 Speed: 87 Speed: 88 ... Speed: 97 Speed: 98 Speed: 99 Stop","title":"Test Motor Driver"},{"location":"grove_motor_driver.html#troubleshooting","text":"If you get errors like the following, make sure the I2C cables from the motor driver to the Raspberry Pi are connected (see Hardware Interfaces for more infos) and use the RESET button on the motor driver. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 fjp@ubuntu:~/git/2wd-robot/ros/src/control$ sudo python grove_i2c_motor_driver.py Traceback (most recent call last): File \"grove_i2c_motor_driver.py\", line 68, in <module> m.MotorSpeedSetAB(100,100) File \"grove_i2c_motor_driver.py\", line 57, in MotorSpeedSetAB bus.write_i2c_block_data(self.I2CMotorDriverAdd, self.MotorSpeedSet, [MotorSpeedA,MotorSpeedB]) IOError: [Errno 110] Connection timed out fjp@ubuntu:~/git/2wd-robot/ros/src/control$ sudo python grove_i2c_motor_driver.py Traceback (most recent call last): File \"grove_i2c_motor_driver.py\", line 68, in <module> m.MotorSpeedSetAB(100,100) File \"grove_i2c_motor_driver.py\", line 57, in MotorSpeedSetAB bus.write_i2c_block_data(self.I2CMotorDriverAdd, self.MotorSpeedSet, [MotorSpeedA,MotorSpeedB]) IOError: [Errno 121] Remote I/O error Try pressing the RESET button and release it right before executing one of the scripts. You should also be able to detect the motor driver with i2cdetect -y 1 : 1 2 3 4 5 6 7 8 9 10 fjp@ubuntu:~/git/2wd-robot/ros/src/control/src$ sudo i2cdetect -y 1 0 1 2 3 4 5 6 7 8 9 a b c d e f 00: -- -- -- -- -- -- -- -- -- -- -- -- 0f 10: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 20: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 30: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 40: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 50: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 60: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 70: -- -- -- -- -- -- -- -- As you can see the address of the motor driver is detected at 0x0f . In case of the following output, where every address of the I2C bus seems to be taken, it is most likely that the SDA (data) and SCL (clock) signal cables are switched: 1 2 3 4 5 6 7 8 9 10 i2cdetect -y 1 0 1 2 3 4 5 6 7 8 9 a b c d e f 00: 03 04 05 06 07 08 09 0a 0b 0c 0d 0e 0f 10: 10 11 12 13 14 15 16 17 18 19 1a 1b 1c 1d 1e 1f 20: 20 21 22 23 24 25 26 27 28 29 2a 2b 2c 2d 2e 2f 30: 30 31 32 33 34 35 36 37 38 39 3a 3b 3c 3d 3e 3f 40: 40 41 42 43 44 45 46 47 48 49 4a 4b 4c 4d 4e 4f 50: 50 51 52 53 54 55 56 57 58 59 5a 5b 5c 5d 5e 5f 60: 60 61 62 63 64 65 66 67 68 69 6a 6b 6c 6d 6e 6f 70: 70 71 72 73 74 75 76 77 To solve this make sure the SDA and SCL cables are connected to the right pins on the Raspberry Pi. See Hardware Interfaces for more infos. Another solution is to restart the Raspberry Pi while making sure that the motor driver is powerd on by connecting it to the battery pack.","title":"Troubleshooting"},{"location":"grove_motor_driver.html#ros-node-for-motor-driver","text":"To use the available library of the Grove I2C motor driver in ROS we need to create a wrapper node, called motor_driver . The node subscribes to the topic /2wd_robot/cmd_vel which is of type Twist message from the geometry_msgs header. To send commands to the motor the . These topics can be published with nodes from the navigation stack or with rostopic pub for test purposes. https://en.wikipedia.org/wiki/Differential_wheeled_robot http://wiki.ros.org/differential_drive After the node has been implemented, we need to build the workspace with catkin build : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 fjp@ubuntu:~/git/2wd-robot/ros$ catkin build ---------------------------------------------------------------- Profile: default Extending: [cached] /opt/ros/melodic Workspace: /home/fjp/git/2wd-robot/ros ---------------------------------------------------------------- Build Space: [exists] /home/fjp/git/2wd-robot/ros/build Devel Space: [exists] /home/fjp/git/2wd-robot/ros/devel Install Space: [unused] /home/fjp/git/2wd-robot/ros/install Log Space: [exists] /home/fjp/git/2wd-robot/ros/logs Source Space: [exists] /home/fjp/git/2wd-robot/ros/src DESTDIR: [unused] None ---------------------------------------------------------------- Devel Space Layout: linked Install Space Layout: None ---------------------------------------------------------------- Additional CMake Args: None Additional Make Args: None Additional catkin Make Args: None Internal Make Job Server: True Cache Job Environments: False ---------------------------------------------------------------- Whitelisted Packages: None Blacklisted Packages: None ---------------------------------------------------------------- Workspace configuration appears valid. ---------------------------------------------------------------- [build] Found '2' packages in 0.0 seconds. [build] Updating package table. Starting >>> grove_motor_driver Starting >>> grove_ultrasonic_ranger Finished <<< grove_motor_driver [ 1.0 seconds ] Finished <<< grove_ultrasonic_ranger [ 1.0 seconds ] [build] Summary: All 2 packages succeeded! [build] Ignored: None. [build] Warnings: None. [build] Abandoned: None. [build] Failed: None. [build] Runtime: 2.0 seconds total. As the final note of the build output suggests, we have to source the setup.bash files in the devel space. 1 fjp@ubuntu:~/git/2wd-robot/ros$ source devel/setup.bash To make the ranger node executable we have to modify the ranger.py file: 1 fjp@ubuntu:~/git/2wd-robot/ros/src/grove_ultrasonic_ranger/src$ sudo chmod a+x ranger.py","title":"ROS Node for Motor Driver"},{"location":"grove_ultrasonic_ranger.html","text":"Grove - Ultrasonic Ranger To avoid obstacles the Grove ultrasonic ranger from Seeed Studio is used. We will create a ROS package with catkin create pkg PKG_NAME [--catkin-deps [DEP [DEP ...]]] : 1 2 3 4 5 6 fjp@ubuntu:~/git/2wd-robot/ros/src$ catkin create pkg grove_ultrasonic_ranger --catkin-deps rospy roscpp sensor_msgs Creating package \"grove_ultrasonic_ranger\" in \"/home/fjp/git/2wd-robot/ros/src\" ... Created file grove_ultrasonic_ranger/CMakeLists.txt Created file grove_ultrasonic_ranger/package.xml Created folder grove_ultrasonic_ranger/include/grove_ultrasonic_ranger Created folder grove_ultrasonic_ranger/src The package depends on the two ROS client libraries rospy and roscpp . To signalise the current distance to obstacles the sensor_msgs/Range message is used. After connecting the signal pin of the sensor to (physical) GPIO 11 of the Raspberry Pi 4 B and power it with 5V and ground, we can test its functionality with the available python script ultrasonic.py from Seed Studio. The following shows the truncated output of the ultrasonic.py script when moving an obstacle in front of the sensor. We see that the distance value changes as expected. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 fjp@ubuntu:~/git/2wd-robot/ros/src/grove_ultrasonic_ranger/src$ sudo python ultrasonic.py SeeedStudio Grove Ultrasonic get data and print Ultrasonic Measurement Distance : 2 .0 CM Ultrasonic Measurement Distance : 2 .1 CM Ultrasonic Measurement Distance : 2 .0 CM Ultrasonic Measurement Distance : 3 .5 CM Ultrasonic Measurement Distance : 3 .5 CM Ultrasonic Measurement Distance : 2 .4 CM Ultrasonic Measurement Distance : 12 .9 CM ... Ultrasonic Measurement Distance : 45 .8 CM Ultrasonic Measurement Distance : 510 .7 CM Ultrasonic Measurement Distance : 510 .7 CM Ultrasonic Measurement Distance : 45 .7 CM Ultrasonic Measurement Distance : 161 .8 CM Ultrasonic Measurement Distance : 26 .9 CM Ultrasonic Measurement Distance : 18 .3 CM Ultrasonic Measurement Distance : 160 .9 CM Ultrasonic Measurement Distance : 158 .3 CM Ultrasonic Measurement Distance : 159 .4 CM ... Modified GroveUltrasonicRanger Library To use the ultrasonic ranger as a ROS node it is convenient to wrap the sensor functionality in a class. This provides an easy to extend interface for the ultrasonic ranger ( API ) Therefore I copied the core functionality of the ultrasonic.py script from Seeed Studio in a class named GroveUltrasonicRanger . Executing the grove_ultrasonic_ranger.py will result in the following output: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 fjp@ubuntu:~/git/2wd-robot/ros/src/grove_ultrasonic_ranger/src$ sudo python grove_ultrasonic_ranger.py SeeedStudio Grove Ultrasonic get data and print Distance : 0 .051 m Distance : 0 .069 m Distance : 0 .098 m Distance : 0 .131 m Distance : 0 .153 m Distance : 0 .172 m Distance : 0 .207 m Distance : 0 .210 m Distance : 0 .234 m Distance : 0 .256 m GPIO.cleanup () GPIO.cleanup () done ROS Node for Ultrasonic Ranger ROS provides the Range Message in the sensor_msgs header . This message type can be used to write a wrapper that will act as a ROS node for the Grove ultrasonic sensor. To design this node we will send out measurements periodically over a topic of type sensor_msgs/Range . The code for this node is in ranger.py . After writing the node we need to build the packages in the workspace with catkin build . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 fjp@ubuntu:~/git/2wd-robot/ros$ catkin build ---------------------------------------------------------------- Profile: default Extending: [ env ] /opt/ros/melodic Workspace: /home/fjp/git/2wd-robot/ros ---------------------------------------------------------------- Build Space: [ exists ] /home/fjp/git/2wd-robot/ros/build Devel Space: [ exists ] /home/fjp/git/2wd-robot/ros/devel Install Space: [ unused ] /home/fjp/git/2wd-robot/ros/install Log Space: [ missing ] /home/fjp/git/2wd-robot/ros/logs Source Space: [ exists ] /home/fjp/git/2wd-robot/ros/src DESTDIR: [ unused ] None ---------------------------------------------------------------- Devel Space Layout: linked Install Space Layout: None ---------------------------------------------------------------- Additional CMake Args: None Additional Make Args: None Additional catkin Make Args: None Internal Make Job Server: True Cache Job Environments: False ---------------------------------------------------------------- Whitelisted Packages: None Blacklisted Packages: None ---------------------------------------------------------------- Workspace configuration appears valid. NOTE: Forcing CMake to run for each package. ---------------------------------------------------------------- [ build ] Found '1' package in 0 .0 seconds. [ build ] Updating package table. Starting >>> catkin_tools_prebuild Finished <<< catkin_tools_prebuild [ 9 .9 seconds ] Starting >>> grove_ultrasonic_ranger Finished <<< grove_ultrasonic_ranger [ 12 .0 seconds ] [ build ] Summary: All 2 packages succeeded! [ build ] Ignored: None. [ build ] Warnings: None. [ build ] Abandoned: None. [ build ] Failed: None. [ build ] Runtime: 21 .9 seconds total. [ build ] Note: Workspace packages have changed, please re-source setup files to use them. As the final note of the build output suggests, we have to source the setup.bash files in the devel space. 1 fjp@ubuntu:~/git/2wd-robot/ros$ source devel/setup.bash To make the ranger node executable we have to modify the ranger.py file: 1 fjp@ubuntu:~/git/2wd-robot/ros/src/grove_ultrasonic_ranger/src$ sudo chmod a+x ranger.py Then we can test the node using rosrun : 1 2 3 4 5 6 7 8 9 10 fjp@ubuntu:~/git/2wd-robot/ros$ sudo su [ sudo ] password for fjp: root@ubuntu:/home/fjp/git/2wd-robot/ros# source devel/setup.bash root@ubuntu:/home/fjp/git/2wd-robot/ros# rosrun grove_ultrasonic_ranger ranger.py Distance : 1 .617 m Distance : 1 .617 m Distance : 1 .617 m Distance : 0 .108 m Distance : 0 .092 m Distance : 0 .099 m This lets the node publish range messages which we can capture in another terminal window using rostopic . First we use rostopic list to find the name of the topic we are interested in: 1 2 3 4 fjp@ubuntu:~/git/2wd-robot/ros$ rostopic list /distance /rosout /rosout_agg We named our topic /distance which we can use with the rostopic echo command to see the published messages: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 fjp@ubuntu:~/git/2wd-robot/ros$ rostopic echo /distance header: seq: 1 stamp: secs: 1576778377 nsecs: 746809959 frame_id: \"ranger_distance\" radiation_type: 0 field_of_view: 0 .261799007654 min_range: 0 .019999999553 max_range: 3 .5 range: 1 .61674261093 --- header: seq: 2 stamp: secs: 1576778378 nsecs: 459048986 frame_id: \"ranger_distance\" radiation_type: 0 field_of_view: 0 .261799007654 min_range: 0 .019999999553 max_range: 3 .5 range: 1 .61261284351 --- header: seq: 3 stamp: secs: 1576778379 nsecs: 172172069 frame_id: \"ranger_distance\" radiation_type: 0 field_of_view: 0 .261799007654 min_range: 0 .019999999553 max_range: 3 .5 range: 1 .61657905579 --- header: seq: 4 stamp: secs: 1576778379 nsecs: 884002923 frame_id: \"ranger_distance\" radiation_type: 0 field_of_view: 0 .261799007654 min_range: 0 .019999999553 max_range: 3 .5 range: 1 .61277639866 --- header: seq: 5 stamp: secs: 1576778380 nsecs: 596549034 frame_id: \"ranger_distance\" radiation_type: 0 field_of_view: 0 .261799007654 min_range: 0 .019999999553 max_range: 3 .5 range: 1 .67693090439 --- Informational Distance Measurements To provide additional information when an obstacle is too close or the robot has no obstacle in front of it we use REP-117 as guideline. To successfully implement this REP, checks for valid measurements should use floating-point standards and follow this form: 1 2 3 4 5 6 7 8 9 10 11 if ( minimum_range <= value && value <= maximum_range ){ // Represents expected pre-REP logic and is the only necessary condition for most applications. // This is a valid measurement. } else if ( ! isfinite ( value ) && value < 0 ){ // Object too close to measure. } else if ( ! isfinite ( value ) && value > 0 ){ // No objects detected in range. } else if ( isnan ( value ) ){ // This is an erroneous, invalid, or missing measurement. } else { // The sensor reported these measurements as valid, but they are discarded per the limits defined by minimum_range and maximum_range. }","title":"Grove ultrasonic ranger"},{"location":"grove_ultrasonic_ranger.html#grove-ultrasonic-ranger","text":"To avoid obstacles the Grove ultrasonic ranger from Seeed Studio is used. We will create a ROS package with catkin create pkg PKG_NAME [--catkin-deps [DEP [DEP ...]]] : 1 2 3 4 5 6 fjp@ubuntu:~/git/2wd-robot/ros/src$ catkin create pkg grove_ultrasonic_ranger --catkin-deps rospy roscpp sensor_msgs Creating package \"grove_ultrasonic_ranger\" in \"/home/fjp/git/2wd-robot/ros/src\" ... Created file grove_ultrasonic_ranger/CMakeLists.txt Created file grove_ultrasonic_ranger/package.xml Created folder grove_ultrasonic_ranger/include/grove_ultrasonic_ranger Created folder grove_ultrasonic_ranger/src The package depends on the two ROS client libraries rospy and roscpp . To signalise the current distance to obstacles the sensor_msgs/Range message is used. After connecting the signal pin of the sensor to (physical) GPIO 11 of the Raspberry Pi 4 B and power it with 5V and ground, we can test its functionality with the available python script ultrasonic.py from Seed Studio. The following shows the truncated output of the ultrasonic.py script when moving an obstacle in front of the sensor. We see that the distance value changes as expected. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 fjp@ubuntu:~/git/2wd-robot/ros/src/grove_ultrasonic_ranger/src$ sudo python ultrasonic.py SeeedStudio Grove Ultrasonic get data and print Ultrasonic Measurement Distance : 2 .0 CM Ultrasonic Measurement Distance : 2 .1 CM Ultrasonic Measurement Distance : 2 .0 CM Ultrasonic Measurement Distance : 3 .5 CM Ultrasonic Measurement Distance : 3 .5 CM Ultrasonic Measurement Distance : 2 .4 CM Ultrasonic Measurement Distance : 12 .9 CM ... Ultrasonic Measurement Distance : 45 .8 CM Ultrasonic Measurement Distance : 510 .7 CM Ultrasonic Measurement Distance : 510 .7 CM Ultrasonic Measurement Distance : 45 .7 CM Ultrasonic Measurement Distance : 161 .8 CM Ultrasonic Measurement Distance : 26 .9 CM Ultrasonic Measurement Distance : 18 .3 CM Ultrasonic Measurement Distance : 160 .9 CM Ultrasonic Measurement Distance : 158 .3 CM Ultrasonic Measurement Distance : 159 .4 CM ...","title":"Grove - Ultrasonic Ranger"},{"location":"grove_ultrasonic_ranger.html#modified-groveultrasonicranger-library","text":"To use the ultrasonic ranger as a ROS node it is convenient to wrap the sensor functionality in a class. This provides an easy to extend interface for the ultrasonic ranger ( API ) Therefore I copied the core functionality of the ultrasonic.py script from Seeed Studio in a class named GroveUltrasonicRanger . Executing the grove_ultrasonic_ranger.py will result in the following output: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 fjp@ubuntu:~/git/2wd-robot/ros/src/grove_ultrasonic_ranger/src$ sudo python grove_ultrasonic_ranger.py SeeedStudio Grove Ultrasonic get data and print Distance : 0 .051 m Distance : 0 .069 m Distance : 0 .098 m Distance : 0 .131 m Distance : 0 .153 m Distance : 0 .172 m Distance : 0 .207 m Distance : 0 .210 m Distance : 0 .234 m Distance : 0 .256 m GPIO.cleanup () GPIO.cleanup () done","title":"Modified GroveUltrasonicRanger Library"},{"location":"grove_ultrasonic_ranger.html#ros-node-for-ultrasonic-ranger","text":"ROS provides the Range Message in the sensor_msgs header . This message type can be used to write a wrapper that will act as a ROS node for the Grove ultrasonic sensor. To design this node we will send out measurements periodically over a topic of type sensor_msgs/Range . The code for this node is in ranger.py . After writing the node we need to build the packages in the workspace with catkin build . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 fjp@ubuntu:~/git/2wd-robot/ros$ catkin build ---------------------------------------------------------------- Profile: default Extending: [ env ] /opt/ros/melodic Workspace: /home/fjp/git/2wd-robot/ros ---------------------------------------------------------------- Build Space: [ exists ] /home/fjp/git/2wd-robot/ros/build Devel Space: [ exists ] /home/fjp/git/2wd-robot/ros/devel Install Space: [ unused ] /home/fjp/git/2wd-robot/ros/install Log Space: [ missing ] /home/fjp/git/2wd-robot/ros/logs Source Space: [ exists ] /home/fjp/git/2wd-robot/ros/src DESTDIR: [ unused ] None ---------------------------------------------------------------- Devel Space Layout: linked Install Space Layout: None ---------------------------------------------------------------- Additional CMake Args: None Additional Make Args: None Additional catkin Make Args: None Internal Make Job Server: True Cache Job Environments: False ---------------------------------------------------------------- Whitelisted Packages: None Blacklisted Packages: None ---------------------------------------------------------------- Workspace configuration appears valid. NOTE: Forcing CMake to run for each package. ---------------------------------------------------------------- [ build ] Found '1' package in 0 .0 seconds. [ build ] Updating package table. Starting >>> catkin_tools_prebuild Finished <<< catkin_tools_prebuild [ 9 .9 seconds ] Starting >>> grove_ultrasonic_ranger Finished <<< grove_ultrasonic_ranger [ 12 .0 seconds ] [ build ] Summary: All 2 packages succeeded! [ build ] Ignored: None. [ build ] Warnings: None. [ build ] Abandoned: None. [ build ] Failed: None. [ build ] Runtime: 21 .9 seconds total. [ build ] Note: Workspace packages have changed, please re-source setup files to use them. As the final note of the build output suggests, we have to source the setup.bash files in the devel space. 1 fjp@ubuntu:~/git/2wd-robot/ros$ source devel/setup.bash To make the ranger node executable we have to modify the ranger.py file: 1 fjp@ubuntu:~/git/2wd-robot/ros/src/grove_ultrasonic_ranger/src$ sudo chmod a+x ranger.py Then we can test the node using rosrun : 1 2 3 4 5 6 7 8 9 10 fjp@ubuntu:~/git/2wd-robot/ros$ sudo su [ sudo ] password for fjp: root@ubuntu:/home/fjp/git/2wd-robot/ros# source devel/setup.bash root@ubuntu:/home/fjp/git/2wd-robot/ros# rosrun grove_ultrasonic_ranger ranger.py Distance : 1 .617 m Distance : 1 .617 m Distance : 1 .617 m Distance : 0 .108 m Distance : 0 .092 m Distance : 0 .099 m This lets the node publish range messages which we can capture in another terminal window using rostopic . First we use rostopic list to find the name of the topic we are interested in: 1 2 3 4 fjp@ubuntu:~/git/2wd-robot/ros$ rostopic list /distance /rosout /rosout_agg We named our topic /distance which we can use with the rostopic echo command to see the published messages: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 fjp@ubuntu:~/git/2wd-robot/ros$ rostopic echo /distance header: seq: 1 stamp: secs: 1576778377 nsecs: 746809959 frame_id: \"ranger_distance\" radiation_type: 0 field_of_view: 0 .261799007654 min_range: 0 .019999999553 max_range: 3 .5 range: 1 .61674261093 --- header: seq: 2 stamp: secs: 1576778378 nsecs: 459048986 frame_id: \"ranger_distance\" radiation_type: 0 field_of_view: 0 .261799007654 min_range: 0 .019999999553 max_range: 3 .5 range: 1 .61261284351 --- header: seq: 3 stamp: secs: 1576778379 nsecs: 172172069 frame_id: \"ranger_distance\" radiation_type: 0 field_of_view: 0 .261799007654 min_range: 0 .019999999553 max_range: 3 .5 range: 1 .61657905579 --- header: seq: 4 stamp: secs: 1576778379 nsecs: 884002923 frame_id: \"ranger_distance\" radiation_type: 0 field_of_view: 0 .261799007654 min_range: 0 .019999999553 max_range: 3 .5 range: 1 .61277639866 --- header: seq: 5 stamp: secs: 1576778380 nsecs: 596549034 frame_id: \"ranger_distance\" radiation_type: 0 field_of_view: 0 .261799007654 min_range: 0 .019999999553 max_range: 3 .5 range: 1 .67693090439 ---","title":"ROS Node for Ultrasonic Ranger"},{"location":"grove_ultrasonic_ranger.html#informational-distance-measurements","text":"To provide additional information when an obstacle is too close or the robot has no obstacle in front of it we use REP-117 as guideline. To successfully implement this REP, checks for valid measurements should use floating-point standards and follow this form: 1 2 3 4 5 6 7 8 9 10 11 if ( minimum_range <= value && value <= maximum_range ){ // Represents expected pre-REP logic and is the only necessary condition for most applications. // This is a valid measurement. } else if ( ! isfinite ( value ) && value < 0 ){ // Object too close to measure. } else if ( ! isfinite ( value ) && value > 0 ){ // No objects detected in range. } else if ( isnan ( value ) ){ // This is an erroneous, invalid, or missing measurement. } else { // The sensor reported these measurements as valid, but they are discarded per the limits defined by minimum_range and maximum_range. }","title":"Informational Distance Measurements"},{"location":"hardware-interfaces.html","text":"The hardware interfaces provide an interface between the components (sensors and actuators) of the 2WD robot and its processing units, the Raspberry Pi 4 B (or the Nvidia Jetson Nano) and the microcontroller (in this case the Teensy 4.0). USB The Universial Serial Bus (USB) connections are required to connect the Single Board Computer (SBC) with the microcontroller. Using this connection, it is possible to communicate via rosserial . Another USB connector is used for the RPLidar laser scanner. Info See the section USB Devices below to setup the required permissions and allow the communication between this interface. Single Board Computer GPIO Currently, one GPIO pin is used to connect the ultrasonic ranger. The ultrasonic ranger uses just a single GPIO pin for communicating its measured distances. Therefore, we can use one of the GPIO pins such as physical pin 11 . Info In case you are using LM393 speed sensors, instead of the encoders of the DG01D-E, the LM393 use a single digital GPIO pin each. These pins could be directly connected to the Raspberry Pi GPIOs and setup using software interrupts with the RPi.GPIO library. Alternatively they could be also connected to the pins of the microcontroller, e.g. Teensy. For this build the Microcontroller Digital Pins Four digital pins on the Teensy microcontroller are in use for the two quadrature encoders of the DG01D-E. Info See the diffbot_base package for the running software script to read the encoder ticks. Single Board Computer I2C Connection The I2C connections on the Raspberry Pi 4 B are used for multiple components such as the motor driver and the oled display. I2C Pinout on Raspberry Pi 4 B. Using these ports on the Raspberry Pi 4 B, requires that we enable the I2C interface. To do so, we will use the tool i2cdetect which requires that we install a tool on Ubuntu called i2c-tools : 1 2 3 4 5 6 7 fjp@ubuntu:~/git/2wd-robot$ i2cdetect Command 'i2cdetect' not found, but can be installed with: sudo apt install i2c-tools fjp@ubuntu:~/git/2wd-robot$ sudo apt install i2c-tools This i2cdetect tool is a userspace program to scan an I2C bus for devices given a specific i2cbus argument which indicates the number or name of the I2C bus to be scanned, and should correspond to one of the busses listed by i2cdetect -l . See also info i2cdetect for the manual page. To test if the i2c ports are working we use the following commands: 1 2 3 4 $ i2cdetect -y 0 Error: Could not open file '/dev/i2c-0' or '/dev/i2c/0': No such file or directory $ i2cdetect -y 1 Error: Could not open file '/dev/i2c-1' or '/dev/i2c/1': No such file or directory The ports are not setup correctly yet, which is why we need to enable the following two lines in the /boot/firmware/config.txt file: 1 2 dtparam = i2c0 = on dtparam = i2c1 = on After rebooting the Raspberry Pi and entering the command again the following output will appear: 1 2 3 $ i2cdetect -y 0 Error: Could not open file `/dev/i2c-0': Permission denied Run as root? Running as root using sudo will work (please read on, there is a better way): 1 2 3 4 5 6 7 8 9 10 11 $ sudo i2cdetect -y 0 [sudo] password for fjp: 0 1 2 3 4 5 6 7 8 9 a b c d e f 00: -- -- -- -- -- -- -- -- -- -- -- -- -- 10: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 20: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 30: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 40: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 50: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 60: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 70: -- -- -- -- -- -- -- -- As mentioned there is a better way to access the i2c devices without using power user privileges. When issuing the following: 1 2 ls -l /dev/i2c-0 crw-rw---- 1 root i2c 89, 0 Apr 1 2020 /dev/i2c-0 we see that the /dev/i2c-0 device belongs to user root and i2c user group. To get access without sudo we can add other users, requiering access to the i2c group with: 1 2 3 4 sudo adduser fjp i2c Adding user `fjp' to group `i2c' ... Adding user fjp to group i2c Done. After logging out and back in again the access will be granted and following output will come up: 1 2 3 4 5 6 7 8 9 10 $ i2cdetect -y 0 0 1 2 3 4 5 6 7 8 9 a b c d e f 00: -- -- -- -- -- -- -- -- -- -- -- -- -- 10: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 20: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 30: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 40: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 50: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 60: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 70: -- -- -- -- -- -- -- -- It outputs a table with the list of detected devices on the specified bus. In this case there are no connected devices on I2C bus 0. Alternative setup using raspi-config On Raspian Buster, the official Raspberry OS, we could use the `raspi-config` tool: fjp@ubuntu : ~/git/2wd-robot $ sudo raspi-config Raspberry Pi 4 Model B Rev 1.1 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 Raspberry Pi Software Configuration Tool (raspi-config) \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 1 Change User Password Change password for the current user \u2502 \u2502 2 Network Options Configure network settings \u2502 \u2502 3 Boot Options Configure options for start-up \u2502 \u2502 4 Localisation Options Set up language and regional settings to match your location \u2502 \u2502 5 Interfacing Options Configure connections to peripherals \u2502 \u2502 6 Overclock Configure overclocking for your Pi \u2502 \u2502 7 Advanced Options Configure advanced settings \u2502 \u2502 8 Update Update this tool to the latest version \u2502 \u2502 9 About raspi-config Information about this configuration tool \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 <Select> <Finish> \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Select the i2c option: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 Raspberry Pi Software Configuration Tool (raspi-config) \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 P1 Camera Enable/Disable connection to the Raspberry Pi Camera \u2502 \u2502 P2 SSH Enable/Disable remote command line access to your Pi using SSH \u2502 \u2502 P3 VNC Enable/Disable graphical remote access to your Pi using RealVNC \u2502 \u2502 P4 SPI Enable/Disable automatic loading of SPI kernel module \u2502 \u2502 P5 I2C Enable/Disable automatic loading of I2C kernel module \u2502 \u2502 P6 Serial Enable/Disable shell and kernel messages on the serial connection \u2502 \u2502 P7 1-Wire Enable/Disable one-wire interface \u2502 \u2502 P8 Remote GPIO Enable/Disable remote access to GPIO pins \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 <Select> <Back> \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 And enable the interface: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 Would you like the ARM I2C interface to be enabled? \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 <Yes> <No> \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Confirm the activation and restart the RPi: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 The ARM I2C interface is enabled \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 <Ok> \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 USB Devices Similar to accessing i2c devices, a non root user can use usb connections by adding it to the the dialout group: 1 2 3 4 sudo adduser fjp dialout Adding user `fjp' to group `dialout' ... Adding user fjp to group dialout Done.","title":"Hardware Interfaces"},{"location":"hardware-interfaces.html#usb","text":"The Universial Serial Bus (USB) connections are required to connect the Single Board Computer (SBC) with the microcontroller. Using this connection, it is possible to communicate via rosserial . Another USB connector is used for the RPLidar laser scanner. Info See the section USB Devices below to setup the required permissions and allow the communication between this interface.","title":"USB"},{"location":"hardware-interfaces.html#single-board-computer-gpio","text":"Currently, one GPIO pin is used to connect the ultrasonic ranger. The ultrasonic ranger uses just a single GPIO pin for communicating its measured distances. Therefore, we can use one of the GPIO pins such as physical pin 11 . Info In case you are using LM393 speed sensors, instead of the encoders of the DG01D-E, the LM393 use a single digital GPIO pin each. These pins could be directly connected to the Raspberry Pi GPIOs and setup using software interrupts with the RPi.GPIO library. Alternatively they could be also connected to the pins of the microcontroller, e.g. Teensy. For this build the","title":"Single Board Computer GPIO"},{"location":"hardware-interfaces.html#microcontroller-digital-pins","text":"Four digital pins on the Teensy microcontroller are in use for the two quadrature encoders of the DG01D-E. Info See the diffbot_base package for the running software script to read the encoder ticks.","title":"Microcontroller Digital Pins"},{"location":"hardware-interfaces.html#single-board-computer-i2c-connection","text":"The I2C connections on the Raspberry Pi 4 B are used for multiple components such as the motor driver and the oled display. I2C Pinout on Raspberry Pi 4 B. Using these ports on the Raspberry Pi 4 B, requires that we enable the I2C interface. To do so, we will use the tool i2cdetect which requires that we install a tool on Ubuntu called i2c-tools : 1 2 3 4 5 6 7 fjp@ubuntu:~/git/2wd-robot$ i2cdetect Command 'i2cdetect' not found, but can be installed with: sudo apt install i2c-tools fjp@ubuntu:~/git/2wd-robot$ sudo apt install i2c-tools This i2cdetect tool is a userspace program to scan an I2C bus for devices given a specific i2cbus argument which indicates the number or name of the I2C bus to be scanned, and should correspond to one of the busses listed by i2cdetect -l . See also info i2cdetect for the manual page. To test if the i2c ports are working we use the following commands: 1 2 3 4 $ i2cdetect -y 0 Error: Could not open file '/dev/i2c-0' or '/dev/i2c/0': No such file or directory $ i2cdetect -y 1 Error: Could not open file '/dev/i2c-1' or '/dev/i2c/1': No such file or directory The ports are not setup correctly yet, which is why we need to enable the following two lines in the /boot/firmware/config.txt file: 1 2 dtparam = i2c0 = on dtparam = i2c1 = on After rebooting the Raspberry Pi and entering the command again the following output will appear: 1 2 3 $ i2cdetect -y 0 Error: Could not open file `/dev/i2c-0': Permission denied Run as root? Running as root using sudo will work (please read on, there is a better way): 1 2 3 4 5 6 7 8 9 10 11 $ sudo i2cdetect -y 0 [sudo] password for fjp: 0 1 2 3 4 5 6 7 8 9 a b c d e f 00: -- -- -- -- -- -- -- -- -- -- -- -- -- 10: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 20: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 30: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 40: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 50: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 60: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 70: -- -- -- -- -- -- -- -- As mentioned there is a better way to access the i2c devices without using power user privileges. When issuing the following: 1 2 ls -l /dev/i2c-0 crw-rw---- 1 root i2c 89, 0 Apr 1 2020 /dev/i2c-0 we see that the /dev/i2c-0 device belongs to user root and i2c user group. To get access without sudo we can add other users, requiering access to the i2c group with: 1 2 3 4 sudo adduser fjp i2c Adding user `fjp' to group `i2c' ... Adding user fjp to group i2c Done. After logging out and back in again the access will be granted and following output will come up: 1 2 3 4 5 6 7 8 9 10 $ i2cdetect -y 0 0 1 2 3 4 5 6 7 8 9 a b c d e f 00: -- -- -- -- -- -- -- -- -- -- -- -- -- 10: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 20: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 30: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 40: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 50: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 60: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 70: -- -- -- -- -- -- -- -- It outputs a table with the list of detected devices on the specified bus. In this case there are no connected devices on I2C bus 0. Alternative setup using raspi-config On Raspian Buster, the official Raspberry OS, we could use the `raspi-config` tool: fjp@ubuntu : ~/git/2wd-robot $ sudo raspi-config Raspberry Pi 4 Model B Rev 1.1 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 Raspberry Pi Software Configuration Tool (raspi-config) \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 1 Change User Password Change password for the current user \u2502 \u2502 2 Network Options Configure network settings \u2502 \u2502 3 Boot Options Configure options for start-up \u2502 \u2502 4 Localisation Options Set up language and regional settings to match your location \u2502 \u2502 5 Interfacing Options Configure connections to peripherals \u2502 \u2502 6 Overclock Configure overclocking for your Pi \u2502 \u2502 7 Advanced Options Configure advanced settings \u2502 \u2502 8 Update Update this tool to the latest version \u2502 \u2502 9 About raspi-config Information about this configuration tool \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 <Select> <Finish> \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Select the i2c option: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 Raspberry Pi Software Configuration Tool (raspi-config) \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 P1 Camera Enable/Disable connection to the Raspberry Pi Camera \u2502 \u2502 P2 SSH Enable/Disable remote command line access to your Pi using SSH \u2502 \u2502 P3 VNC Enable/Disable graphical remote access to your Pi using RealVNC \u2502 \u2502 P4 SPI Enable/Disable automatic loading of SPI kernel module \u2502 \u2502 P5 I2C Enable/Disable automatic loading of I2C kernel module \u2502 \u2502 P6 Serial Enable/Disable shell and kernel messages on the serial connection \u2502 \u2502 P7 1-Wire Enable/Disable one-wire interface \u2502 \u2502 P8 Remote GPIO Enable/Disable remote access to GPIO pins \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 <Select> <Back> \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 And enable the interface: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 Would you like the ARM I2C interface to be enabled? \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 <Yes> <No> \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Confirm the activation and restart the RPi: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 The ARM I2C interface is enabled \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 <Ok> \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518","title":"Single Board Computer I2C Connection"},{"location":"hardware-interfaces.html#usb-devices","text":"Similar to accessing i2c devices, a non root user can use usb connections by adding it to the the dialout group: 1 2 3 4 sudo adduser fjp dialout Adding user `fjp' to group `dialout' ... Adding user fjp to group dialout Done.","title":"USB Devices"},{"location":"jetson-nano-setup.html","text":"These are the instructions to setup the official Ubuntu 18.04 image from Nvidia on a Jetson Nano. Note In case you don't have a Jetson Nano, it is also possible to create the robot using a Raspberry Pi 4 B. See the related section in the documentation . Installing the JetPack SDK The JetPack SDK is a tool that installs the software tools and operating system for a Jetson Development Kit. Note JetPack SDK includes the latest Jetson Linux Driver Package (L4T) with Linux operating system and CUDA-X accelerated libraries and APIs for Deep Learning, Computer Vision, Accelerated Computing and Multimedia. It also includes samples, documentation, and developer tools for both host computer and developer kit, and supports higher level SDKs such as DeepStream for streaming video analytics and Isaac for robotics. 1 Start by downloading the 6 GB SD card image for the Jetson Nano Developer Kit from the Nvidia Developer website ( direct download link ). Alternatively you can get the Jetpack from the Jetson Download Center . Note In case you want to use the NVIDIA SDK Manager you need to be part of the NVIDIA Developer Program. You can use this manager to configure the Jetpack image before flashing it. Next use a tool to flash the image onto the SD card. One option is to use balenaEtcher that is available on Ubuntu, Mac and Windows. Info Flashing in this context, means the transfer of software data, also refered to as Firmware, from your computer to a device, such as a the sd card in this case. The term \u201cto flash\u201d comes from the Flash storage component of a device where the Firmware is stored. Power Supply There are four ways you can power the Jetson Nano. Provide 2 Amps at 5 Volts to the micro USB connector 4 Amps at 5 Volts to the barrel jack connector. 5 Volts on the GPIO headers, where each of the two 5 Volt pins can handle up to 3 A. The Nano has two of these 5 Volt pins, which means you could consume 6 A total at 5 V by having the two pins connected to a power supply that is capable of delivering 6 A at 5 V, corresponding to 30 Watts. Power Over Ethernet (POE) In its default power consumption state ('mode 0') the Nano itself uses 10 Watts, which is 2 A at 5 V. This, however, is only for the Jetson Nano compute module and doesn't include the peripherals. To provide power to peripherals, for example over the USB connectors, you need to operate the Nano itself in 5 Watts mode ('mode 1' or MAXN). This way there are 5 Watts left for peripherals via the micro USB connector, that provides 2 A at 5 V. The following table 2 shows the power modes for the Jetson Nano predefined by NVIDIA and the associated caps on use of the module's resources. Property MAXN 3 5W Power Budget 10 Watts 5 Watts Mode ID 0 1 Online CPU 4 2 CPU Maximal Frequency (MHz) 1479 918 GPU TPC 1 1 GPU Maximal Frequency (MHz) 921.6 640 Memory Maximal Frequency (MHz) 1600 1600 SOC clocks maximal frequency (MHz) All modes adsp 844.8 csi 750 se 627.2 ape 499.2 nvdec 716.8 tsec 408 host1x 408 nvenc 716.8 tsecb 627.2 isp 793.6 nvjpg 627.2 vi 793.6 display 665.6 pcie 500 vic03 627.2 Note In practice, when running from the micro-USB connector, we should be running in 5V mode to power the rest of the sensors, like the laser scanner. The drawback is that this will slow down the computations on the Jetson Nano. See the section below on how to do that. To develop and test code that requires high power, it is convenient to use the second option. To connect the 4 Amps @ 5 Volts barrel jack connector to the Jetson Nano a Jumper is required. Note Make sure to use a 5 V 4 Amps switching power supply. For example the Mean Well GST25E05-P1J or the AC/DC Desktop Adapter 5 V from Adafruit . DC barrel jack 5.5 mm OD / 2.1 mm ID / 9.5 mm length, center pin positive. Connect the jumper on J48. J48 is located between the Barrel Jack connector and the Camera connector. This jumper tells the Nano to use the Barrel Jack instead of the micro-USB port. Then plug the power supply into the Barrel Jack, and the Nano boots. TODO add image of Jumper location Note There are two variants of the Jetson Nano. The older A02 and the revised B01. Depending on the variant the location of the jumper is slightly different. - For an A02 carrier board (pre-2020) J48 is the solo header next to the camera port. - For a B01 carrier board (2020+, has two camera ports) J48 is a solo header behind the barrel jack and the HDMI port. Prepare Ubuntu After flashing the image to the sd card insert it to the Jetson Nano, hook it up to a monitor via HDMI and power it up by pluggin in the micro USB or even better barrel jack connector (don't forget the jumper on J48). The follow the instructions on the screen to setup Ubuntu 18.04 Bionic Beaver. For this you will need to accept the Nvidia End User License Agreement, set the desired language, keyboard, time zone, login credentials, APP Partition size (choose max possible), and delete unused bootloader that is done automatically with the new QSPI image (MaxSPI) of Jetpack 4.5. This QSPI update will take about two mins. Finally select the NVPModel mode explained above . First we go with the default MAXN 10 Watts mode ('mode 0'). We will changed this later at runtime - when powering the robot over the power bank - using the nvpmodel GUI or nvpmodel command line utility. Refer to the NVIDIA Jetson Linux Developer Guide for further information. Once finished, follow the next steps to install ROS Melodic. Note To proceed with the next steps on installing ROS and other related dependencies you can run a bash script. Just clone this repository: https://github.com/fjp/diffbot.git and run ubuntu-setup.sh . But to learn more, you should follow the instructions on the following pages. Resources JetPack SDK Jetson Nano \u2013 Use More Power! - Jetson Hacks https://developer.nvidia.com/embedded/jetpack \u21a9 Nvidia Jetson Nano: Supported Modes and Power Efficiency \u21a9 The default mode is MAXN (power budget 10 watts, mode ID 0). \u21a9","title":"Jetson Nano Setup"},{"location":"jetson-nano-setup.html#installing-the-jetpack-sdk","text":"The JetPack SDK is a tool that installs the software tools and operating system for a Jetson Development Kit. Note JetPack SDK includes the latest Jetson Linux Driver Package (L4T) with Linux operating system and CUDA-X accelerated libraries and APIs for Deep Learning, Computer Vision, Accelerated Computing and Multimedia. It also includes samples, documentation, and developer tools for both host computer and developer kit, and supports higher level SDKs such as DeepStream for streaming video analytics and Isaac for robotics. 1 Start by downloading the 6 GB SD card image for the Jetson Nano Developer Kit from the Nvidia Developer website ( direct download link ). Alternatively you can get the Jetpack from the Jetson Download Center . Note In case you want to use the NVIDIA SDK Manager you need to be part of the NVIDIA Developer Program. You can use this manager to configure the Jetpack image before flashing it. Next use a tool to flash the image onto the SD card. One option is to use balenaEtcher that is available on Ubuntu, Mac and Windows. Info Flashing in this context, means the transfer of software data, also refered to as Firmware, from your computer to a device, such as a the sd card in this case. The term \u201cto flash\u201d comes from the Flash storage component of a device where the Firmware is stored.","title":"Installing the JetPack SDK"},{"location":"jetson-nano-setup.html#power-supply","text":"There are four ways you can power the Jetson Nano. Provide 2 Amps at 5 Volts to the micro USB connector 4 Amps at 5 Volts to the barrel jack connector. 5 Volts on the GPIO headers, where each of the two 5 Volt pins can handle up to 3 A. The Nano has two of these 5 Volt pins, which means you could consume 6 A total at 5 V by having the two pins connected to a power supply that is capable of delivering 6 A at 5 V, corresponding to 30 Watts. Power Over Ethernet (POE) In its default power consumption state ('mode 0') the Nano itself uses 10 Watts, which is 2 A at 5 V. This, however, is only for the Jetson Nano compute module and doesn't include the peripherals. To provide power to peripherals, for example over the USB connectors, you need to operate the Nano itself in 5 Watts mode ('mode 1' or MAXN). This way there are 5 Watts left for peripherals via the micro USB connector, that provides 2 A at 5 V. The following table 2 shows the power modes for the Jetson Nano predefined by NVIDIA and the associated caps on use of the module's resources. Property MAXN 3 5W Power Budget 10 Watts 5 Watts Mode ID 0 1 Online CPU 4 2 CPU Maximal Frequency (MHz) 1479 918 GPU TPC 1 1 GPU Maximal Frequency (MHz) 921.6 640 Memory Maximal Frequency (MHz) 1600 1600 SOC clocks maximal frequency (MHz) All modes adsp 844.8 csi 750 se 627.2 ape 499.2 nvdec 716.8 tsec 408 host1x 408 nvenc 716.8 tsecb 627.2 isp 793.6 nvjpg 627.2 vi 793.6 display 665.6 pcie 500 vic03 627.2 Note In practice, when running from the micro-USB connector, we should be running in 5V mode to power the rest of the sensors, like the laser scanner. The drawback is that this will slow down the computations on the Jetson Nano. See the section below on how to do that. To develop and test code that requires high power, it is convenient to use the second option. To connect the 4 Amps @ 5 Volts barrel jack connector to the Jetson Nano a Jumper is required. Note Make sure to use a 5 V 4 Amps switching power supply. For example the Mean Well GST25E05-P1J or the AC/DC Desktop Adapter 5 V from Adafruit . DC barrel jack 5.5 mm OD / 2.1 mm ID / 9.5 mm length, center pin positive. Connect the jumper on J48. J48 is located between the Barrel Jack connector and the Camera connector. This jumper tells the Nano to use the Barrel Jack instead of the micro-USB port. Then plug the power supply into the Barrel Jack, and the Nano boots. TODO add image of Jumper location Note There are two variants of the Jetson Nano. The older A02 and the revised B01. Depending on the variant the location of the jumper is slightly different. - For an A02 carrier board (pre-2020) J48 is the solo header next to the camera port. - For a B01 carrier board (2020+, has two camera ports) J48 is a solo header behind the barrel jack and the HDMI port.","title":"Power Supply"},{"location":"jetson-nano-setup.html#prepare-ubuntu","text":"After flashing the image to the sd card insert it to the Jetson Nano, hook it up to a monitor via HDMI and power it up by pluggin in the micro USB or even better barrel jack connector (don't forget the jumper on J48). The follow the instructions on the screen to setup Ubuntu 18.04 Bionic Beaver. For this you will need to accept the Nvidia End User License Agreement, set the desired language, keyboard, time zone, login credentials, APP Partition size (choose max possible), and delete unused bootloader that is done automatically with the new QSPI image (MaxSPI) of Jetpack 4.5. This QSPI update will take about two mins. Finally select the NVPModel mode explained above . First we go with the default MAXN 10 Watts mode ('mode 0'). We will changed this later at runtime - when powering the robot over the power bank - using the nvpmodel GUI or nvpmodel command line utility. Refer to the NVIDIA Jetson Linux Developer Guide for further information. Once finished, follow the next steps to install ROS Melodic. Note To proceed with the next steps on installing ROS and other related dependencies you can run a bash script. Just clone this repository: https://github.com/fjp/diffbot.git and run ubuntu-setup.sh . But to learn more, you should follow the instructions on the following pages.","title":"Prepare Ubuntu"},{"location":"jetson-nano-setup.html#resources","text":"JetPack SDK Jetson Nano \u2013 Use More Power! - Jetson Hacks https://developer.nvidia.com/embedded/jetpack \u21a9 Nvidia Jetson Nano: Supported Modes and Power Efficiency \u21a9 The default mode is MAXN (power budget 10 watts, mode ID 0). \u21a9","title":"Resources"},{"location":"laser-range-scanner.html","text":"Laser Range Scanner For SLAM (simultaneous localization and mapping) and navigation a laser range scanner is a cruical requirement to use the ROS navigation stack (see also diffbot_navigation ). DiffBot uses the 360 field of view RPLidar A2 M8 with 12 meter range. Mounting When installing the RPLidar on DiffBot make sure it is located high enough to avoid occluding the laser. It is also important to mount the laser in the correct location and that its direction and orientation matches the origin alignment in the robot description. In the figure below we can see, that the side with the communication cable is pointing in the positive x-direction. In case the cable should be mounted in the opposite direction, it is necessary to adapt the origin of the laser link in the robot description. Installation manual for RPLidar A2 (source: [robopeak/rplidar_ros/](https://github.com/robopeak/rplidar_ros/wiki/How-to-use-rplidar).","title":"Laser Range Scanner"},{"location":"laser-range-scanner.html#laser-range-scanner","text":"For SLAM (simultaneous localization and mapping) and navigation a laser range scanner is a cruical requirement to use the ROS navigation stack (see also diffbot_navigation ). DiffBot uses the 360 field of view RPLidar A2 M8 with 12 meter range.","title":"Laser Range Scanner"},{"location":"laser-range-scanner.html#mounting","text":"When installing the RPLidar on DiffBot make sure it is located high enough to avoid occluding the laser. It is also important to mount the laser in the correct location and that its direction and orientation matches the origin alignment in the robot description. In the figure below we can see, that the side with the communication cable is pointing in the positive x-direction. In case the cable should be mounted in the opposite direction, it is necessary to adapt the origin of the laser link in the robot description. Installation manual for RPLidar A2 (source: [robopeak/rplidar_ros/](https://github.com/robopeak/rplidar_ros/wiki/How-to-use-rplidar).","title":"Mounting"},{"location":"lm393_speed_sensor.html","text":"LM393 Speed Sensor - Odometry Note that this sensor is deprecated in the current version of DiffBot because it isn't a quadrature encoder. Instead, the DG01D-E motor includes a quadrature encoder that can measure ticks (can be converted to speed) and the direction the motor is turning (clock-wise or anti-clock-wise). The LM393 speed sensor could be used in combination with an additional information about the current driving direction, for example coming from the software. Howerver, this won't be as accurate as using a quadrature encoder that provides this information. To measure how far the robot has driven, we use the LM393 speed sensor from Joy-IT as odometry sensor. First, we will create a ROS package with catkin create pkg PKG_NAME [--catkin-deps [DEP [DEP ...]]] : 1 2 3 4 5 6 7 fjp@ubuntu:~/git/2wd-robot/ros/src$ catkin create pkg lm393_speed_sensor --catkin-deps rospy roscpp nav_msgs Creating package \"lm393_speed_sensor\" in \"/home/fjp/git/2wd-robot/ros/src\" ... Created file lm393_speed_sensor/CMakeLists.txt Created file lm393_speed_sensor/package.xml Created folder lm393_speed_sensor/include/lm393_speed_sensor Created folder lm393_speed_sensor/src Successfully created package files in /home/fjp/git/2wd-robot/ros/src/lm393_speed_sensor. The package depends on the two ROS client libraries rospy and roscpp . The current implementation uses python and the RPi.GPIO library for interrupts. To achieve more percise results, C++ should be used instead. To signalise the current pose of the robot in the odometry frame, the nav_msgs/Range message is used. Connection To get the speed sensors working, we connect the signal pins to (physical) GPIO 15 and (physical) GPIO 16 of the Raspberry Pi 4 B and power them with 3.3V. The ground pins are connected to ground of the Pi. LM393 Speed Sensor Library To use the LM393 speed sensor as a ROS node the sensor functionality is wraped in a class. This provides an easy to extend interface for the speed sensor ( API ) The code consists of a class LM393SpeedSensor which has two interrupt service routines (ISR) methods. Using the RPi.GPIO interrupt capabilities, these ISR methods are used as callback functions when the sensor measures a falling edge. This is the case when the rotary disk spins and the optocoupler measures a high to low signal due to the spinning disk. The sensor API is implemented in the lm393_speed_sensor.py python module. Executing the module will result in the following output when the motors are spinning freely with full speed and using some force to slow them down. We see that the rotational speed \\(\\omega\\) , measured in RPM ( revolutions per minute ) changes. 1 2 fjp@ubuntu:~/git/2wd-robot/ros/src/lm393_speed_sensor/src$ sudo python lm393_speed_sensor.py TODO The rotational speed \\(\\omega\\) values will be converted to tangential velocity values using the radius \\(r\\) of the wheels. \\[ v = \\omega \\cdot r = 2 \\pi n \\cdot r \\] ROS Node for LM393 Speed Sensor ROS provides the Odometry Message in the nav_msgs header . This message type can be used to write a wrapper that will act as a ROS node for the LM393 speed sensor. To design this node we will send out measurements periodically over a topic of type nav_msgs/Odometry . The code for this node is in speed_sensor.py . After writing the node we need to build the packages in the workspace with catkin build . 1 2 fjp@ubuntu:~/git/2wd-robot/ros$ catkin build TODO As the final note of the build output suggests, we have to source the setup.bash files in the devel space. 1 fjp@ubuntu:~/git/2wd-robot/ros$ source devel/setup.bash To make the speed_sensor node executable we have to modify the speed_sensor.py file: 1 fjp@ubuntu:~/git/2wd-robot/ros/src/grove_ultrasonic_ranger/src$ sudo chmod a+x speed_sensor.py Then we can test the node using rosrun : 1 2 3 4 fjp@ubuntu:~/git/2wd-robot/ros$ sudo su [ sudo ] password for fjp: root@ubuntu:/home/fjp/git/2wd-robot/ros# source devel/setup.bash root@ubuntu:/home/fjp/git/2wd-robot/ros# rosrun lm393_speed_sensor speed_sensor.py This lets the node publish range messages which we can capture in another terminal window using rostopic . First we use rostopic list to find the name of the topic we are interested in: 1 2 fjp@ubuntu:~/git/2wd-robot/ros$ rostopic list TODO We named our topic /odom which we can use with the rostopic echo command to see the published messages:","title":"Lm393 speed sensor"},{"location":"lm393_speed_sensor.html#lm393-speed-sensor-odometry","text":"Note that this sensor is deprecated in the current version of DiffBot because it isn't a quadrature encoder. Instead, the DG01D-E motor includes a quadrature encoder that can measure ticks (can be converted to speed) and the direction the motor is turning (clock-wise or anti-clock-wise). The LM393 speed sensor could be used in combination with an additional information about the current driving direction, for example coming from the software. Howerver, this won't be as accurate as using a quadrature encoder that provides this information. To measure how far the robot has driven, we use the LM393 speed sensor from Joy-IT as odometry sensor. First, we will create a ROS package with catkin create pkg PKG_NAME [--catkin-deps [DEP [DEP ...]]] : 1 2 3 4 5 6 7 fjp@ubuntu:~/git/2wd-robot/ros/src$ catkin create pkg lm393_speed_sensor --catkin-deps rospy roscpp nav_msgs Creating package \"lm393_speed_sensor\" in \"/home/fjp/git/2wd-robot/ros/src\" ... Created file lm393_speed_sensor/CMakeLists.txt Created file lm393_speed_sensor/package.xml Created folder lm393_speed_sensor/include/lm393_speed_sensor Created folder lm393_speed_sensor/src Successfully created package files in /home/fjp/git/2wd-robot/ros/src/lm393_speed_sensor. The package depends on the two ROS client libraries rospy and roscpp . The current implementation uses python and the RPi.GPIO library for interrupts. To achieve more percise results, C++ should be used instead. To signalise the current pose of the robot in the odometry frame, the nav_msgs/Range message is used.","title":"LM393 Speed Sensor - Odometry"},{"location":"lm393_speed_sensor.html#connection","text":"To get the speed sensors working, we connect the signal pins to (physical) GPIO 15 and (physical) GPIO 16 of the Raspberry Pi 4 B and power them with 3.3V. The ground pins are connected to ground of the Pi.","title":"Connection"},{"location":"lm393_speed_sensor.html#lm393-speed-sensor-library","text":"To use the LM393 speed sensor as a ROS node the sensor functionality is wraped in a class. This provides an easy to extend interface for the speed sensor ( API ) The code consists of a class LM393SpeedSensor which has two interrupt service routines (ISR) methods. Using the RPi.GPIO interrupt capabilities, these ISR methods are used as callback functions when the sensor measures a falling edge. This is the case when the rotary disk spins and the optocoupler measures a high to low signal due to the spinning disk. The sensor API is implemented in the lm393_speed_sensor.py python module. Executing the module will result in the following output when the motors are spinning freely with full speed and using some force to slow them down. We see that the rotational speed \\(\\omega\\) , measured in RPM ( revolutions per minute ) changes. 1 2 fjp@ubuntu:~/git/2wd-robot/ros/src/lm393_speed_sensor/src$ sudo python lm393_speed_sensor.py TODO The rotational speed \\(\\omega\\) values will be converted to tangential velocity values using the radius \\(r\\) of the wheels. \\[ v = \\omega \\cdot r = 2 \\pi n \\cdot r \\]","title":"LM393 Speed Sensor Library"},{"location":"lm393_speed_sensor.html#ros-node-for-lm393-speed-sensor","text":"ROS provides the Odometry Message in the nav_msgs header . This message type can be used to write a wrapper that will act as a ROS node for the LM393 speed sensor. To design this node we will send out measurements periodically over a topic of type nav_msgs/Odometry . The code for this node is in speed_sensor.py . After writing the node we need to build the packages in the workspace with catkin build . 1 2 fjp@ubuntu:~/git/2wd-robot/ros$ catkin build TODO As the final note of the build output suggests, we have to source the setup.bash files in the devel space. 1 fjp@ubuntu:~/git/2wd-robot/ros$ source devel/setup.bash To make the speed_sensor node executable we have to modify the speed_sensor.py file: 1 fjp@ubuntu:~/git/2wd-robot/ros/src/grove_ultrasonic_ranger/src$ sudo chmod a+x speed_sensor.py Then we can test the node using rosrun : 1 2 3 4 fjp@ubuntu:~/git/2wd-robot/ros$ sudo su [ sudo ] password for fjp: root@ubuntu:/home/fjp/git/2wd-robot/ros# source devel/setup.bash root@ubuntu:/home/fjp/git/2wd-robot/ros# rosrun lm393_speed_sensor speed_sensor.py This lets the node publish range messages which we can capture in another terminal window using rostopic . First we use rostopic list to find the name of the topic we are interested in: 1 2 fjp@ubuntu:~/git/2wd-robot/ros$ rostopic list TODO We named our topic /odom which we can use with the rostopic echo command to see the published messages:","title":"ROS Node for LM393 Speed Sensor"},{"location":"oak.html","text":"OpenCV AI Kit The OpenCV AI Kit (OAK) is an open source hardware and software project for spatial AI and is a result of a very successful Kickstarter Campaign . The hardware can basically be separated into two devices: OAK\u2014D is a spatial AI powerhouse, capable of simultaneously running advanced neural networks while providing depth from two stereo cameras and color information from a single 4K camera in the center. The OAK\u2014D hardware comes with a 1 meter USB-C cable and a 5V power supply. OAK\u20141 is the tiny-but-mighty 4K camera capable of running the same advanced neural networks as OAK\u2014D, but in an even more minuscule form factor for projects where space and power are at a premium. Each OAK\u20141 kit includes the OAK\u20141 module with aluminum enclosure, 1 meter USB 3 Type-A to Type-C cable, and getting started guide. Note There are also options for onboard Wifi and Power over Ethernet (POE). !!! quote What is spacial AI and 3D Object Localization? First, it is necessary to define what 'Object Detection' is: It is the technical term for finding the bounding box of an object of interest, in pixel space (i.e. pixel coordinates), in an image. 1 2 3D Object Localization (or 3D Object Detection), is all about finding such objects in physical space, instead of pixel space. This is useful when trying to real-time measure or interact with the physical world. This part of the documentation explains the following Installing software for OAK on your host (Linux, macOS, Windows, Raspberry Pi) Interacting with OAK and how to run the demo programs Deploy a custom model with OpenVINO Toolkit Training your own Model with SuperAnnotate Train your own Model on a GPU, either locally or in the Cloud, using e.g. Tensorflow Keras. Project using OAK Setup of OAK-1 and OAK-D The documentation of OAK can be found at docs.luxonis.com . To install the software for both devices on your different host platforms please follow the instructions in the Python API . TODO install command for different os Test installation instructions Interaction with OAK Demo Program Overview Examples Object Detection with Yolo-v3 ... Python API: DepthAI Deplyinig Neural Network Models to OAK SuperAnnotate Training Custom Models","title":"OpenCV AI Kit"},{"location":"oak.html#opencv-ai-kit","text":"The OpenCV AI Kit (OAK) is an open source hardware and software project for spatial AI and is a result of a very successful Kickstarter Campaign . The hardware can basically be separated into two devices: OAK\u2014D is a spatial AI powerhouse, capable of simultaneously running advanced neural networks while providing depth from two stereo cameras and color information from a single 4K camera in the center. The OAK\u2014D hardware comes with a 1 meter USB-C cable and a 5V power supply. OAK\u20141 is the tiny-but-mighty 4K camera capable of running the same advanced neural networks as OAK\u2014D, but in an even more minuscule form factor for projects where space and power are at a premium. Each OAK\u20141 kit includes the OAK\u20141 module with aluminum enclosure, 1 meter USB 3 Type-A to Type-C cable, and getting started guide. Note There are also options for onboard Wifi and Power over Ethernet (POE). !!! quote What is spacial AI and 3D Object Localization? First, it is necessary to define what 'Object Detection' is: It is the technical term for finding the bounding box of an object of interest, in pixel space (i.e. pixel coordinates), in an image. 1 2 3D Object Localization (or 3D Object Detection), is all about finding such objects in physical space, instead of pixel space. This is useful when trying to real-time measure or interact with the physical world. This part of the documentation explains the following Installing software for OAK on your host (Linux, macOS, Windows, Raspberry Pi) Interacting with OAK and how to run the demo programs Deploy a custom model with OpenVINO Toolkit Training your own Model with SuperAnnotate Train your own Model on a GPU, either locally or in the Cloud, using e.g. Tensorflow Keras. Project using OAK","title":"OpenCV AI Kit"},{"location":"oak.html#setup-of-oak-1-and-oak-d","text":"The documentation of OAK can be found at docs.luxonis.com . To install the software for both devices on your different host platforms please follow the instructions in the Python API . TODO install command for different os Test installation instructions","title":"Setup of OAK-1 and OAK-D"},{"location":"oak.html#interaction-with-oak","text":"Demo Program Overview Examples Object Detection with Yolo-v3 ...","title":"Interaction with OAK"},{"location":"oak.html#python-api-depthai","text":"","title":"Python API: DepthAI"},{"location":"oak.html#deplyinig-neural-network-models-to-oak","text":"","title":"Deplyinig Neural Network Models to OAK"},{"location":"oak.html#superannotate","text":"","title":"SuperAnnotate"},{"location":"oak.html#training-custom-models","text":"","title":"Training Custom Models"},{"location":"packages.html","text":"Diffbot ROS Packages The following describes the easiest way to make use of diffbot's ROS packages inside the ros-mobile-robots/diffbot repository. The following steps will be performed on both, the workstation/development PC and the single board computer (SBC). Git: clone diffbot repository After setting up ROS on your workstation PC and the SBC (either Raspberry Pi 4B or Jetson Nano ), create a ros workspace in your users home folder and clone the diffbot repository : 1 2 mkdir -p ros_ws/src git clone https://github.com/ros-mobile-robots/diffbot.git Obtain (system) Dependencies The diffbot repository relies on two sorts of dependencies: Source (non binary) dependencies from other (git) repositories. System dependencies available in the (ROS) Ubuntu package repositories. Also refered to as pre built binaries. Source Dependencies Let's first obtain source dependencies from other repositories. To do this the recommended tool to use is vcstool (see also https://github.com/dirk-thomas/vcstool for additional documentation and examples.). Note vcstool replaces wstool . Inside the cloned diffbot repository, make use of the import command and the diffbot.repos file containing the required source repositories: 1 vcs import < diffbot.repos This will clone all repositories which are stored in the diffbot.repos that get passed in via stdin in YAML format. Note The file diffbot.repos contains relative paths and will clone the listed repositories in the parent folder from where the vcs import command is called. When it is called from inside the diffbot repository, which should be located in the src folder of a catkin workspace, then the other repositories are also cloned in the src folder. For the SBC not all dependencies in diffbot.repos are needed. Instead the diffbot_robot.repos is here to clone the rplidar_ros repository. 1 vcs import < diffbot_robot.repos Now that additional packages are inside the catkin workspace it is time to install the system dependencies. System Dependencies All the needed ROS system dependencies which are required by diffbot's packages can be installed using rosdep command, which was installed during the ROS setup. To install all system dependencies use the following command: 1 rosdep install --from-paths src --ignore-src -r -y Info On the following packages pages it is explained that the dependencies of a ROS package are defined inside its package.xml . After the installation of all dependencies finished (which can take a while), it is time to build the catkin workspace. Inside the workspace use catkin-tools to build the packages inside the src folder. Note The first time you run the following command, make sure to execute it inside your catkin workspace and not the src directory. 1 catkin build Now source the catkin workspace either using the created alias or the full command for the bash shell: 1 source devel/setup.bash Examples Now you are ready to follow the examples listed in the readme. Info TODO extend documentation with examples","title":"Packages"},{"location":"packages.html#diffbot-ros-packages","text":"The following describes the easiest way to make use of diffbot's ROS packages inside the ros-mobile-robots/diffbot repository. The following steps will be performed on both, the workstation/development PC and the single board computer (SBC).","title":"Diffbot ROS Packages"},{"location":"packages.html#git-clone-diffbot-repository","text":"After setting up ROS on your workstation PC and the SBC (either Raspberry Pi 4B or Jetson Nano ), create a ros workspace in your users home folder and clone the diffbot repository : 1 2 mkdir -p ros_ws/src git clone https://github.com/ros-mobile-robots/diffbot.git","title":"Git: clone diffbot repository"},{"location":"packages.html#obtain-system-dependencies","text":"The diffbot repository relies on two sorts of dependencies: Source (non binary) dependencies from other (git) repositories. System dependencies available in the (ROS) Ubuntu package repositories. Also refered to as pre built binaries.","title":"Obtain (system) Dependencies"},{"location":"packages.html#source-dependencies","text":"Let's first obtain source dependencies from other repositories. To do this the recommended tool to use is vcstool (see also https://github.com/dirk-thomas/vcstool for additional documentation and examples.). Note vcstool replaces wstool . Inside the cloned diffbot repository, make use of the import command and the diffbot.repos file containing the required source repositories: 1 vcs import < diffbot.repos This will clone all repositories which are stored in the diffbot.repos that get passed in via stdin in YAML format. Note The file diffbot.repos contains relative paths and will clone the listed repositories in the parent folder from where the vcs import command is called. When it is called from inside the diffbot repository, which should be located in the src folder of a catkin workspace, then the other repositories are also cloned in the src folder. For the SBC not all dependencies in diffbot.repos are needed. Instead the diffbot_robot.repos is here to clone the rplidar_ros repository. 1 vcs import < diffbot_robot.repos Now that additional packages are inside the catkin workspace it is time to install the system dependencies.","title":"Source Dependencies"},{"location":"packages.html#system-dependencies","text":"All the needed ROS system dependencies which are required by diffbot's packages can be installed using rosdep command, which was installed during the ROS setup. To install all system dependencies use the following command: 1 rosdep install --from-paths src --ignore-src -r -y Info On the following packages pages it is explained that the dependencies of a ROS package are defined inside its package.xml . After the installation of all dependencies finished (which can take a while), it is time to build the catkin workspace. Inside the workspace use catkin-tools to build the packages inside the src folder. Note The first time you run the following command, make sure to execute it inside your catkin workspace and not the src directory. 1 catkin build Now source the catkin workspace either using the created alias or the full command for the bash shell: 1 source devel/setup.bash","title":"System Dependencies"},{"location":"packages.html#examples","text":"Now you are ready to follow the examples listed in the readme. Info TODO extend documentation with examples","title":"Examples"},{"location":"power-supply.html","text":"Power Supply DiffBot uses two different power sources. One USB type C power bank providing 5V to the Raspberry Pi 4B. The other source comes from battery holder, mounted at the bottom of the base plate which can hold up to four AA sized batteries. The standard AA battery has a charge of 2700 mAh and a nominal Voltage of 1.5 V. This means that with four batteries in series the total Voltage supplied to the motor driver would be 6 V. The voltage drop on the motor driver is roughly 2.12 V which means that each motor would operate on 3.88 V. According to the datasheet the DG01D-E motor requires a voltage between 3-9 V, has a gearbox ratio of 1:48 and a speed of 90RPM at 4.5V. Therefore 3.88 V is at the lower operating range of this motor, hence a higher input source to the motor driver is requried to provide higher output voltage to the motors. The maximum allowed input Voltage to the motor driver is 15 V, hence a power source between 6 V to 15 V should be used, in the range of 10 V to 15 V to drive with different speeds. In order to reuse the battery holder on the bottom plate a battery with the same AA form factor but higher voltage is what we are looking for. A helpful overview of different batteris with their exact naming can be found in the List of battery sizes on Wikipedia. Here we can see that the 14500 recharchable Lithium-ion battery provides 3.7 V with a capacity of approximately 900 mAh. This means that four batteries of these provide 14.8 V which is right below the maximum 15 V that the motor driver can handle. Although the capacity is lower compared to non recharchable AA batteries (2700 mAh), these type of batteries can be reused multiple times when charged with a suitable battery charchger such as the Nitecore UMS4 or the XTAR VC4 . DiffBot uses four 14500 batteries from Trustfire with 3.7 V and 900 mAh. Note that four standard AA bateries with 1.5 V each is perfectly fine. Only the motor driver has to provide 100% of its input voltage to avoid stalling the motors. With higher Voltage rated batteries more different speeds will be possible.","title":"Power Supply"},{"location":"power-supply.html#power-supply","text":"DiffBot uses two different power sources. One USB type C power bank providing 5V to the Raspberry Pi 4B. The other source comes from battery holder, mounted at the bottom of the base plate which can hold up to four AA sized batteries. The standard AA battery has a charge of 2700 mAh and a nominal Voltage of 1.5 V. This means that with four batteries in series the total Voltage supplied to the motor driver would be 6 V. The voltage drop on the motor driver is roughly 2.12 V which means that each motor would operate on 3.88 V. According to the datasheet the DG01D-E motor requires a voltage between 3-9 V, has a gearbox ratio of 1:48 and a speed of 90RPM at 4.5V. Therefore 3.88 V is at the lower operating range of this motor, hence a higher input source to the motor driver is requried to provide higher output voltage to the motors. The maximum allowed input Voltage to the motor driver is 15 V, hence a power source between 6 V to 15 V should be used, in the range of 10 V to 15 V to drive with different speeds. In order to reuse the battery holder on the bottom plate a battery with the same AA form factor but higher voltage is what we are looking for. A helpful overview of different batteris with their exact naming can be found in the List of battery sizes on Wikipedia. Here we can see that the 14500 recharchable Lithium-ion battery provides 3.7 V with a capacity of approximately 900 mAh. This means that four batteries of these provide 14.8 V which is right below the maximum 15 V that the motor driver can handle. Although the capacity is lower compared to non recharchable AA batteries (2700 mAh), these type of batteries can be reused multiple times when charged with a suitable battery charchger such as the Nitecore UMS4 or the XTAR VC4 . DiffBot uses four 14500 batteries from Trustfire with 3.7 V and 900 mAh. Note that four standard AA bateries with 1.5 V each is perfectly fine. Only the motor driver has to provide 100% of its input voltage to avoid stalling the motors. With higher Voltage rated batteries more different speeds will be possible.","title":"Power Supply"},{"location":"robot-description.html","text":"DiffBot Robot Description The description of the 2WD robot will be created in its own package named diffbot_description . The description uses URDF and xacro . For this we create a package with catkin create pkg PKG_NAME [--catkin-deps [DEP [DEP ...]]] : 1 2 3 4 5 fjp@ubuntu:~/git/diffbot/ros/src$ catkin create pkg diffbot_description Creating package \"diffbot_description\" in \"/home/fjp/git/diffbot/ros/src\"... Created file diffbot_description/CMakeLists.txt Created file diffbot_description/package.xml Successfully created package files in /home/fjp/git/diffbot/ros/src/diffbot_description. Because this package contains only descriptions and launch files it doesn't require any dependencies. According to ROS conventions we create the following folders where the individual files realted to the robot description will be placed: 1 fjp@ubuntu:~/git/diffbot/ros/src/robot_description$ mkdir urdf meshes launch The urdf folder will be used to keep the urdf and xacro files. The meshes folder keeps the meshes that are included in the urdf file, and the launch folder keeps the ROS launch files. Robot Model To model the two wheeled differential drive robot we follow REP-120 . It states to use a base_link and a base_footprint . The resulting description files can be found in the diffbot_description package. Required Tools To check a urdf file we can make use of the tools check_urdf and urdf_to_graphiz in the liburdfdom-tools debian package. Install it with the following command: 1 sudo apt install liburdfdom-tools First we need to convert the robot description of DiffBot, which is present as xacro file, to a urdf file by issuing the following command: 1 fjp@ubuntu:~/git/diffbot/ros$ rosrun xacro xacro ` rospack find diffbot_description ` /urdf/diffbot.urdf.xacro -o /tmp/diffbot.urdf After we've created the urdf from the xacro file we can check the urdf files for errors with: 1 2 3 4 5 6 7 8 fjp@ubuntu:/tmp$ check_urdf diffbot.urdf robot name is: diffbot ---------- Successfully Parsed XML --------------- root Link: base_footprint has 1 child(ren) child(1): base_link child(1): caster_link child(2): front_left_wheel child(3): front_right_wheel It is also helpful to output a graphviz diagram of the robot model: 1 2 3 4 fjp@ubuntu:/tmp$ urdf_to_graphiz diffbot.urdf Created file diffbot.gv Created file diffbot.pdf fjp@ubuntu:/tmp$ evince diffbot.pdf Graphviz diagram of DiffBot URDF robot description. To visualize the 3D model in RViz we first need to install the joint-state-publisher-gui which was separated from non-gui joint-state-publisher . There exists a debian package which can be installed with the following command: 1 fjp@ubuntu:~/git/diffbot/ros$ sudo apt install ros-noetic-joint-state-publisher-gui After installing the required dependency, the view_diffbot.launch launch file can be executed using roslaunch command: 1 fjp@ubuntu:~/git/diffbot/ros$ roslaunch diffbot_description view_diffbot.launch According to the launch file's configuration, this will show the robot in RViz together with the joint-state-publisher-gui to set the joint values: DiffBot displayed in RViz. With the robot descripton loaded on the ROS parameter server, it's possible to use the TF Tree rqt plugin to display the transformation tree (see image above). In the next section, Gazebo Simulation , the robot model is prepared for simulation inside of Gazebo . URDF in Gazebo http://gazebosim.org/tutorials?tut=ros_urdf&cat=connect_ros#Tutorial:UsingaURDFinGazebo References http://gazebosim.org/tutorials?tut=ros_urdf&cat=connect_ros#Tutorial:UsingaURDFinGazebo https://answers.ros.org/question/30539/choosing-the-right-coefficients-for-gazebo-simulation/ https://answers.ros.org/question/231880/how-to-improve-amcl-pose-estimate/","title":"Robot Description"},{"location":"robot-description.html#diffbot-robot-description","text":"The description of the 2WD robot will be created in its own package named diffbot_description . The description uses URDF and xacro . For this we create a package with catkin create pkg PKG_NAME [--catkin-deps [DEP [DEP ...]]] : 1 2 3 4 5 fjp@ubuntu:~/git/diffbot/ros/src$ catkin create pkg diffbot_description Creating package \"diffbot_description\" in \"/home/fjp/git/diffbot/ros/src\"... Created file diffbot_description/CMakeLists.txt Created file diffbot_description/package.xml Successfully created package files in /home/fjp/git/diffbot/ros/src/diffbot_description. Because this package contains only descriptions and launch files it doesn't require any dependencies. According to ROS conventions we create the following folders where the individual files realted to the robot description will be placed: 1 fjp@ubuntu:~/git/diffbot/ros/src/robot_description$ mkdir urdf meshes launch The urdf folder will be used to keep the urdf and xacro files. The meshes folder keeps the meshes that are included in the urdf file, and the launch folder keeps the ROS launch files.","title":"DiffBot Robot Description"},{"location":"robot-description.html#robot-model","text":"To model the two wheeled differential drive robot we follow REP-120 . It states to use a base_link and a base_footprint . The resulting description files can be found in the diffbot_description package.","title":"Robot Model"},{"location":"robot-description.html#required-tools","text":"To check a urdf file we can make use of the tools check_urdf and urdf_to_graphiz in the liburdfdom-tools debian package. Install it with the following command: 1 sudo apt install liburdfdom-tools First we need to convert the robot description of DiffBot, which is present as xacro file, to a urdf file by issuing the following command: 1 fjp@ubuntu:~/git/diffbot/ros$ rosrun xacro xacro ` rospack find diffbot_description ` /urdf/diffbot.urdf.xacro -o /tmp/diffbot.urdf After we've created the urdf from the xacro file we can check the urdf files for errors with: 1 2 3 4 5 6 7 8 fjp@ubuntu:/tmp$ check_urdf diffbot.urdf robot name is: diffbot ---------- Successfully Parsed XML --------------- root Link: base_footprint has 1 child(ren) child(1): base_link child(1): caster_link child(2): front_left_wheel child(3): front_right_wheel It is also helpful to output a graphviz diagram of the robot model: 1 2 3 4 fjp@ubuntu:/tmp$ urdf_to_graphiz diffbot.urdf Created file diffbot.gv Created file diffbot.pdf fjp@ubuntu:/tmp$ evince diffbot.pdf Graphviz diagram of DiffBot URDF robot description. To visualize the 3D model in RViz we first need to install the joint-state-publisher-gui which was separated from non-gui joint-state-publisher . There exists a debian package which can be installed with the following command: 1 fjp@ubuntu:~/git/diffbot/ros$ sudo apt install ros-noetic-joint-state-publisher-gui After installing the required dependency, the view_diffbot.launch launch file can be executed using roslaunch command: 1 fjp@ubuntu:~/git/diffbot/ros$ roslaunch diffbot_description view_diffbot.launch According to the launch file's configuration, this will show the robot in RViz together with the joint-state-publisher-gui to set the joint values: DiffBot displayed in RViz. With the robot descripton loaded on the ROS parameter server, it's possible to use the TF Tree rqt plugin to display the transformation tree (see image above). In the next section, Gazebo Simulation , the robot model is prepared for simulation inside of Gazebo .","title":"Required Tools"},{"location":"robot-description.html#urdf-in-gazebo","text":"http://gazebosim.org/tutorials?tut=ros_urdf&cat=connect_ros#Tutorial:UsingaURDFinGazebo","title":"URDF in Gazebo"},{"location":"robot-description.html#references","text":"http://gazebosim.org/tutorials?tut=ros_urdf&cat=connect_ros#Tutorial:UsingaURDFinGazebo https://answers.ros.org/question/30539/choosing-the-right-coefficients-for-gazebo-simulation/ https://answers.ros.org/question/231880/how-to-improve-amcl-pose-estimate/","title":"References"},{"location":"ros-network-setup.html","text":"ROS Network Setup ROS is a distributed computing environment. This allows running compute expensive tasks such as visualization or path planning for navigation on machines with more performance and sending goals to robots operating on less performant hardware like DiffBot with its Raspberry Pi 4 B. For detailed instructions see ROS Network Setup and the ROS Environment Variables . The setup between the work machine that handles compute heavy tasks and DiffBot is as follows: TODO image On DiffBot we configure the ROS_MASTER_URI to be the IP address of the work machine. 1 export ROS_MASTER_URI=http://192.168.0.9:11311/ To test run the master on the work machine using the roscore command. Then run the the listener from the roscpp_tutorials package in another terminal: 1 fjp@workmachine:~/git/diffbot/ros$ rosrun roscpp_tutorials listener Then switch to a terminal on DiffBot's Raspberry Pi and run the talker from roscpp_tutorials : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 fjp@diffbot:~/git/diffbot/ros$ rosrun roscpp_tutorials talker [ INFO] [1602018325.633133449]: hello world 0 [ INFO] [1602018325.733137152]: hello world 1 [ INFO] [1602018325.833112540]: hello world 2 [ INFO] [1602018325.933114483]: hello world 3 [ INFO] [1602018326.033114093]: hello world 4 [ INFO] [1602018326.133112684]: hello world 5 [ INFO] [1602018326.233112183]: hello world 6 [ INFO] [1602018326.333113126]: hello world 7 [ INFO] [1602018326.433113680]: hello world 8 [ INFO] [1602018326.533113031]: hello world 9 [ INFO] [1602018326.633110140]: hello world 10 [ INFO] [1602018326.733108954]: hello world 11 [ INFO] [1602018326.833113267]: hello world 12 [ INFO] [1602018326.933164505]: hello world 13 [ INFO] [1602018327.033119135]: hello world 14 [ INFO] [1602018327.133113559]: hello world 15 [ INFO] [1602018327.233111003]: hello world 16 [ INFO] [1602018327.333110705]: hello world 17 [ INFO] [1602018327.433126425]: hello world 18 [ INFO] [1602018327.533111498]: hello world 19 [ INFO] [1602018327.633107978]: hello world 20 [ INFO] [1602018327.733110736]: hello world 21 [ INFO] [1602018327.833107605]: hello world 22 [ INFO] [1602018327.933111659]: hello world 23 [ INFO] [1602018328.033108065]: hello world 24 [ INFO] [1602018328.133110379]: hello world 25 [ INFO] [1602018328.233150191]: hello world 26 [ INFO] [1602018328.333135986]: hello world 27 [ INFO] [1602018328.433153558]: hello world 28 [ INFO] [1602018328.533154557]: hello world 29 [ INFO] [1602018328.633151667]: hello world 30 [ INFO] [1602018328.733128777]: hello world 31 [ INFO] [1602018328.833170108]: hello world 32 [ INFO] [1602018328.933172402]: hello world 33 Looking back at the terminal on the work machine you should see the output: 1 2 3 4 5 6 7 8 fjp.github.io git:(master) rosrun roscpp_tutorials listener [ INFO] [1602018328.330070016]: I heard: [hello world 27] [ INFO] [1602018328.430244670]: I heard: [hello world 28] [ INFO] [1602018328.530173113]: I heard: [hello world 29] [ INFO] [1602018328.630251690]: I heard: [hello world 30] [ INFO] [1602018328.730334064]: I heard: [hello world 31] [ INFO] [1602018328.830346566]: I heard: [hello world 32] [ INFO] [1602018328.930009032]: I heard: [hello world 33] Note that it can take some time receiving the messages from DiffBot on the work machine, which we can see from the time stamps in the outputs above.","title":"ROS Network Setup"},{"location":"ros-network-setup.html#ros-network-setup","text":"ROS is a distributed computing environment. This allows running compute expensive tasks such as visualization or path planning for navigation on machines with more performance and sending goals to robots operating on less performant hardware like DiffBot with its Raspberry Pi 4 B. For detailed instructions see ROS Network Setup and the ROS Environment Variables . The setup between the work machine that handles compute heavy tasks and DiffBot is as follows: TODO image On DiffBot we configure the ROS_MASTER_URI to be the IP address of the work machine. 1 export ROS_MASTER_URI=http://192.168.0.9:11311/ To test run the master on the work machine using the roscore command. Then run the the listener from the roscpp_tutorials package in another terminal: 1 fjp@workmachine:~/git/diffbot/ros$ rosrun roscpp_tutorials listener Then switch to a terminal on DiffBot's Raspberry Pi and run the talker from roscpp_tutorials : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 fjp@diffbot:~/git/diffbot/ros$ rosrun roscpp_tutorials talker [ INFO] [1602018325.633133449]: hello world 0 [ INFO] [1602018325.733137152]: hello world 1 [ INFO] [1602018325.833112540]: hello world 2 [ INFO] [1602018325.933114483]: hello world 3 [ INFO] [1602018326.033114093]: hello world 4 [ INFO] [1602018326.133112684]: hello world 5 [ INFO] [1602018326.233112183]: hello world 6 [ INFO] [1602018326.333113126]: hello world 7 [ INFO] [1602018326.433113680]: hello world 8 [ INFO] [1602018326.533113031]: hello world 9 [ INFO] [1602018326.633110140]: hello world 10 [ INFO] [1602018326.733108954]: hello world 11 [ INFO] [1602018326.833113267]: hello world 12 [ INFO] [1602018326.933164505]: hello world 13 [ INFO] [1602018327.033119135]: hello world 14 [ INFO] [1602018327.133113559]: hello world 15 [ INFO] [1602018327.233111003]: hello world 16 [ INFO] [1602018327.333110705]: hello world 17 [ INFO] [1602018327.433126425]: hello world 18 [ INFO] [1602018327.533111498]: hello world 19 [ INFO] [1602018327.633107978]: hello world 20 [ INFO] [1602018327.733110736]: hello world 21 [ INFO] [1602018327.833107605]: hello world 22 [ INFO] [1602018327.933111659]: hello world 23 [ INFO] [1602018328.033108065]: hello world 24 [ INFO] [1602018328.133110379]: hello world 25 [ INFO] [1602018328.233150191]: hello world 26 [ INFO] [1602018328.333135986]: hello world 27 [ INFO] [1602018328.433153558]: hello world 28 [ INFO] [1602018328.533154557]: hello world 29 [ INFO] [1602018328.633151667]: hello world 30 [ INFO] [1602018328.733128777]: hello world 31 [ INFO] [1602018328.833170108]: hello world 32 [ INFO] [1602018328.933172402]: hello world 33 Looking back at the terminal on the work machine you should see the output: 1 2 3 4 5 6 7 8 fjp.github.io git:(master) rosrun roscpp_tutorials listener [ INFO] [1602018328.330070016]: I heard: [hello world 27] [ INFO] [1602018328.430244670]: I heard: [hello world 28] [ INFO] [1602018328.530173113]: I heard: [hello world 29] [ INFO] [1602018328.630251690]: I heard: [hello world 30] [ INFO] [1602018328.730334064]: I heard: [hello world 31] [ INFO] [1602018328.830346566]: I heard: [hello world 32] [ INFO] [1602018328.930009032]: I heard: [hello world 33] Note that it can take some time receiving the messages from DiffBot on the work machine, which we can see from the time stamps in the outputs above.","title":"ROS Network Setup"},{"location":"ros-setup.html","text":"ROS Installation The robot setup is supposed to run on Ubuntu Mate 20.04 Focal Fossa. ROS Noetic is intended to run with this Ubuntu version. To install ROS follow the installation instructions . Info In the 1.4 Installation step](http://wiki.ros.org/noetic/Installation/Ubuntu#Installation-1) you have to choose how much of ROS you want to install. For the development pc you can go with the sudo apt install ros-noetic-desktop-full command. For the robot install the ros-noetic-robot Ubuntu package. Other system dependencies will be installed with the rosdep command, explained in the following section. Another program that is required to run ROS nodes written with the rospy client library is python-is-python3 . Install it with: 1 sudo apt install python-is-python3 Dependencies After having git cloned one or more ROS packages, such as diffbot , it is necessary to install system dependencies of the packages in the catkin workspace. For this, ROS provides the rosdep tool. To install all system dependencies of the packages in your catkin workspace make use of the following command ( source ): 1 rosdep install --from-paths src --ignore-src -r -y This will go through each package's package.xml file and install the listed dependencies that are currently not installed on your system. Build Tool: catkin_tools To work with ROS we will use catkin_tools instead of catkin_make . catkin_tools provide commands such as catkin build which we will use instead of catkin_make because the catkin_tools are more actively developed than catkin_make ref . Note It is recommended to use catkin_tools instead of the default catkin when building ROS workspaces. catkin_tools provides a number of benefits over regular catkin_make and will be used in the documentation. All packages can be built using catkin_make however: use catkin_make in place of catkin build where appropriate. Bug The current way to install catkin-tools in the documentation from the Ubuntu package repository doesn't work. Follow the steps below instead for now. Success As of now the correct way to install catkin-tools is to use the following command: 1 sudo apt-get install python3-osrf-pycommon python3-catkin-tools For your reference, you can read more about it in this open issue . After sucessfully installing catkin_tools we can create and initialize a workspace (called ros for this project) with the commands listed in the build_tools documentation : Note Note that we already source d the setup.bash while following the ROS installation instructions . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 fjp@ubuntu:~/git/2wd-robot/ros$ mkdir -p ~/git/2wd-robot/ros/src # Make a new workspace and source space fjp@ubuntu:~/git/2wd-robot/ros$ cd ~/git/2wd-robot/ros # Navigate to the workspace root fjp@ubuntu:~/git/2wd-robot/ros$ catkin init # Initialize it with a hidden marker file Initializing catkin workspace in ` /home/fjp/git/2wd-robot/ros ` . ---------------------------------------------------------------- Profile: default Extending: [ env ] /opt/ros/melodic Workspace: /home/fjp/git/2wd-robot/ros ---------------------------------------------------------------- Build Space: [ missing ] /home/fjp/git/2wd-robot/ros/build Devel Space: [ missing ] /home/fjp/git/2wd-robot/ros/devel Install Space: [ unused ] /home/fjp/git/2wd-robot/ros/install Log Space: [ missing ] /home/fjp/git/2wd-robot/ros/logs Source Space: [ exists ] /home/fjp/git/2wd-robot/ros/src DESTDIR: [ unused ] None ---------------------------------------------------------------- Devel Space Layout: linked Install Space Layout: None ---------------------------------------------------------------- Additional CMake Args: None Additional Make Args: None Additional catkin Make Args: None Internal Make Job Server: True Cache Job Environments: False ---------------------------------------------------------------- Whitelisted Packages: None Blacklisted Packages: None ---------------------------------------------------------------- Workspace configuration appears valid. ---------------------------------------------------------------- Command Overview of catkin_tools To create packages , which will be covered in the next posts in more depth, we will use catkin create pkg PKG_NAME . Building the workspace is done with catkin build . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 fjp@ubuntu:~/git/2wd-robot/ros$ catkin build ---------------------------------------------------------------- Profile: default Extending: [ env ] /opt/ros/melodic Workspace: /home/fjp/git/2wd-robot/ros ---------------------------------------------------------------- Build Space: [ exists ] /home/fjp/git/2wd-robot/ros/build Devel Space: [ exists ] /home/fjp/git/2wd-robot/ros/devel Install Space: [ unused ] /home/fjp/git/2wd-robot/ros/install Log Space: [ missing ] /home/fjp/git/2wd-robot/ros/logs Source Space: [ exists ] /home/fjp/git/2wd-robot/ros/src DESTDIR: [ unused ] None ---------------------------------------------------------------- Devel Space Layout: linked Install Space Layout: None ---------------------------------------------------------------- Additional CMake Args: None Additional Make Args: None Additional catkin Make Args: None Internal Make Job Server: True Cache Job Environments: False ---------------------------------------------------------------- Whitelisted Packages: None Blacklisted Packages: None ---------------------------------------------------------------- Workspace configuration appears valid. NOTE: Forcing CMake to run for each package. ---------------------------------------------------------------- [ build ] No packages were found in the source space '/home/fjp/git/2wd-robot/ros/src' [ build ] No packages to be built. [ build ] Package table is up to date. Starting >>> catkin_tools_prebuild Finished <<< catkin_tools_prebuild [ 10 .0 seconds ] [ build ] Summary: All 1 packages succeeded! [ build ] Ignored: None. [ build ] Warnings: None. [ build ] Abandoned: None. [ build ] Failed: None. [ build ] Runtime: 10 .1 seconds total. Environment Setup Finally the newly built packages have to be loaded in the environment using source . 1 fjp@ubuntu:~/git/2wd-robot/ros$ source ~/git/2wd-robot/ros/devel/setup.bash # Load the workspace's environment Tip To avoid tediously typing the above source command, it is convenient to create an alias in your ~/.bashrc or ~/.zshrc similar to the following: 1 alias s='source devel/setup.bash' or using the absolute path 1 alias sa='source ~/git/2wd-robot/ros/devel/setup.bash' It is recommended to use the correct setup script for the shell you use ( bash , zsh , etc.). In case you are unsure, you can check with the echo $SHELL command which will most likely output /bin/bash . Info Instead of source it is possible to use the . command instead. Don't confuse it though with the current directory, which is also represented as . . Resources Although the catkin tutorial uses catkin_make it provides a helpful guide to create a workspace","title":"ROS Setup"},{"location":"ros-setup.html#ros-installation","text":"The robot setup is supposed to run on Ubuntu Mate 20.04 Focal Fossa. ROS Noetic is intended to run with this Ubuntu version. To install ROS follow the installation instructions . Info In the 1.4 Installation step](http://wiki.ros.org/noetic/Installation/Ubuntu#Installation-1) you have to choose how much of ROS you want to install. For the development pc you can go with the sudo apt install ros-noetic-desktop-full command. For the robot install the ros-noetic-robot Ubuntu package. Other system dependencies will be installed with the rosdep command, explained in the following section. Another program that is required to run ROS nodes written with the rospy client library is python-is-python3 . Install it with: 1 sudo apt install python-is-python3","title":"ROS Installation"},{"location":"ros-setup.html#dependencies","text":"After having git cloned one or more ROS packages, such as diffbot , it is necessary to install system dependencies of the packages in the catkin workspace. For this, ROS provides the rosdep tool. To install all system dependencies of the packages in your catkin workspace make use of the following command ( source ): 1 rosdep install --from-paths src --ignore-src -r -y This will go through each package's package.xml file and install the listed dependencies that are currently not installed on your system.","title":"Dependencies"},{"location":"ros-setup.html#build-tool-catkin_tools","text":"To work with ROS we will use catkin_tools instead of catkin_make . catkin_tools provide commands such as catkin build which we will use instead of catkin_make because the catkin_tools are more actively developed than catkin_make ref . Note It is recommended to use catkin_tools instead of the default catkin when building ROS workspaces. catkin_tools provides a number of benefits over regular catkin_make and will be used in the documentation. All packages can be built using catkin_make however: use catkin_make in place of catkin build where appropriate. Bug The current way to install catkin-tools in the documentation from the Ubuntu package repository doesn't work. Follow the steps below instead for now. Success As of now the correct way to install catkin-tools is to use the following command: 1 sudo apt-get install python3-osrf-pycommon python3-catkin-tools For your reference, you can read more about it in this open issue . After sucessfully installing catkin_tools we can create and initialize a workspace (called ros for this project) with the commands listed in the build_tools documentation : Note Note that we already source d the setup.bash while following the ROS installation instructions . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 fjp@ubuntu:~/git/2wd-robot/ros$ mkdir -p ~/git/2wd-robot/ros/src # Make a new workspace and source space fjp@ubuntu:~/git/2wd-robot/ros$ cd ~/git/2wd-robot/ros # Navigate to the workspace root fjp@ubuntu:~/git/2wd-robot/ros$ catkin init # Initialize it with a hidden marker file Initializing catkin workspace in ` /home/fjp/git/2wd-robot/ros ` . ---------------------------------------------------------------- Profile: default Extending: [ env ] /opt/ros/melodic Workspace: /home/fjp/git/2wd-robot/ros ---------------------------------------------------------------- Build Space: [ missing ] /home/fjp/git/2wd-robot/ros/build Devel Space: [ missing ] /home/fjp/git/2wd-robot/ros/devel Install Space: [ unused ] /home/fjp/git/2wd-robot/ros/install Log Space: [ missing ] /home/fjp/git/2wd-robot/ros/logs Source Space: [ exists ] /home/fjp/git/2wd-robot/ros/src DESTDIR: [ unused ] None ---------------------------------------------------------------- Devel Space Layout: linked Install Space Layout: None ---------------------------------------------------------------- Additional CMake Args: None Additional Make Args: None Additional catkin Make Args: None Internal Make Job Server: True Cache Job Environments: False ---------------------------------------------------------------- Whitelisted Packages: None Blacklisted Packages: None ---------------------------------------------------------------- Workspace configuration appears valid. ----------------------------------------------------------------","title":"Build Tool: catkin_tools"},{"location":"ros-setup.html#command-overview-of-catkin_tools","text":"To create packages , which will be covered in the next posts in more depth, we will use catkin create pkg PKG_NAME . Building the workspace is done with catkin build . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 fjp@ubuntu:~/git/2wd-robot/ros$ catkin build ---------------------------------------------------------------- Profile: default Extending: [ env ] /opt/ros/melodic Workspace: /home/fjp/git/2wd-robot/ros ---------------------------------------------------------------- Build Space: [ exists ] /home/fjp/git/2wd-robot/ros/build Devel Space: [ exists ] /home/fjp/git/2wd-robot/ros/devel Install Space: [ unused ] /home/fjp/git/2wd-robot/ros/install Log Space: [ missing ] /home/fjp/git/2wd-robot/ros/logs Source Space: [ exists ] /home/fjp/git/2wd-robot/ros/src DESTDIR: [ unused ] None ---------------------------------------------------------------- Devel Space Layout: linked Install Space Layout: None ---------------------------------------------------------------- Additional CMake Args: None Additional Make Args: None Additional catkin Make Args: None Internal Make Job Server: True Cache Job Environments: False ---------------------------------------------------------------- Whitelisted Packages: None Blacklisted Packages: None ---------------------------------------------------------------- Workspace configuration appears valid. NOTE: Forcing CMake to run for each package. ---------------------------------------------------------------- [ build ] No packages were found in the source space '/home/fjp/git/2wd-robot/ros/src' [ build ] No packages to be built. [ build ] Package table is up to date. Starting >>> catkin_tools_prebuild Finished <<< catkin_tools_prebuild [ 10 .0 seconds ] [ build ] Summary: All 1 packages succeeded! [ build ] Ignored: None. [ build ] Warnings: None. [ build ] Abandoned: None. [ build ] Failed: None. [ build ] Runtime: 10 .1 seconds total.","title":"Command Overview of catkin_tools"},{"location":"ros-setup.html#environment-setup","text":"Finally the newly built packages have to be loaded in the environment using source . 1 fjp@ubuntu:~/git/2wd-robot/ros$ source ~/git/2wd-robot/ros/devel/setup.bash # Load the workspace's environment Tip To avoid tediously typing the above source command, it is convenient to create an alias in your ~/.bashrc or ~/.zshrc similar to the following: 1 alias s='source devel/setup.bash' or using the absolute path 1 alias sa='source ~/git/2wd-robot/ros/devel/setup.bash' It is recommended to use the correct setup script for the shell you use ( bash , zsh , etc.). In case you are unsure, you can check with the echo $SHELL command which will most likely output /bin/bash . Info Instead of source it is possible to use the . command instead. Don't confuse it though with the current directory, which is also represented as . .","title":"Environment Setup"},{"location":"ros-setup.html#resources","text":"Although the catkin tutorial uses catkin_make it provides a helpful guide to create a workspace","title":"Resources"},{"location":"rpi-setup.html","text":"These are the instructions to setup a custom Ubuntu 20.04 Focal Fossa on Raspberry Pi 4 B. Note In case you don't have a Raspberry Pi 4 B, it is also possible to create the robot using a Jetson Nano from Nvidia. See the related section in the documentation . Obtain Ubuntu 20.04 Mate Image for Raspberry Pi To install the long term supported (LTS) Ubuntu 20.04 on the Raspberry Pi 4B we make use of arm64 version of Ubuntu Mate . Download the latest release of the image and flash it to an empty sd card. To do this follow the instructions on the Raspberry Pi documentation or balenaEtcher . Another way is to use the Raspberry Pi Imager explained here . Info Flashing in this context, means the transfer of software data, also refered to as Firmware, from your computer to a device, such as a the sd card in this case. The term \u201cto flash\u201d comes from the Flash storage component of a device where the Firmware is stored. Wifi Issues So far there are no known issues using WiFi with Ubuntu Mate 20.04 arm64 on the Raspberry Pi 4B. Possible issues with other images If you are not in the US it is possible that you encounter connection problems when connected to a 5Ghz Wifi network. If you are in a different country than the US you need to update your regulatory country. 5Ghz needs this to know the right bands to use. This can be changed by editing the value of `REGDOMAIN` in the file `/etc/default/crda` ([Central Regulatory Domain Agent](https://wireless.wiki.kernel.org/en/developers/regulatory/crda)) to the code for your country [ref](https://github.com/TheRemote/Ubuntu-Server-raspi4-unofficial/issues/98). Prepare Ubuntu After flashing the image to the sd card insert it to the Pi, hook it up to a monitor via HDMI and power it up by pluggin in the USB-C connector. Then you should follow the installation instructions on the screen. Once finished, follow the next steps to install ROS Noetic. Note To proceed with the next steps on installing ROS and other related dependencies you can run a bash script. Just clone this repository: https://github.com/fjp/diffbot.git and run ubuntu-setup.sh . But to learn more, you should follow the instructions on the following pages.","title":"Raspberry Pi Setup"},{"location":"rpi-setup.html#obtain-ubuntu-2004-mate-image-for-raspberry-pi","text":"To install the long term supported (LTS) Ubuntu 20.04 on the Raspberry Pi 4B we make use of arm64 version of Ubuntu Mate . Download the latest release of the image and flash it to an empty sd card. To do this follow the instructions on the Raspberry Pi documentation or balenaEtcher . Another way is to use the Raspberry Pi Imager explained here . Info Flashing in this context, means the transfer of software data, also refered to as Firmware, from your computer to a device, such as a the sd card in this case. The term \u201cto flash\u201d comes from the Flash storage component of a device where the Firmware is stored.","title":"Obtain Ubuntu 20.04 Mate Image for Raspberry Pi"},{"location":"rpi-setup.html#wifi-issues","text":"So far there are no known issues using WiFi with Ubuntu Mate 20.04 arm64 on the Raspberry Pi 4B. Possible issues with other images If you are not in the US it is possible that you encounter connection problems when connected to a 5Ghz Wifi network. If you are in a different country than the US you need to update your regulatory country. 5Ghz needs this to know the right bands to use. This can be changed by editing the value of `REGDOMAIN` in the file `/etc/default/crda` ([Central Regulatory Domain Agent](https://wireless.wiki.kernel.org/en/developers/regulatory/crda)) to the code for your country [ref](https://github.com/TheRemote/Ubuntu-Server-raspi4-unofficial/issues/98).","title":"Wifi Issues"},{"location":"rpi-setup.html#prepare-ubuntu","text":"After flashing the image to the sd card insert it to the Pi, hook it up to a monitor via HDMI and power it up by pluggin in the USB-C connector. Then you should follow the installation instructions on the screen. Once finished, follow the next steps to install ROS Noetic. Note To proceed with the next steps on installing ROS and other related dependencies you can run a bash script. Just clone this repository: https://github.com/fjp/diffbot.git and run ubuntu-setup.sh . But to learn more, you should follow the instructions on the following pages.","title":"Prepare Ubuntu"},{"location":"teensy-mcu.html","text":"Teensy Setup The Teensy 3.2 microcontroller (MCU) is used to get the ticks from the encoders attached to the motors and send this information (counts) as a message over the /diffbot/ticks_left and /diffbot/ticks_right ropics. For this rosserial is running on the Teensy MCU which allows it to create a node on the Teensy that can communicate with the ROS Master running on the Raspberry Pi. To setup rosserial on the work PC and the Raspberry Pi the following package has to be installed: 1 sudo apt install ros-noetic-rosserial To program the Teensy board with the work PC the Arduino IDE with the Teensyduino add-on can be used. Other options are to use PlatformIO plugin for VSCode. How to install the Arduino IDE and Teensyduino is listed in the instructions on the Teensy website . Here the instructions to setup Teensyduino in Linux are listed: Download the Linux udev rules (link at the top of this page) and copy the file to /etc/udev/rules.d. sudo cp 49-teensy.rules /etc/udev/rules.d/ Download and extract one of Arduino's Linux packages. Note: Arduino from Linux distro packages is not supported. Download the corresponding Teensyduino installer. Run the installer by adding execute permission and then execute it. chmod 755 TeensyduinoInstall.linux64 ./TeensyduinoInstall.linux64 The first step can be used on the work PC and the Raspberry Pi to enable the USB communication with the Teensy board. Step two of these instructions are only necessary on the work PC to actually program the Teensy board. Note Make sure to download the Arduino IDE from the website and don't install it from the Ubuntu repositories. The following video shows installation process, more instructions to setup the Arduino IDE can be found in the ROS wiki . To check if the connection to the Teensy board works use these commands on the Raspberry Pi: 1 2 3 4 5 $ lsusb Bus 002 Device 001: ID 1d6b:0003 Linux Foundation 3.0 root hub Bus 001 Device 003: ID 16c0:0483 Van Ooijen Technische Informatica Teensyduino Serial Bus 001 Device 002: ID 2109:3431 VIA Labs, Inc. Hub Bus 001 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub And to see on which serial port it is connected use: 1 2 $ ls /dev | grep ttyACM0 ttyACM0 If the output is empty it might be the case that the board is connected to another port like ttyUSB0 . Encoder Program When installing Teensyduino new example programs are provided. One of them is to test Encoders. The code for the motor encoders uses it as basis together with a pubsub example from rosserial: TODO link to code encoders.ino After the program is flashed to the Teensy board it can be tested with the following procedure: Start a ROS master by executing roscore in a new terminal. Create a rosserial node using rosserial_python package: 1 2 3 4 5 6 7 8 9 $ rosrun rosserial_python serial_node.py _port: = /dev/ttyACM0 _baud: = 115200 [INFO] [1602784903.659869]: ROS Serial Python Node [INFO] [1602784903.692366]: Connecting to /dev/ttyACM0 at 115200 baud [INFO] [1602784905.809722]: Requesting topics... [INFO] [1602784905.824418]: Note: publish buffer size is 512 bytes [INFO] [1602784905.829712]: Setup publisher on /diffbot/ticks_left [std_msgs/Int32] [INFO] [1602784905.839914]: Setup publisher on /diffbot/ticks_right [std_msgs/Int32] [INFO] [1602784905.856772]: Note: subscribe buffer size is 512 bytes [INFO] [1602784905.861749]: Setup subscriber on /reset [std_msgs/Empty] In case of the following error, probably the wrong program is flashed to the Teensy board: 1 [ERROR] [1602782376.724880]: Unable to sync with device; possible link problem or link software version mismatch such as hydro rosserial_python with groovy Arduino Note Note that the rosserial node needs to be stopped to flash new sketches to the Teensy board. Each DG01D-E motor has two signal pins for its built-in encoder. For these, the Teensy pins 5, 6 are used for the left encoder and 7, 8 are used for the right one, see also the Teensy pinout . Teensy 4.0 Pins. TODO add connection schematic With one motor encoder connected to pins 5, 6, echo the /encoder_ticks topic: 1 rostopic echo /encoder_ticks Rotating a wheel attached to the motor shaft 360 degree (one full turn) will increase the first value of the encoders array: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 --- header: seq: 190323 stamp: secs: 0 nsecs: 0 frame_id: '' encoders: [0, 0] --- header: seq: 190324 stamp: secs: 0 nsecs: 0 frame_id: '' encoders: [230, 0] --- header: seq: 190325 stamp: secs: 0 nsecs: 0 frame_id: '' encoders: [350, 0] --- header: seq: 190326 stamp: secs: 0 nsecs: 0 frame_id: '' encoders: [480, 0] --- header: seq: 190327 stamp: secs: 0 nsecs: 0 frame_id: '' encoders: [540, 0] The found value 540 for a full turn of the wheel is important for the hardware interface.","title":"Teensy MCU"},{"location":"teensy-mcu.html#teensy-setup","text":"The Teensy 3.2 microcontroller (MCU) is used to get the ticks from the encoders attached to the motors and send this information (counts) as a message over the /diffbot/ticks_left and /diffbot/ticks_right ropics. For this rosserial is running on the Teensy MCU which allows it to create a node on the Teensy that can communicate with the ROS Master running on the Raspberry Pi. To setup rosserial on the work PC and the Raspberry Pi the following package has to be installed: 1 sudo apt install ros-noetic-rosserial To program the Teensy board with the work PC the Arduino IDE with the Teensyduino add-on can be used. Other options are to use PlatformIO plugin for VSCode. How to install the Arduino IDE and Teensyduino is listed in the instructions on the Teensy website . Here the instructions to setup Teensyduino in Linux are listed: Download the Linux udev rules (link at the top of this page) and copy the file to /etc/udev/rules.d. sudo cp 49-teensy.rules /etc/udev/rules.d/ Download and extract one of Arduino's Linux packages. Note: Arduino from Linux distro packages is not supported. Download the corresponding Teensyduino installer. Run the installer by adding execute permission and then execute it. chmod 755 TeensyduinoInstall.linux64 ./TeensyduinoInstall.linux64 The first step can be used on the work PC and the Raspberry Pi to enable the USB communication with the Teensy board. Step two of these instructions are only necessary on the work PC to actually program the Teensy board. Note Make sure to download the Arduino IDE from the website and don't install it from the Ubuntu repositories. The following video shows installation process, more instructions to setup the Arduino IDE can be found in the ROS wiki . To check if the connection to the Teensy board works use these commands on the Raspberry Pi: 1 2 3 4 5 $ lsusb Bus 002 Device 001: ID 1d6b:0003 Linux Foundation 3.0 root hub Bus 001 Device 003: ID 16c0:0483 Van Ooijen Technische Informatica Teensyduino Serial Bus 001 Device 002: ID 2109:3431 VIA Labs, Inc. Hub Bus 001 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub And to see on which serial port it is connected use: 1 2 $ ls /dev | grep ttyACM0 ttyACM0 If the output is empty it might be the case that the board is connected to another port like ttyUSB0 .","title":"Teensy Setup"},{"location":"teensy-mcu.html#encoder-program","text":"When installing Teensyduino new example programs are provided. One of them is to test Encoders. The code for the motor encoders uses it as basis together with a pubsub example from rosserial: TODO link to code encoders.ino After the program is flashed to the Teensy board it can be tested with the following procedure: Start a ROS master by executing roscore in a new terminal. Create a rosserial node using rosserial_python package: 1 2 3 4 5 6 7 8 9 $ rosrun rosserial_python serial_node.py _port: = /dev/ttyACM0 _baud: = 115200 [INFO] [1602784903.659869]: ROS Serial Python Node [INFO] [1602784903.692366]: Connecting to /dev/ttyACM0 at 115200 baud [INFO] [1602784905.809722]: Requesting topics... [INFO] [1602784905.824418]: Note: publish buffer size is 512 bytes [INFO] [1602784905.829712]: Setup publisher on /diffbot/ticks_left [std_msgs/Int32] [INFO] [1602784905.839914]: Setup publisher on /diffbot/ticks_right [std_msgs/Int32] [INFO] [1602784905.856772]: Note: subscribe buffer size is 512 bytes [INFO] [1602784905.861749]: Setup subscriber on /reset [std_msgs/Empty] In case of the following error, probably the wrong program is flashed to the Teensy board: 1 [ERROR] [1602782376.724880]: Unable to sync with device; possible link problem or link software version mismatch such as hydro rosserial_python with groovy Arduino Note Note that the rosserial node needs to be stopped to flash new sketches to the Teensy board. Each DG01D-E motor has two signal pins for its built-in encoder. For these, the Teensy pins 5, 6 are used for the left encoder and 7, 8 are used for the right one, see also the Teensy pinout . Teensy 4.0 Pins. TODO add connection schematic With one motor encoder connected to pins 5, 6, echo the /encoder_ticks topic: 1 rostopic echo /encoder_ticks Rotating a wheel attached to the motor shaft 360 degree (one full turn) will increase the first value of the encoders array: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 --- header: seq: 190323 stamp: secs: 0 nsecs: 0 frame_id: '' encoders: [0, 0] --- header: seq: 190324 stamp: secs: 0 nsecs: 0 frame_id: '' encoders: [230, 0] --- header: seq: 190325 stamp: secs: 0 nsecs: 0 frame_id: '' encoders: [350, 0] --- header: seq: 190326 stamp: secs: 0 nsecs: 0 frame_id: '' encoders: [480, 0] --- header: seq: 190327 stamp: secs: 0 nsecs: 0 frame_id: '' encoders: [540, 0] The found value 540 for a full turn of the wheel is important for the hardware interface.","title":"Encoder Program"},{"location":"theory/index.html","text":"Theory behind Robotics The following pages will provide an overview of the following important topics of robotics: For more details be sure to check out the Self Driving cars with Duckietown edX MOOC . Also don't forget to read through the resources listed on the main page of this documentation and consult your robotics engineering text books of choice. Modeling and Control Introduction to control systems Representations and models PID control Robot Vision Introduction to projective geometry Camera modeling and calibration Image processing Object Detection Convolutional neural networks One and two stage object detection State Estimation and Localization Bayes filtering framework Parameterized methods (Kalman filter) Sampling-based methods (Particle and histogram filter) Planning I Planning formalization Graphs Planning II Probabilistic roadmaps Sampling-based planning Learning by Reinforcement Markov decision processes Value functions Policy gradients Domain randomization Learning by Imitation Behaviour cloning Online imitation learning Safety and uncertainty Resources Self Driving cars with Duckietown edX MOOC","title":"Theory behind Robotics"},{"location":"theory/index.html#theory-behind-robotics","text":"The following pages will provide an overview of the following important topics of robotics: For more details be sure to check out the Self Driving cars with Duckietown edX MOOC . Also don't forget to read through the resources listed on the main page of this documentation and consult your robotics engineering text books of choice. Modeling and Control Introduction to control systems Representations and models PID control Robot Vision Introduction to projective geometry Camera modeling and calibration Image processing Object Detection Convolutional neural networks One and two stage object detection State Estimation and Localization Bayes filtering framework Parameterized methods (Kalman filter) Sampling-based methods (Particle and histogram filter) Planning I Planning formalization Graphs Planning II Probabilistic roadmaps Sampling-based planning Learning by Reinforcement Markov decision processes Value functions Policy gradients Domain randomization Learning by Imitation Behaviour cloning Online imitation learning Safety and uncertainty","title":"Theory behind Robotics"},{"location":"theory/index.html#resources","text":"Self Driving cars with Duckietown edX MOOC","title":"Resources"},{"location":"theory/actuation.html","text":"Actuators Motor Encoder Gearbox Gearboxes can be used to fine-tune the performance characteristics of a motion axis. They're most commonly considered torque multipliers but actually serve several other functions, including speed matching, inertia reduction, and resolution increase ref .","title":"Actuation"},{"location":"theory/actuation.html#actuators","text":"","title":"Actuators"},{"location":"theory/actuation.html#motor","text":"","title":"Motor"},{"location":"theory/actuation.html#encoder","text":"","title":"Encoder"},{"location":"theory/actuation.html#gearbox","text":"Gearboxes can be used to fine-tune the performance characteristics of a motion axis. They're most commonly considered torque multipliers but actually serve several other functions, including speed matching, inertia reduction, and resolution increase ref .","title":"Gearbox"},{"location":"theory/motion-and-odometry.html","text":"Robotic Motion and Odometry The following section describes the theory of robotic motion and odometry, which is part of the book Elements of Robotics . The section focuses on a detailed look on the quadrature encoders that are attached to the robot wheels. For DiffBot the encoders are part of the motors DG01D-E . This section reviews the basic concepts of distance, time, velocity and acceleration. The physics of motion can be described using calculus, but a computer cannot deal with continuous functions; instead, discrete approximations must be used. Odometry, the fundamental algorithm for computing robotic motion. An approximation of the location of a robot can be obtained by repeatedly computing the distance moved and the change direction from the velocity of the wheels in a short period of time. Unfortunately, odometry is subject to serious errors. It is important to understand that errors in direction are much more significant than errors in distance. See the following video explaining odometry: In the simplest implementation, the speed of the wheels of a robot is assumed to be proportional to the power applied by the motors. To improve the accuracy of odometry wheel encoders can be used, which measure the actual number of revolutions of the wheels. The following video from Sparkfun gives an overview of Encoders Distance, Velocity and Time In general, if a robot moves at a constant applied to the motors it causes the wheels to rotate, which in turn causes the robot velocity \\(v\\) for a period of time \\(t\\) , the distance \\(s\\) it moves is \\(s = v \\cdot t\\) . When power is to move at some velocity. However, we cannot specify that a certain power causes a certain velocity No two electrical or mechanical components are ever precisely identical. The environment affects the velocity of a robot because of different friction of the surface External forces can affect the velocity of a robot. It needs more power to sustain a specific velocity when moving uphill and less power when moving downhill, because the force of gravity decreases and increases the velocity. Note Velocity is speed in a direction. A robot can be moving 10cm/s forwards or backwards; in both cases, the speed is the same but the velocity is different. Acceleration as Change in Velocity To get a true picture of the motion of a robot, we need to divide its motion into small segments \\(s_1,s_2,\\dots\\) and measure the distance and time for each segment individually. Then, we can compute the velocities for each segment. In symbols, if we denote the length of the segment si by \\(\\Delta s_i = x_{i+1} \u2212 x_i\\) and the time it takes the robot to cross segment si by \\(\\Delta t_i = t_{i+1} \u2212 t_i\\) , then \\(v_i\\) , the velocity in segment \\(s_i\\) is given by: \\[ v_i = \\frac{\\Delta s_i}{\\Delta t_i} \\] Acceleration is defined as the change in velocity over a period of time \\[a_i = \\frac{\\Delta v_i}{\\Delta t_i}\\] References Kinematics equations for Differential Drive and Articulated Steering","title":"Motion and Odometry"},{"location":"theory/motion-and-odometry.html#robotic-motion-and-odometry","text":"The following section describes the theory of robotic motion and odometry, which is part of the book Elements of Robotics . The section focuses on a detailed look on the quadrature encoders that are attached to the robot wheels. For DiffBot the encoders are part of the motors DG01D-E . This section reviews the basic concepts of distance, time, velocity and acceleration. The physics of motion can be described using calculus, but a computer cannot deal with continuous functions; instead, discrete approximations must be used. Odometry, the fundamental algorithm for computing robotic motion. An approximation of the location of a robot can be obtained by repeatedly computing the distance moved and the change direction from the velocity of the wheels in a short period of time. Unfortunately, odometry is subject to serious errors. It is important to understand that errors in direction are much more significant than errors in distance. See the following video explaining odometry: In the simplest implementation, the speed of the wheels of a robot is assumed to be proportional to the power applied by the motors. To improve the accuracy of odometry wheel encoders can be used, which measure the actual number of revolutions of the wheels. The following video from Sparkfun gives an overview of Encoders","title":"Robotic Motion and Odometry"},{"location":"theory/motion-and-odometry.html#distance-velocity-and-time","text":"In general, if a robot moves at a constant applied to the motors it causes the wheels to rotate, which in turn causes the robot velocity \\(v\\) for a period of time \\(t\\) , the distance \\(s\\) it moves is \\(s = v \\cdot t\\) . When power is to move at some velocity. However, we cannot specify that a certain power causes a certain velocity No two electrical or mechanical components are ever precisely identical. The environment affects the velocity of a robot because of different friction of the surface External forces can affect the velocity of a robot. It needs more power to sustain a specific velocity when moving uphill and less power when moving downhill, because the force of gravity decreases and increases the velocity. Note Velocity is speed in a direction. A robot can be moving 10cm/s forwards or backwards; in both cases, the speed is the same but the velocity is different.","title":"Distance, Velocity and Time"},{"location":"theory/motion-and-odometry.html#acceleration-as-change-in-velocity","text":"To get a true picture of the motion of a robot, we need to divide its motion into small segments \\(s_1,s_2,\\dots\\) and measure the distance and time for each segment individually. Then, we can compute the velocities for each segment. In symbols, if we denote the length of the segment si by \\(\\Delta s_i = x_{i+1} \u2212 x_i\\) and the time it takes the robot to cross segment si by \\(\\Delta t_i = t_{i+1} \u2212 t_i\\) , then \\(v_i\\) , the velocity in segment \\(s_i\\) is given by: \\[ v_i = \\frac{\\Delta s_i}{\\Delta t_i} \\] Acceleration is defined as the change in velocity over a period of time \\[a_i = \\frac{\\Delta v_i}{\\Delta t_i}\\]","title":"Acceleration as Change in Velocity"},{"location":"theory/motion-and-odometry.html#references","text":"Kinematics equations for Differential Drive and Articulated Steering","title":"References"},{"location":"theory/architecture/docker.html","text":"Docker Docker Containers A container includes an application and its dependencies. It's goal is to easily ship (deploy) and handle applications. Like real world shipping containers, Docker containers wrap up an application in a filesystem containing everything the application needs to run: source code runtime libraries system tools configruation files The result is that a containerzied application will run identically on any host. And there are no incompatibilieties of any kind. Why Containerization In the traditional operating systems like Linux Ubuntu a package manager install apps. These apps run on shared runtime libraries. Therfore, applications are coupled, because they share the same dependencies. This can can lead to compatibility issues , when, for example, two or more applications requires a different version of the same shared library. When using containerization, each application running in a container comes with its own needed set of libraries. This way, each application container is isolated and can be updated independently. Difference between Containerization and Virtual Machines Containers are different to virtual machines. In a virtual machine environment there is a host system that runs a hypervisor (e.g. VMWare). The hypervisor divides the physical hardware resources among the virtual machines. Each virtual machine runs its own operating system. The disadvantage of this is that there is a considerable overhead and processes cannot communicate over different virtual machines. On the other hand, containerization has minimal overhead and it allow us to deploy multiple applications so that they can communicate with each other.","title":"Docker"},{"location":"theory/architecture/docker.html#docker","text":"","title":"Docker"},{"location":"theory/architecture/docker.html#docker-containers","text":"A container includes an application and its dependencies. It's goal is to easily ship (deploy) and handle applications. Like real world shipping containers, Docker containers wrap up an application in a filesystem containing everything the application needs to run: source code runtime libraries system tools configruation files The result is that a containerzied application will run identically on any host. And there are no incompatibilieties of any kind.","title":"Docker Containers"},{"location":"theory/architecture/docker.html#why-containerization","text":"In the traditional operating systems like Linux Ubuntu a package manager install apps. These apps run on shared runtime libraries. Therfore, applications are coupled, because they share the same dependencies. This can can lead to compatibility issues , when, for example, two or more applications requires a different version of the same shared library. When using containerization, each application running in a container comes with its own needed set of libraries. This way, each application container is isolated and can be updated independently.","title":"Why Containerization"},{"location":"theory/architecture/docker.html#difference-between-containerization-and-virtual-machines","text":"Containers are different to virtual machines. In a virtual machine environment there is a host system that runs a hypervisor (e.g. VMWare). The hypervisor divides the physical hardware resources among the virtual machines. Each virtual machine runs its own operating system. The disadvantage of this is that there is a considerable overhead and processes cannot communicate over different virtual machines. On the other hand, containerization has minimal overhead and it allow us to deploy multiple applications so that they can communicate with each other.","title":"Difference between Containerization and Virtual Machines"},{"location":"theory/modeling-control/control-systems-introduction.html","text":"Introduction to Control Systems Quote Control systems is the science of making machines behave the way we want them to behave as opposed to how they would naturally behave. Systems have input and output signals and a behavior that evolves over time. Systems can be broken down into components, which are something that we think we understand. In physical systems, inputs are generated by actuators. For example, DC motors or LEDs. Outputs are measured by sensors, for example, cameras or wheel encoders. These sensors produce observations. graph RL A[Actuators] -- Inputs --> P[Process and Environment]; DIR(\"<img src='https://iconscout.com/ms-icon-310x310.png' width='30' />\") P -->|Outputs| S[Sensors]; S -- Observations --> id1[\"<img src='' />\"]; style id1 fill:#ffff,stroke-width:0px In the case of a mobile robot, the system is the real physical robot and its environment around it. This system receives inputs from the robot's actuators, which results in an observable output by the robot's sensors. For example a change in the camera viewport of the environment or a change in encoder ticks because of rotating wheels. Controlling a system means to design a logical component that will output, at every instant in time, the commands for the plant actuators so that the output of our system follow a given plan. Need for Control Systems The main objectives of the controller are the following: Stability Performance Robustness Stability Mathematically, stability can be formalized as Bounded Input Bounded Output . The output of a system will be bounded for every input to the system that is bounded. TODO Add stability image In other words, if finite energy is provided to the system, then finite energy should exit the system. TODO Add stability I/O image According to the definition of stability provided (BIBO stability), a system goes unstable when an output of the system diverges (always continues growing in time), when provided with a finite energy input. When defining the output as \"distance from the center of the lane\", driving outside of the road would be an example of instability. \\begin{tikzpicture} % Draw axes \\draw[help lines, color=gray!30, dashed] (-0.1,-0.1) grid (3.0,2.0); \\draw [<->,thick] (0,2) node (a) [above] {$y$} |- (3,0) node (b) [right] {$x$}; % Draw two intersecting lines \\draw[color=red] (0,0) -- (0,1.5) coordinate (b_1) -- (2.5,1.5) coordinate (b_2); \\end{tikzpicture} \\begin{tikzpicture}[node distance=2.5cm,auto] \\node [draw, fill=blue!20, minimum size=2em, pin={[pin edge={to-,thin,black}]above:$v_0$}] (a) {$\\frac{1}{s}$}; \\node (b) [left of=a,node distance=2cm, coordinate] {a}; \\node [draw, fill=blue!20, minimum size=2em, pin={[pin edge={to-,thin,black}]above:$p_0$}] (c) [right of=a] {$\\frac{1}{s}$}; \\node [coordinate] (end) [right of=c, node distance=2cm]{}; \\path[->] (b) edge node {$a$} (a); \\path[->] (a) edge node {$v$} (c); \\draw[->] (c) edge node {$p$} (end); \\end{tikzpicture} \\begin{tikzpicture} % Draw axes \\draw[help lines, color=gray!30, dashed] (-0.1,-0.1) grid (3.0,2.0); \\draw [<->,thick] (0,2) node (a) [above] {$y$} |- (3,0) node (b) [right] {$x$}; % Draw two intersecting lines \\draw[color=red] (0,0) -- (0,1.5) coordinate (b_1) -- (2.5,1.5) coordinate (b_2); \\end{tikzpicture} \\begin{tikzpicture}[node distance=2.5cm,auto] \\node [draw, fill=blue!20, minimum size=2em, pin={[pin edge={to-,thin,black}]above:$v_0$}] (a) {$\\frac{1}{s}$}; \\node (b) [left of=a,node distance=2cm, coordinate] {a}; \\node [draw, fill=blue!20, minimum size=2em, pin={[pin edge={to-,thin,black}]above:$p_0$}] (c) [right of=a] {$\\frac{1}{s}$}; \\node [coordinate] (end) [right of=c, node distance=2cm]{}; \\path[->] (b) edge node {$a$} (a); \\path[->] (a) edge node {$v$} (c); \\draw[->] (c) edge node {$p$} (end); \\end{tikzpicture} Performance Beside being stable a system needs also to perform well. Performance can be measured in several ways: Time before reaching the target state: How quickly does the system converge to the plan Tracking error: how closely a reference signal is followed or how precisely does the system converge. Maximum acceptable error at any point in time Disturbance rejection: the ability to compensate for external stimuli. How well can the system recover from unexpected external stimuli, such as the noise in the measurements or disturbances, like a sudden gust of wind or hitting a bump on the road. Noise attenuation: the ability to minimize the effect of high-frequency signals added to the measurements or inputs. These are all good examples of performance criteria for controller design. TODO Add performance image with direct path to target and max acceptable error Robustness Robustness, is the ability of the controller to provide stability and performance even in the presence of uncertainty in the mathematical model of the plant. TODO Add model uncertainty image Model uncertainty can mean that the wheels of the robot are slightly of different sizes or the disposition of mass is different than what's assumed. Also the parameters parameters of the system might just change over time because of wear and tear. A robust controller would handle these cases well despite the uncertainty. Robustness and performance are trade offs and striking the right balance is a challenge that really depends on the application. Model uncertainty is defined as a \"bounded variation\" of the parameters describing the controlled system's model. Choosing completely different sensors (e.g., a camera instead of the planned pressure sensor for altitude control of a hot air balloon) might induce a completely different structure in the plant's model, which wouldn't not therefore be a \"bounded\" variation. \"Bounded variation\" in the system behavior of a mobile robot could count as: Wear and tear of components over time. Slight imperfections in the assembly. Controller Structure TODO Add stability, performance, robustness images With the mentioned objectives in mind, stability, performance, and robustness, it is important to think about the structure of the controller. Open Loop Control Close Loop Control The simplest approach is for the user to send a predetermined sequence of commands directly to the actuators, which is called open loop control. TODO Add open loop image Open Loop Pros it is stable it is convenient works when the model of the system is good Open Loop Cons everything must be planned in advance If our understanding of the platform response is good, we will obtain the desired outcome. If it is not good or if something unexpected happens during the execution, then the end result will diverge from the plan. In open loop control, information flows only in one direction, from the controller to the plant. To enable the controller to take into account the actual execution, it is possible to close the loop by feeding the sensor measurements back to the controller, creating a feedback loop. TODO Add closed loop image Closed Feedback Loop Pros it adapts to circumstances Closed Feedback Loop Cons it is less convenient it can destabalize a stable system Feedback control is very powerful because it allows the whole system to adapt to circumstances as they are unfolding and apply corrections on the fly. The measurements themselves, though, generally need to be processed before being fed back to the controller. This is required because the raw measurements might include a lot of redundant data straight out of the sensors. How to structure this agent is up to the designer, to us. TODO Add closed loop empty agent image We could use different approaches, for example, a deep neural network trained from real data or a simulation to translate the images from the camera directly into commands to the wheels to keep the robot following a desired plan. Traditionally, agents are designed with three main components, perception, planning, and control: to see, to plan, and to act. TODO Add closed loop designed agent image Perception Perception graph LR id1[\"<img src='' />\"] -- observations --> P[perception] subgraph agent/controller P --> |estimate/belief|PL[planner] --> C[controller] P --> |estimate/belief|C end C -- commands --> id2[\"<img src='' />\"] style id1 fill:#ffff,stroke-width:0px style id2 fill:#ffff,stroke-width:0px TODO Add closed loop empty agent image: perception highlighted The perception block is responsible for transforming the data from the sensors into actionable information for the robot. This passage is sometimes called filtering or estimation. These estimates of relevant quantities, for example, the position or the orientation of the robot's reference frame with respect to the world and the other objects in it, represent what is the robot's belief of his current state. This belief might be more or less corresponding to the truth depending on the quality of the measurements and the perception solution used. Planner TODO Add closed loop empty agent image: planner highlighted The planner instead provides a reference path for the robot to follow. For example, between two navigation points in a more general task of reaching a goal position while avoiding obstacles on the road. The planner receives the state estimate as input so it can adjust the nominal plan on the fly. Controller TODO Add closed loop empty agent image: controller highlighted The plan and the state estimate are finally fed into the actual controller, which uses them to compute a decision applying a certain logic. For a mobile robot, the decision of the controller will be a sequence of commands which will be sent to the motors, finally closing the loop. Summary Systems are input/output relations we can formalize Control is about making the system's output follow a given plan The control objectives are stability, performance and robustness The traditional feedback control architecture includes perception planning and control steps Designing a controller allows to have systems behave in a desired way, rather than following their natural dynamics. Although we can measure quantities of interest and drive the system through actuators, not all systems are controllable. Some systems simply can't be controlled. Systems have their natural dynamics, fundamentally dictated by the laws of physics. More often than not, just \"letting the system go\" will not meet the user's requirements. Control systems leverage our abilities to measure quantities of interest and to actuate (or influence the physical world through devices), to drive the system where we want, and how we want. In practice a controller consists of lines of code or one or more mechanical devices. A controller is typically a logic , that outputs decisions . These decisions are translated to the real world through actuators. It is actually possible to create control logics with analog devices too. Stability is the first design objective for most controllers because unstable systems are potentially unsafe. An unstable system might lead to overshooting driving behaviour, not following a reference path and even driving outside of the road without heading back. There are very few instances in which sending a system unstable might be desirable (exceptions being, e.g., acrobatic flight or selected military/destructive applications). An unstable system will behave unpredictably, potentially with catastrophic results. References Self Driving Cars with Duckietown","title":"Introduction to Control Systems"},{"location":"theory/modeling-control/control-systems-introduction.html#introduction-to-control-systems","text":"Quote Control systems is the science of making machines behave the way we want them to behave as opposed to how they would naturally behave. Systems have input and output signals and a behavior that evolves over time. Systems can be broken down into components, which are something that we think we understand. In physical systems, inputs are generated by actuators. For example, DC motors or LEDs. Outputs are measured by sensors, for example, cameras or wheel encoders. These sensors produce observations. graph RL A[Actuators] -- Inputs --> P[Process and Environment]; DIR(\"<img src='https://iconscout.com/ms-icon-310x310.png' width='30' />\") P -->|Outputs| S[Sensors]; S -- Observations --> id1[\"<img src='' />\"]; style id1 fill:#ffff,stroke-width:0px In the case of a mobile robot, the system is the real physical robot and its environment around it. This system receives inputs from the robot's actuators, which results in an observable output by the robot's sensors. For example a change in the camera viewport of the environment or a change in encoder ticks because of rotating wheels. Controlling a system means to design a logical component that will output, at every instant in time, the commands for the plant actuators so that the output of our system follow a given plan.","title":"Introduction to Control Systems"},{"location":"theory/modeling-control/control-systems-introduction.html#need-for-control-systems","text":"The main objectives of the controller are the following: Stability Performance Robustness","title":"Need for Control Systems"},{"location":"theory/modeling-control/control-systems-introduction.html#stability","text":"Mathematically, stability can be formalized as Bounded Input Bounded Output . The output of a system will be bounded for every input to the system that is bounded. TODO Add stability image In other words, if finite energy is provided to the system, then finite energy should exit the system. TODO Add stability I/O image According to the definition of stability provided (BIBO stability), a system goes unstable when an output of the system diverges (always continues growing in time), when provided with a finite energy input. When defining the output as \"distance from the center of the lane\", driving outside of the road would be an example of instability. \\begin{tikzpicture} % Draw axes \\draw[help lines, color=gray!30, dashed] (-0.1,-0.1) grid (3.0,2.0); \\draw [<->,thick] (0,2) node (a) [above] {$y$} |- (3,0) node (b) [right] {$x$}; % Draw two intersecting lines \\draw[color=red] (0,0) -- (0,1.5) coordinate (b_1) -- (2.5,1.5) coordinate (b_2); \\end{tikzpicture} \\begin{tikzpicture}[node distance=2.5cm,auto] \\node [draw, fill=blue!20, minimum size=2em, pin={[pin edge={to-,thin,black}]above:$v_0$}] (a) {$\\frac{1}{s}$}; \\node (b) [left of=a,node distance=2cm, coordinate] {a}; \\node [draw, fill=blue!20, minimum size=2em, pin={[pin edge={to-,thin,black}]above:$p_0$}] (c) [right of=a] {$\\frac{1}{s}$}; \\node [coordinate] (end) [right of=c, node distance=2cm]{}; \\path[->] (b) edge node {$a$} (a); \\path[->] (a) edge node {$v$} (c); \\draw[->] (c) edge node {$p$} (end); \\end{tikzpicture} \\begin{tikzpicture} % Draw axes \\draw[help lines, color=gray!30, dashed] (-0.1,-0.1) grid (3.0,2.0); \\draw [<->,thick] (0,2) node (a) [above] {$y$} |- (3,0) node (b) [right] {$x$}; % Draw two intersecting lines \\draw[color=red] (0,0) -- (0,1.5) coordinate (b_1) -- (2.5,1.5) coordinate (b_2); \\end{tikzpicture} \\begin{tikzpicture}[node distance=2.5cm,auto] \\node [draw, fill=blue!20, minimum size=2em, pin={[pin edge={to-,thin,black}]above:$v_0$}] (a) {$\\frac{1}{s}$}; \\node (b) [left of=a,node distance=2cm, coordinate] {a}; \\node [draw, fill=blue!20, minimum size=2em, pin={[pin edge={to-,thin,black}]above:$p_0$}] (c) [right of=a] {$\\frac{1}{s}$}; \\node [coordinate] (end) [right of=c, node distance=2cm]{}; \\path[->] (b) edge node {$a$} (a); \\path[->] (a) edge node {$v$} (c); \\draw[->] (c) edge node {$p$} (end); \\end{tikzpicture}","title":"Stability"},{"location":"theory/modeling-control/control-systems-introduction.html#performance","text":"Beside being stable a system needs also to perform well. Performance can be measured in several ways: Time before reaching the target state: How quickly does the system converge to the plan Tracking error: how closely a reference signal is followed or how precisely does the system converge. Maximum acceptable error at any point in time Disturbance rejection: the ability to compensate for external stimuli. How well can the system recover from unexpected external stimuli, such as the noise in the measurements or disturbances, like a sudden gust of wind or hitting a bump on the road. Noise attenuation: the ability to minimize the effect of high-frequency signals added to the measurements or inputs. These are all good examples of performance criteria for controller design. TODO Add performance image with direct path to target and max acceptable error","title":"Performance"},{"location":"theory/modeling-control/control-systems-introduction.html#robustness","text":"Robustness, is the ability of the controller to provide stability and performance even in the presence of uncertainty in the mathematical model of the plant. TODO Add model uncertainty image Model uncertainty can mean that the wheels of the robot are slightly of different sizes or the disposition of mass is different than what's assumed. Also the parameters parameters of the system might just change over time because of wear and tear. A robust controller would handle these cases well despite the uncertainty. Robustness and performance are trade offs and striking the right balance is a challenge that really depends on the application. Model uncertainty is defined as a \"bounded variation\" of the parameters describing the controlled system's model. Choosing completely different sensors (e.g., a camera instead of the planned pressure sensor for altitude control of a hot air balloon) might induce a completely different structure in the plant's model, which wouldn't not therefore be a \"bounded\" variation. \"Bounded variation\" in the system behavior of a mobile robot could count as: Wear and tear of components over time. Slight imperfections in the assembly.","title":"Robustness"},{"location":"theory/modeling-control/control-systems-introduction.html#controller-structure","text":"TODO Add stability, performance, robustness images With the mentioned objectives in mind, stability, performance, and robustness, it is important to think about the structure of the controller. Open Loop Control Close Loop Control The simplest approach is for the user to send a predetermined sequence of commands directly to the actuators, which is called open loop control. TODO Add open loop image Open Loop Pros it is stable it is convenient works when the model of the system is good Open Loop Cons everything must be planned in advance If our understanding of the platform response is good, we will obtain the desired outcome. If it is not good or if something unexpected happens during the execution, then the end result will diverge from the plan. In open loop control, information flows only in one direction, from the controller to the plant. To enable the controller to take into account the actual execution, it is possible to close the loop by feeding the sensor measurements back to the controller, creating a feedback loop. TODO Add closed loop image Closed Feedback Loop Pros it adapts to circumstances Closed Feedback Loop Cons it is less convenient it can destabalize a stable system Feedback control is very powerful because it allows the whole system to adapt to circumstances as they are unfolding and apply corrections on the fly. The measurements themselves, though, generally need to be processed before being fed back to the controller. This is required because the raw measurements might include a lot of redundant data straight out of the sensors. How to structure this agent is up to the designer, to us. TODO Add closed loop empty agent image We could use different approaches, for example, a deep neural network trained from real data or a simulation to translate the images from the camera directly into commands to the wheels to keep the robot following a desired plan. Traditionally, agents are designed with three main components, perception, planning, and control: to see, to plan, and to act. TODO Add closed loop designed agent image","title":"Controller Structure"},{"location":"theory/modeling-control/control-systems-introduction.html#perception","text":"Perception graph LR id1[\"<img src='' />\"] -- observations --> P[perception] subgraph agent/controller P --> |estimate/belief|PL[planner] --> C[controller] P --> |estimate/belief|C end C -- commands --> id2[\"<img src='' />\"] style id1 fill:#ffff,stroke-width:0px style id2 fill:#ffff,stroke-width:0px TODO Add closed loop empty agent image: perception highlighted The perception block is responsible for transforming the data from the sensors into actionable information for the robot. This passage is sometimes called filtering or estimation. These estimates of relevant quantities, for example, the position or the orientation of the robot's reference frame with respect to the world and the other objects in it, represent what is the robot's belief of his current state. This belief might be more or less corresponding to the truth depending on the quality of the measurements and the perception solution used.","title":"Perception"},{"location":"theory/modeling-control/control-systems-introduction.html#planner","text":"TODO Add closed loop empty agent image: planner highlighted The planner instead provides a reference path for the robot to follow. For example, between two navigation points in a more general task of reaching a goal position while avoiding obstacles on the road. The planner receives the state estimate as input so it can adjust the nominal plan on the fly.","title":"Planner"},{"location":"theory/modeling-control/control-systems-introduction.html#controller","text":"TODO Add closed loop empty agent image: controller highlighted The plan and the state estimate are finally fed into the actual controller, which uses them to compute a decision applying a certain logic. For a mobile robot, the decision of the controller will be a sequence of commands which will be sent to the motors, finally closing the loop.","title":"Controller"},{"location":"theory/modeling-control/control-systems-introduction.html#summary","text":"Systems are input/output relations we can formalize Control is about making the system's output follow a given plan The control objectives are stability, performance and robustness The traditional feedback control architecture includes perception planning and control steps Designing a controller allows to have systems behave in a desired way, rather than following their natural dynamics. Although we can measure quantities of interest and drive the system through actuators, not all systems are controllable. Some systems simply can't be controlled. Systems have their natural dynamics, fundamentally dictated by the laws of physics. More often than not, just \"letting the system go\" will not meet the user's requirements. Control systems leverage our abilities to measure quantities of interest and to actuate (or influence the physical world through devices), to drive the system where we want, and how we want. In practice a controller consists of lines of code or one or more mechanical devices. A controller is typically a logic , that outputs decisions . These decisions are translated to the real world through actuators. It is actually possible to create control logics with analog devices too. Stability is the first design objective for most controllers because unstable systems are potentially unsafe. An unstable system might lead to overshooting driving behaviour, not following a reference path and even driving outside of the road without heading back. There are very few instances in which sending a system unstable might be desirable (exceptions being, e.g., acrobatic flight or selected military/destructive applications). An unstable system will behave unpredictably, potentially with catastrophic results.","title":"Summary"},{"location":"theory/modeling-control/control-systems-introduction.html#references","text":"Self Driving Cars with Duckietown","title":"References"},{"location":"theory/modeling-control/modeling-differential-drive-robot.html","text":"Modeling of a differential drive robot Mathematical models are powerful tools because they allow us to predict the future. \\[ \\begin{align} \\dot{x}_t &= f(x_t,{\\color{orange}{u_t}}) \\\\ {\\color{green}{y}_t} &= g(x_t,{\\color{orange}{u_t}}) \\end{align} \\] Models map between inputs and outputs of systems and can be derived from first principles or learned from data. TODO Add robot input output image We use models to quantify some essential variable that is useful to accomplish a task, not to provide a faithful description of the exact reality of all the physical processes going on. The Diffbot is a differential drive robot, where the motion of each wheel is controlled by one DC motor. TODO Add image of robot and two dc motors as input DC motors receive voltages \\({\\color{orange}V_{l/r,t}}\\) as inputs and produce torques on the motor drive axis that spins the wheels, and leads to angular velocity \\(\\dot{\\phi}_{l/r,t}\\) of the motor shaft and wheel. The movement of the wheels will produce an evolution of the robots pose \\({\\color{green}q_t}\\) over time, which is what we want to quantify. Forward and Inverse Kinematics Through these models, we can answer two questions. (Forward Kinematics) Given a sequence of commands to the wheels \\(\\dot{\\phi}_{1}, \\dot{\\phi}_{2}, \\cdots, \\dot{\\phi}_{t}\\) , how will the robot move? (Inverse Kinematics) If we want the robot to move in a certain way, given a desired movement \\((q_1, q_2, \\cdots, q_t)\\) , what commands should we send to the wheels? Notations We know that the pose of a robot is the position and the orientation of the body frame with respect to the world frame. We define the robot body frame so that the origin, \\(A\\) , is in the mid-axle point. World Frame: \\(\\{{\\color{blue}x^{w}}, {\\color{blue}y^{w}}\\}\\) Body (robot) frame: \\(\\{{\\color{orange}x^{r}}, {\\color{orange}y^{r}}\\}\\) TODO Add image of robot including orange reference frame and blue world reference frame. Assumption 1 : robot is symmetric along longitudinal axis ( \\(x^r\\) ) and we take it as the x direction of the robot frame. Equidsitand wheels (axle length = \\(2L\\) ). Both wheels will be at a distance \\(L\\) from point \\(A\\) . Identical wheels with diameter \\(R\\) ($R_l = R_r = R) Center of mass of the robot will lie on x-axis \\(x^r\\) at distance \\(c\\) from \\(A\\) Assumption 2 : robot chassis is rigid body . Distance between any two points of the robot does not change in time. in particular \\(\\dot{c} = 0\\) , whre \\((\\dot{\\star}) = \\frac{d(\\star)}{dt}\\) .","title":"Modeling a Differential Drive Robot"},{"location":"theory/modeling-control/modeling-differential-drive-robot.html#modeling-of-a-differential-drive-robot","text":"Mathematical models are powerful tools because they allow us to predict the future. \\[ \\begin{align} \\dot{x}_t &= f(x_t,{\\color{orange}{u_t}}) \\\\ {\\color{green}{y}_t} &= g(x_t,{\\color{orange}{u_t}}) \\end{align} \\] Models map between inputs and outputs of systems and can be derived from first principles or learned from data. TODO Add robot input output image We use models to quantify some essential variable that is useful to accomplish a task, not to provide a faithful description of the exact reality of all the physical processes going on. The Diffbot is a differential drive robot, where the motion of each wheel is controlled by one DC motor. TODO Add image of robot and two dc motors as input DC motors receive voltages \\({\\color{orange}V_{l/r,t}}\\) as inputs and produce torques on the motor drive axis that spins the wheels, and leads to angular velocity \\(\\dot{\\phi}_{l/r,t}\\) of the motor shaft and wheel. The movement of the wheels will produce an evolution of the robots pose \\({\\color{green}q_t}\\) over time, which is what we want to quantify.","title":"Modeling of a differential drive robot"},{"location":"theory/modeling-control/modeling-differential-drive-robot.html#forward-and-inverse-kinematics","text":"Through these models, we can answer two questions. (Forward Kinematics) Given a sequence of commands to the wheels \\(\\dot{\\phi}_{1}, \\dot{\\phi}_{2}, \\cdots, \\dot{\\phi}_{t}\\) , how will the robot move? (Inverse Kinematics) If we want the robot to move in a certain way, given a desired movement \\((q_1, q_2, \\cdots, q_t)\\) , what commands should we send to the wheels?","title":"Forward and Inverse Kinematics"},{"location":"theory/modeling-control/modeling-differential-drive-robot.html#notations","text":"We know that the pose of a robot is the position and the orientation of the body frame with respect to the world frame. We define the robot body frame so that the origin, \\(A\\) , is in the mid-axle point. World Frame: \\(\\{{\\color{blue}x^{w}}, {\\color{blue}y^{w}}\\}\\) Body (robot) frame: \\(\\{{\\color{orange}x^{r}}, {\\color{orange}y^{r}}\\}\\) TODO Add image of robot including orange reference frame and blue world reference frame. Assumption 1 : robot is symmetric along longitudinal axis ( \\(x^r\\) ) and we take it as the x direction of the robot frame. Equidsitand wheels (axle length = \\(2L\\) ). Both wheels will be at a distance \\(L\\) from point \\(A\\) . Identical wheels with diameter \\(R\\) ($R_l = R_r = R) Center of mass of the robot will lie on x-axis \\(x^r\\) at distance \\(c\\) from \\(A\\) Assumption 2 : robot chassis is rigid body . Distance between any two points of the robot does not change in time. in particular \\(\\dot{c} = 0\\) , whre \\((\\dot{\\star}) = \\frac{d(\\star)}{dt}\\) .","title":"Notations"},{"location":"theory/modeling-control/odometry.html","text":"Odometry As robots move in the world to reach an objective or avoid an obstacle, it is important for them to know where they are. Through odometry, robots can update their pose in time, as long as they know where they started from. Odometry comes from the Greek words \u1f41\u03b4\u1f79\u03c2 [odos] (route) and \u03bc\u1f73\u03c4\u03c1\u03bf\u03bd [metron] (measurement), which literally mean: \"measurement of the route\". The odometry problem can be formulated as: given an initial pose \\(q\\) of \\(t_0\\) at some initial time, find the pose at any future time \\(t_0 + \\Delta t\\) . Given: \\(q(t_0) = q_{t_0} = q_0 = \\begin{bmatrix}x_0 & y_0 & \\theta_0 \\end{bmatrix}^T\\) Find: \\(q_{t_0 + \\Delta t}, \\forall \\Delta t > 0\\) When \\(\\Delta t\\) is small enough to consider the angular speed of the wheels constant, the pose update can be approximated as a simple sum. \\(q_{t} = \\begin{bmatrix}x_t & y_t & \\theta_t \\end{bmatrix}^T\\) \\(q_{t_{k+1}} = q_{t_k} + \\dot{q}_{t_k}(t_{k+1} - t_k)\\) The process can then be applied iteratively to find the pose at any time, and at each iteration using the previous estimate as an initial condition. TODO Add odometry update image in world frame The essence of odometry is to use the measurements of the distance traveled by each wheel in a certain time interval and use them to derive the linear and the angular displacement of the robot in time through a motion model. TODO Add odometry update image with \\(\\Delta x\\) and \\(\\Delta y\\) displacements For example, if our robot starts at time 0 with pose (0, 0, 0), and drives straight for 1 second at the speed of 1 meter per second, the odometery will tell us that the final pose will be (1 meter, 0, 0). Recall, the kinematics model, that maps the wheel velocities to the variation of the pose in time. \\[ q_{t_{k+1}} \\approx q_{t_k} + {\\color{orange}{\\dot{q}_{t_k}}}(t_{k+1} - t_k) \\] Kinematic model \\[ {\\color{orange}{\\dot{q}_{t}}} = \\begin{bmatrix}\\dot{x}_t \\\\ \\dot{y}_t \\\\ \\dot{\\theta}_t \\end{bmatrix} = \\frac{ \\color{green}{R} }{2} \\begin{bmatrix}cos(\\theta_t) & 0 \\\\ sin(\\theta_t) & 0 \\\\ 0 & 1\\end{bmatrix} \\begin{bmatrix}1 & 1 \\\\ \\frac{1}{ \\color{green}{L} } & -\\frac{1}{ \\color{green}{L} } \\end{bmatrix} \\begin{bmatrix} \\color{red}{\\dot{\\phi}_{r,t}} \\\\ \\color{red}{\\dot{\\phi}_{l,t}} \\end{bmatrix} \\] This model allows us to perform the pose update once we determine its parameters, which are the wheel radii, which we assume identical, and the distance between the wheels, or the baseline. Parameters \\({\\color{green}{R}}\\) : wheel radius \\(2 {\\color{green}{L}}\\) : baseline (distance between wheels) What's also needed is to measure the wheel angular velocities. Measurements \\(\\color{red}{\\dot{\\phi}_{l,t}}\\) : wheel angular speeds Wheel Encoders To measure the wheel angular velocities, we can use wheel encoders. Although there are various implementations, the operation principle of wheel encoders is simple. The outer rim of the motor's rotor has some evenly spaced, fixed markers. Optical encoders, for example, have a perforated disk where the empty space are the markers. TODO Add encoder wheel image robotc.net One pulse every \\({\\color{red}{\\alpha}} = \\frac{2\\pi}{N_{tot}}\\) radians Every time any of these markers transitions through some reference position on the stator, the marker is sensed, and a signal is produced. For example, in optical encoders, a light sensor will pick up the light from a source every time an empty space in the disk comes about. Knowing how many markers there are in a whole circumference, we can derive how much each wheel rotated by just counting the pulses in each of the k-th time interval. Wheel rotation (in \\(\\Delta t_k\\) ): \\({\\color{orange}{\\Delta \\phi_k}} = N_k \\cdot {\\color{red}{\\alpha}}\\) By dividing the total rotation by delta t, we can then measure the average wheel angular speed in that time frame. Angular speed: \\(\\dot{\\phi}_{t_k} \\approx \\frac{ {\\color{orange}{\\Delta \\phi_k}} }{\\Delta t_k}\\) Expanding the kinematics model expressions, we gain insight on the pose update process. \\[ \\begin{align} &&{\\color{orange}{\\dot{q}_{t}}} &= \\begin{bmatrix}\\dot{x}_t \\\\ \\dot{y}_t \\\\ \\dot{\\theta}_t \\end{bmatrix} = \\frac{ \\color{green}{R} }{2} \\begin{bmatrix}cos(\\theta_t) & 0 \\\\ sin(\\theta_t) & 0 \\\\ 0 & 1\\end{bmatrix} \\begin{bmatrix}1 & 1 \\\\ \\frac{1}{ \\color{green}{L} } & -\\frac{1}{ \\color{green}{L} } \\end{bmatrix} \\begin{bmatrix} \\color{red}{\\dot{\\phi}_{r,t}} \\\\ \\color{red}{\\dot{\\phi}_{l,t}} \\end{bmatrix} {\\color{red}{\\longleftarrow}} {\\color{red}{\\dot{\\phi}_{t_k}}} \\approx \\frac{ \\Delta \\phi_k }{\\Delta t_k} \\\\ &&{\\color{orange}{\\downarrow}} \\\\ && {\\color{green}{q_{t_{k+1}}}} \\approx q_{t_k} + &{\\color{orange}{\\dot{q}_{t_k}}}(t_{k+1} - t_k) \\\\ &&{\\color{green}{\\downarrow}} \\\\ &&{\\color{green}{x_{t_{k+1}}}} \\approx x_{t_k} + & \\frac{R}{2 \\cancel{\\Delta t_k}} \\left(\\Delta \\phi_{r,t} + \\Delta \\phi_{l,t} \\right) \\cos(\\theta_{t_k}) \\cancel{\\Delta t_k} \\\\ &&{\\color{green}{y_{t_{k+1}}}} \\approx y_{t_k} + & \\frac{R}{2} \\left(\\Delta \\phi_{r,t} + \\Delta \\phi_{l,t} \\right) \\sin(\\theta_{t_k}) \\\\ &&{\\color{green}{\\theta_{t_{k+1}}}} \\approx \\theta_{t_k} + & \\frac{R}{2L} \\left(\\Delta \\phi_{r,t} - \\Delta \\phi_{l,t} \\right) \\\\ \\end{align} \\] Notice how the time intervals cancel out, so we don't need to actually compute the angular speed of each wheel, but just the total rotation. The first step in solving the odometry is transforming the wheel rotations into traveled distances for each wheel. We count the pulses from the encoders, derive the rotation of each wheel, and then multiply by the radius of each wheel. \\[ \\begin{align} x_{t_{k+1}} &\\approx x_{t_k} + \\frac{ {\\color{orange}{R}} }{2} \\left({\\color{orange}{\\Delta\\phi_{r,t}}} + {\\color{orange}{\\Delta\\phi_{l,t}}} \\right) \\cos(\\theta_{t_k}) \\\\ y_{t_{k+1}} &\\approx y_{t_k} + \\frac{ {\\color{orange}{R}} }{2} \\left({\\color{orange}{\\Delta\\phi_{r,t}}} + {\\color{orange}{\\Delta\\phi_{l,t}}} \\right) \\sin(\\theta_{t_k}) \\\\ \\theta_{t_{k+1}} &\\approx \\theta_{t_k} + \\frac{ {\\color{orange}{R}} }{2L} \\left({\\color{orange}{\\Delta\\phi_{r,t}}} - {\\color{orange}{\\Delta\\phi_{l,t}}} \\right) \\\\ \\end{align} \\] Wheel travelled distance: \\(\\color{orange}{d_{l/r}} = R \\cdot \\Delta \\phi_{r/l}\\) TODO Add robot image with \\(d_l\\) \\(d_r\\) travelled distance The second step is to transform the wheel displacements into the linear and the angular displacements of the robot reference frame, as we have seen in the modeling section . \\[ \\begin{align} x_{t_{k+1}} &\\approx x_{t_k} + {\\color{orange}{ \\frac{ d_{r,t_k} + d_{l,t_k} }{2} }} \\cos(\\theta_{t_k}) \\\\ y_{t_{k+1}} &\\approx y_{t_k} + {\\color{orange}{ \\frac{ d_{r,t_k} + d_{l,t_k} }{2} }} \\sin(\\theta_{t_k}) \\\\ \\theta_{t_{k+1}} &\\approx \\theta_{t_k} + {\\color{red}{ \\frac{ d_{r,t_k} - d_{l,t_k} }{2L} }} \\\\ \\end{align} \\] TODO Add robot image with \\(d_l\\) \\(d_r\\) travelled distance and \\(\\Delta \\theta\\) The final step is to represent the displacement in the world frame and add the increments to the previous estimates. \\[ \\begin{align} x_{t_{k+1}} &\\approx x_{t_k} + {\\color{orange}{ d_{A,t_k} \\cos(\\theta_{t_k}) }} \\\\ y_{t_{k+1}} &\\approx y_{t_k} + {\\color{orange}{ d_{A,t_k} \\sin(\\theta_{t_k}) }} \\\\ \\theta_{t_{k+1}} &\\approx \\theta_{t_k} + \\Delta \\theta_{t_k} \\\\ \\end{align} \\] TODO Add robot image showing increment update in world frame Summary of Odometry Equations \\[ \\begin{align} x_{t_{k+1}} &\\approx x_{t_k} + d_{A,t_k} \\cos(\\theta_{t_k}) \\\\ y_{t_{k+1}} &\\approx y_{t_k} + d_{A,t_k} \\sin(\\theta_{t_k}) \\\\ \\theta_{t_{k+1}} &\\approx \\theta_{t_k} + \\Delta \\theta_{t_k} \\\\ \\end{align} \\] \\[ \\begin{align} d_{A,t_k} &= \\frac{ d_{r,t_k} + d_{l,t_k} }{2} \\\\ \\Delta \\theta_{t_k} &= \\frac{ d_{r,t_k} - d_{l,t_k} }{2L} \\\\ d_{r/l,t_k} &= 2\\pi R \\frac{N_k}{N_{tot}} \\end{align} \\] Challenges in Odometry There are practical challenges in odometry. \"Dead reconing\" The first practical challenge stems from using this dead reckoning approach, which is the official name of always adding an increment to a previous estimate in order to obtain a new one. \\[ \\begin{align} x_{t_{k+1}} &\\approx x_{t_k} + \\Delta x_{t_k} \\\\ y_{t_{k+1}} &\\approx y_{t_k} + \\Delta y_{t_k} \\\\ \\theta_{t_{k+1}} &\\approx \\theta_{t_k} + \\Delta \\theta_{t_k} \\\\ \\end{align} \\] TODO Reuse odometry update image in world frame While it might work well for short distances, over time, errors like the discrete time approximation will accumulate, making the estimate drift from reality. Kinematic Model Second, we're using a mathematical model, that of the kinematics of a differential drive robot, to translate the actual measurements to the pose of the robot. \\[ \\dot{q}_{t} = \\frac{R}{2} \\begin{bmatrix}cos(\\theta_t) & 0 \\\\ sin(\\theta_t) & 0 \\\\ 0 & 1\\end{bmatrix} \\begin{bmatrix}1 & 1 \\\\ \\frac{1}{L} & -\\frac{1}{L} \\end{bmatrix} \\begin{bmatrix}\\dot{\\phi}_{r,t} \\\\ \\dot{\\phi}_{l,t} \\end{bmatrix} \\] You might recall that we previously said that all models are wrong, although some are useful. This wisdom is ever more true when the assumptions of the model are not respected. Wheel Slip In particular, we impose the condition of no slippage of the wheels. TODO Add wheel slip images/animation When the wheels slip, it means that the motor will be spinning, the encoders will be producing measurements, but the robot will not be moving the same distance as we are assuming. This will induce errors in the odometry, and they will compound over time. Odometry Calibration Finally, we need to use some actual numerical values for the parameters of the model: the wheel radii - which, by the way, are assumed to be identical, but will they really be - and the robot baseline. \\[ \\begin{align} \\Delta \\theta_{t_k} &= \\frac{ d_{r,t_k} - d_{l,t_k} }{2 {\\color{red}{L}} } \\\\ d_{r/l,t_k} &= 2\\pi {\\color{red}{R}} \\frac{N_k}{N_{tot}} \\end{align} \\] Accurately measuring these parameters is very important. Even small imperfections will induce systematic errors in the odometry that, again, will compound over time. Note that although nominally identical, no two real-world physical robots will ever be the same due to manufacturing, assembly, or handling differences. To find the values of the parameters of the model that best fit our robot, we will need to perform an odometry calibration procedure. Summary Wheel encoders can be used to update the robot's pose in time: Measure the motor's angular displacements \\(\\Delta \\phi\\) in \\(\\Delta t\\) Use the kinematics mdoel to find the robot's \\(\\Delta x\\) , \\(\\Delta y\\) , \\(\\Delta \\theta\\) Update the pose by adding the calculated increments Subject to dfit in time due to accumulation of numerical, slipping/skidding and calibration impercision errors.","title":"Odometry"},{"location":"theory/modeling-control/odometry.html#odometry","text":"As robots move in the world to reach an objective or avoid an obstacle, it is important for them to know where they are. Through odometry, robots can update their pose in time, as long as they know where they started from. Odometry comes from the Greek words \u1f41\u03b4\u1f79\u03c2 [odos] (route) and \u03bc\u1f73\u03c4\u03c1\u03bf\u03bd [metron] (measurement), which literally mean: \"measurement of the route\". The odometry problem can be formulated as: given an initial pose \\(q\\) of \\(t_0\\) at some initial time, find the pose at any future time \\(t_0 + \\Delta t\\) . Given: \\(q(t_0) = q_{t_0} = q_0 = \\begin{bmatrix}x_0 & y_0 & \\theta_0 \\end{bmatrix}^T\\) Find: \\(q_{t_0 + \\Delta t}, \\forall \\Delta t > 0\\) When \\(\\Delta t\\) is small enough to consider the angular speed of the wheels constant, the pose update can be approximated as a simple sum. \\(q_{t} = \\begin{bmatrix}x_t & y_t & \\theta_t \\end{bmatrix}^T\\) \\(q_{t_{k+1}} = q_{t_k} + \\dot{q}_{t_k}(t_{k+1} - t_k)\\) The process can then be applied iteratively to find the pose at any time, and at each iteration using the previous estimate as an initial condition. TODO Add odometry update image in world frame The essence of odometry is to use the measurements of the distance traveled by each wheel in a certain time interval and use them to derive the linear and the angular displacement of the robot in time through a motion model. TODO Add odometry update image with \\(\\Delta x\\) and \\(\\Delta y\\) displacements For example, if our robot starts at time 0 with pose (0, 0, 0), and drives straight for 1 second at the speed of 1 meter per second, the odometery will tell us that the final pose will be (1 meter, 0, 0). Recall, the kinematics model, that maps the wheel velocities to the variation of the pose in time. \\[ q_{t_{k+1}} \\approx q_{t_k} + {\\color{orange}{\\dot{q}_{t_k}}}(t_{k+1} - t_k) \\] Kinematic model \\[ {\\color{orange}{\\dot{q}_{t}}} = \\begin{bmatrix}\\dot{x}_t \\\\ \\dot{y}_t \\\\ \\dot{\\theta}_t \\end{bmatrix} = \\frac{ \\color{green}{R} }{2} \\begin{bmatrix}cos(\\theta_t) & 0 \\\\ sin(\\theta_t) & 0 \\\\ 0 & 1\\end{bmatrix} \\begin{bmatrix}1 & 1 \\\\ \\frac{1}{ \\color{green}{L} } & -\\frac{1}{ \\color{green}{L} } \\end{bmatrix} \\begin{bmatrix} \\color{red}{\\dot{\\phi}_{r,t}} \\\\ \\color{red}{\\dot{\\phi}_{l,t}} \\end{bmatrix} \\] This model allows us to perform the pose update once we determine its parameters, which are the wheel radii, which we assume identical, and the distance between the wheels, or the baseline. Parameters \\({\\color{green}{R}}\\) : wheel radius \\(2 {\\color{green}{L}}\\) : baseline (distance between wheels) What's also needed is to measure the wheel angular velocities. Measurements \\(\\color{red}{\\dot{\\phi}_{l,t}}\\) : wheel angular speeds","title":"Odometry"},{"location":"theory/modeling-control/odometry.html#wheel-encoders","text":"To measure the wheel angular velocities, we can use wheel encoders. Although there are various implementations, the operation principle of wheel encoders is simple. The outer rim of the motor's rotor has some evenly spaced, fixed markers. Optical encoders, for example, have a perforated disk where the empty space are the markers. TODO Add encoder wheel image robotc.net One pulse every \\({\\color{red}{\\alpha}} = \\frac{2\\pi}{N_{tot}}\\) radians Every time any of these markers transitions through some reference position on the stator, the marker is sensed, and a signal is produced. For example, in optical encoders, a light sensor will pick up the light from a source every time an empty space in the disk comes about. Knowing how many markers there are in a whole circumference, we can derive how much each wheel rotated by just counting the pulses in each of the k-th time interval. Wheel rotation (in \\(\\Delta t_k\\) ): \\({\\color{orange}{\\Delta \\phi_k}} = N_k \\cdot {\\color{red}{\\alpha}}\\) By dividing the total rotation by delta t, we can then measure the average wheel angular speed in that time frame. Angular speed: \\(\\dot{\\phi}_{t_k} \\approx \\frac{ {\\color{orange}{\\Delta \\phi_k}} }{\\Delta t_k}\\) Expanding the kinematics model expressions, we gain insight on the pose update process. \\[ \\begin{align} &&{\\color{orange}{\\dot{q}_{t}}} &= \\begin{bmatrix}\\dot{x}_t \\\\ \\dot{y}_t \\\\ \\dot{\\theta}_t \\end{bmatrix} = \\frac{ \\color{green}{R} }{2} \\begin{bmatrix}cos(\\theta_t) & 0 \\\\ sin(\\theta_t) & 0 \\\\ 0 & 1\\end{bmatrix} \\begin{bmatrix}1 & 1 \\\\ \\frac{1}{ \\color{green}{L} } & -\\frac{1}{ \\color{green}{L} } \\end{bmatrix} \\begin{bmatrix} \\color{red}{\\dot{\\phi}_{r,t}} \\\\ \\color{red}{\\dot{\\phi}_{l,t}} \\end{bmatrix} {\\color{red}{\\longleftarrow}} {\\color{red}{\\dot{\\phi}_{t_k}}} \\approx \\frac{ \\Delta \\phi_k }{\\Delta t_k} \\\\ &&{\\color{orange}{\\downarrow}} \\\\ && {\\color{green}{q_{t_{k+1}}}} \\approx q_{t_k} + &{\\color{orange}{\\dot{q}_{t_k}}}(t_{k+1} - t_k) \\\\ &&{\\color{green}{\\downarrow}} \\\\ &&{\\color{green}{x_{t_{k+1}}}} \\approx x_{t_k} + & \\frac{R}{2 \\cancel{\\Delta t_k}} \\left(\\Delta \\phi_{r,t} + \\Delta \\phi_{l,t} \\right) \\cos(\\theta_{t_k}) \\cancel{\\Delta t_k} \\\\ &&{\\color{green}{y_{t_{k+1}}}} \\approx y_{t_k} + & \\frac{R}{2} \\left(\\Delta \\phi_{r,t} + \\Delta \\phi_{l,t} \\right) \\sin(\\theta_{t_k}) \\\\ &&{\\color{green}{\\theta_{t_{k+1}}}} \\approx \\theta_{t_k} + & \\frac{R}{2L} \\left(\\Delta \\phi_{r,t} - \\Delta \\phi_{l,t} \\right) \\\\ \\end{align} \\] Notice how the time intervals cancel out, so we don't need to actually compute the angular speed of each wheel, but just the total rotation. The first step in solving the odometry is transforming the wheel rotations into traveled distances for each wheel. We count the pulses from the encoders, derive the rotation of each wheel, and then multiply by the radius of each wheel. \\[ \\begin{align} x_{t_{k+1}} &\\approx x_{t_k} + \\frac{ {\\color{orange}{R}} }{2} \\left({\\color{orange}{\\Delta\\phi_{r,t}}} + {\\color{orange}{\\Delta\\phi_{l,t}}} \\right) \\cos(\\theta_{t_k}) \\\\ y_{t_{k+1}} &\\approx y_{t_k} + \\frac{ {\\color{orange}{R}} }{2} \\left({\\color{orange}{\\Delta\\phi_{r,t}}} + {\\color{orange}{\\Delta\\phi_{l,t}}} \\right) \\sin(\\theta_{t_k}) \\\\ \\theta_{t_{k+1}} &\\approx \\theta_{t_k} + \\frac{ {\\color{orange}{R}} }{2L} \\left({\\color{orange}{\\Delta\\phi_{r,t}}} - {\\color{orange}{\\Delta\\phi_{l,t}}} \\right) \\\\ \\end{align} \\] Wheel travelled distance: \\(\\color{orange}{d_{l/r}} = R \\cdot \\Delta \\phi_{r/l}\\) TODO Add robot image with \\(d_l\\) \\(d_r\\) travelled distance The second step is to transform the wheel displacements into the linear and the angular displacements of the robot reference frame, as we have seen in the modeling section . \\[ \\begin{align} x_{t_{k+1}} &\\approx x_{t_k} + {\\color{orange}{ \\frac{ d_{r,t_k} + d_{l,t_k} }{2} }} \\cos(\\theta_{t_k}) \\\\ y_{t_{k+1}} &\\approx y_{t_k} + {\\color{orange}{ \\frac{ d_{r,t_k} + d_{l,t_k} }{2} }} \\sin(\\theta_{t_k}) \\\\ \\theta_{t_{k+1}} &\\approx \\theta_{t_k} + {\\color{red}{ \\frac{ d_{r,t_k} - d_{l,t_k} }{2L} }} \\\\ \\end{align} \\] TODO Add robot image with \\(d_l\\) \\(d_r\\) travelled distance and \\(\\Delta \\theta\\) The final step is to represent the displacement in the world frame and add the increments to the previous estimates. \\[ \\begin{align} x_{t_{k+1}} &\\approx x_{t_k} + {\\color{orange}{ d_{A,t_k} \\cos(\\theta_{t_k}) }} \\\\ y_{t_{k+1}} &\\approx y_{t_k} + {\\color{orange}{ d_{A,t_k} \\sin(\\theta_{t_k}) }} \\\\ \\theta_{t_{k+1}} &\\approx \\theta_{t_k} + \\Delta \\theta_{t_k} \\\\ \\end{align} \\] TODO Add robot image showing increment update in world frame","title":"Wheel Encoders"},{"location":"theory/modeling-control/odometry.html#summary-of-odometry-equations","text":"\\[ \\begin{align} x_{t_{k+1}} &\\approx x_{t_k} + d_{A,t_k} \\cos(\\theta_{t_k}) \\\\ y_{t_{k+1}} &\\approx y_{t_k} + d_{A,t_k} \\sin(\\theta_{t_k}) \\\\ \\theta_{t_{k+1}} &\\approx \\theta_{t_k} + \\Delta \\theta_{t_k} \\\\ \\end{align} \\] \\[ \\begin{align} d_{A,t_k} &= \\frac{ d_{r,t_k} + d_{l,t_k} }{2} \\\\ \\Delta \\theta_{t_k} &= \\frac{ d_{r,t_k} - d_{l,t_k} }{2L} \\\\ d_{r/l,t_k} &= 2\\pi R \\frac{N_k}{N_{tot}} \\end{align} \\]","title":"Summary of Odometry Equations"},{"location":"theory/modeling-control/odometry.html#challenges-in-odometry","text":"There are practical challenges in odometry.","title":"Challenges in Odometry"},{"location":"theory/modeling-control/odometry.html#dead-reconing","text":"The first practical challenge stems from using this dead reckoning approach, which is the official name of always adding an increment to a previous estimate in order to obtain a new one. \\[ \\begin{align} x_{t_{k+1}} &\\approx x_{t_k} + \\Delta x_{t_k} \\\\ y_{t_{k+1}} &\\approx y_{t_k} + \\Delta y_{t_k} \\\\ \\theta_{t_{k+1}} &\\approx \\theta_{t_k} + \\Delta \\theta_{t_k} \\\\ \\end{align} \\] TODO Reuse odometry update image in world frame While it might work well for short distances, over time, errors like the discrete time approximation will accumulate, making the estimate drift from reality.","title":"\"Dead reconing\""},{"location":"theory/modeling-control/odometry.html#kinematic-model","text":"Second, we're using a mathematical model, that of the kinematics of a differential drive robot, to translate the actual measurements to the pose of the robot. \\[ \\dot{q}_{t} = \\frac{R}{2} \\begin{bmatrix}cos(\\theta_t) & 0 \\\\ sin(\\theta_t) & 0 \\\\ 0 & 1\\end{bmatrix} \\begin{bmatrix}1 & 1 \\\\ \\frac{1}{L} & -\\frac{1}{L} \\end{bmatrix} \\begin{bmatrix}\\dot{\\phi}_{r,t} \\\\ \\dot{\\phi}_{l,t} \\end{bmatrix} \\] You might recall that we previously said that all models are wrong, although some are useful. This wisdom is ever more true when the assumptions of the model are not respected.","title":"Kinematic Model"},{"location":"theory/modeling-control/odometry.html#wheel-slip","text":"In particular, we impose the condition of no slippage of the wheels. TODO Add wheel slip images/animation When the wheels slip, it means that the motor will be spinning, the encoders will be producing measurements, but the robot will not be moving the same distance as we are assuming. This will induce errors in the odometry, and they will compound over time.","title":"Wheel Slip"},{"location":"theory/modeling-control/odometry.html#odometry-calibration","text":"Finally, we need to use some actual numerical values for the parameters of the model: the wheel radii - which, by the way, are assumed to be identical, but will they really be - and the robot baseline. \\[ \\begin{align} \\Delta \\theta_{t_k} &= \\frac{ d_{r,t_k} - d_{l,t_k} }{2 {\\color{red}{L}} } \\\\ d_{r/l,t_k} &= 2\\pi {\\color{red}{R}} \\frac{N_k}{N_{tot}} \\end{align} \\] Accurately measuring these parameters is very important. Even small imperfections will induce systematic errors in the odometry that, again, will compound over time. Note that although nominally identical, no two real-world physical robots will ever be the same due to manufacturing, assembly, or handling differences. To find the values of the parameters of the model that best fit our robot, we will need to perform an odometry calibration procedure.","title":"Odometry Calibration"},{"location":"theory/modeling-control/odometry.html#summary","text":"Wheel encoders can be used to update the robot's pose in time: Measure the motor's angular displacements \\(\\Delta \\phi\\) in \\(\\Delta t\\) Use the kinematics mdoel to find the robot's \\(\\Delta x\\) , \\(\\Delta y\\) , \\(\\Delta \\theta\\) Update the pose by adding the calculated increments Subject to dfit in time due to accumulation of numerical, slipping/skidding and calibration impercision errors.","title":"Summary"},{"location":"theory/modeling-control/robot-representations.html","text":"Robot Representations Representations of the robot and its environment are fundamental to the capabilities that make a vehicle autonomous. To sense, to plan, and to act. Different tasks might require different representations. For example, navigating the city or avoiding a pedestrian on the road. State To quantify representations, we use states. The state \\(x_t\\) of a robot and of the world has the following properties \\(x_t \\in X\\) exists independently of us and the algorithms that we choose to determine it. The state evolves over time, \\[ \\underbrace{x_0, x_1, \\dots,}_{\\color{orange}Past} \\quad \\overbrace{x_t}^{\\color{green}Present} \\quad \\underbrace{x_{t+1}, x_{t+n}, \\dots}_{\\color{red}Future} \\] the robot will need to estimate the \\(\\color{green}present\\) and \\(\\color{red}future\\) state on the fly, so it should be efficiently computable. \\[ {\\color{red}x_{t+1}} = f(\\color{green}x_t, \\color{orange}x_{t-1}, \\dots, x_0; \\color{green}u_t, \\dots, \\color{orange}u_0) \\] Good choices of state are such that given the present information, the future state is independent of the past. This is called Markov property , and it's very desirable because it allows the robot not having to keep track of all the information gathered in the past. The state is typically observed from the sensor measurements, but taking the whole history of the measurements as choice of a state is inefficient, because measurements contain redundant information and increase over time, so they require more and more computation and memory to process. Robot Pose A sufficient and efficient representation of a mobile robot is the pose \\(q_t\\) . Pose Definition \\(q_t\\) : position and orientation of the \\({\\color{orange}\\text{robot}}\\) ( \\({\\color{orange}\\text{body}}\\) ) frame relative to a \\({\\color{red}\\text{world}}\\) ( \\({\\color{red}\\text{fixed}}\\) ) reference frame. That is, the position and the orientation of the robot in space. The pose may also include the linear and the angular velocities. The environment of a mobile robot can be seen as a 2D world, but pose can be generalized to 3D as well. TODO Add reference frame image Reference Frames ( \\(\\mathbb{R}^2\\) ) To formalize a robot's pose, we need to introduce reference systems. We take a world frame with origin in W and a robot, or body, frame, which moves with the robot and has origin in point A at position (x,y) in the world frame. \\({\\color{red}\\text{World frame }}(x^w, y^w)\\) , origin in \\(W\\) \\({\\color{orange}\\text{Robot frame }}(x^r, y^r)\\) , origin in \\(A\\) orientation \\(\\theta\\) with \\(x^w\\) TODO Add reference frames image It is important to express the coordinates of any point with respect to the robot and the world frames, which in the general case are rotated and translated one with respect to the other. Moving between Reference Frames Let's look at the math on how to move between frames, starting from the simpler case of translations. TODO Add points frames image Translations Take two reference frames and assume that they are purely translated with respect to each other by \\(x_A\\) and \\(y_A\\) .","title":"Representations and models"},{"location":"theory/modeling-control/robot-representations.html#robot-representations","text":"Representations of the robot and its environment are fundamental to the capabilities that make a vehicle autonomous. To sense, to plan, and to act. Different tasks might require different representations. For example, navigating the city or avoiding a pedestrian on the road.","title":"Robot Representations"},{"location":"theory/modeling-control/robot-representations.html#state","text":"To quantify representations, we use states. The state \\(x_t\\) of a robot and of the world has the following properties \\(x_t \\in X\\) exists independently of us and the algorithms that we choose to determine it. The state evolves over time, \\[ \\underbrace{x_0, x_1, \\dots,}_{\\color{orange}Past} \\quad \\overbrace{x_t}^{\\color{green}Present} \\quad \\underbrace{x_{t+1}, x_{t+n}, \\dots}_{\\color{red}Future} \\] the robot will need to estimate the \\(\\color{green}present\\) and \\(\\color{red}future\\) state on the fly, so it should be efficiently computable. \\[ {\\color{red}x_{t+1}} = f(\\color{green}x_t, \\color{orange}x_{t-1}, \\dots, x_0; \\color{green}u_t, \\dots, \\color{orange}u_0) \\] Good choices of state are such that given the present information, the future state is independent of the past. This is called Markov property , and it's very desirable because it allows the robot not having to keep track of all the information gathered in the past. The state is typically observed from the sensor measurements, but taking the whole history of the measurements as choice of a state is inefficient, because measurements contain redundant information and increase over time, so they require more and more computation and memory to process.","title":"State"},{"location":"theory/modeling-control/robot-representations.html#robot-pose","text":"A sufficient and efficient representation of a mobile robot is the pose \\(q_t\\) . Pose Definition \\(q_t\\) : position and orientation of the \\({\\color{orange}\\text{robot}}\\) ( \\({\\color{orange}\\text{body}}\\) ) frame relative to a \\({\\color{red}\\text{world}}\\) ( \\({\\color{red}\\text{fixed}}\\) ) reference frame. That is, the position and the orientation of the robot in space. The pose may also include the linear and the angular velocities. The environment of a mobile robot can be seen as a 2D world, but pose can be generalized to 3D as well. TODO Add reference frame image","title":"Robot Pose"},{"location":"theory/modeling-control/robot-representations.html#reference-frames-mathbbr2","text":"To formalize a robot's pose, we need to introduce reference systems. We take a world frame with origin in W and a robot, or body, frame, which moves with the robot and has origin in point A at position (x,y) in the world frame. \\({\\color{red}\\text{World frame }}(x^w, y^w)\\) , origin in \\(W\\) \\({\\color{orange}\\text{Robot frame }}(x^r, y^r)\\) , origin in \\(A\\) orientation \\(\\theta\\) with \\(x^w\\) TODO Add reference frames image It is important to express the coordinates of any point with respect to the robot and the world frames, which in the general case are rotated and translated one with respect to the other.","title":"Reference Frames (\\(\\mathbb{R}^2\\))"},{"location":"theory/modeling-control/robot-representations.html#moving-between-reference-frames","text":"Let's look at the math on how to move between frames, starting from the simpler case of translations. TODO Add points frames image","title":"Moving between Reference Frames"},{"location":"theory/modeling-control/robot-representations.html#translations","text":"Take two reference frames and assume that they are purely translated with respect to each other by \\(x_A\\) and \\(y_A\\) .","title":"Translations"}]}
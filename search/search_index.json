{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to DiffBot Documentation This project guides you on how to build an autonomous two wheel differential drive robot. The robot can operate on a Raspberry Pi 4 B or NVIDIA Jetson Nano Developer Kit running ROS Noetic or ROS Melodic middleware on Ubuntu Mate 20.04 and Ubuntu 18.04 respectively. With a motor driver and two actuators it can drive autonomously to a desired location while sensing its environment using sensors, such as a camera and an laser scanner to avoid obstacles. Odometry wheel encoders (also refered to as speed sensors) combined with an inertial measurement unit (IMU) are used for localization. The project is split into multiple parts, to adress the following main aspects of the robot. Bill of Materials (BOM) and the theory behind the parts. Theory of (mobile) robots . Assembly of the robot platform and the components. Setup of ROS (Noetic or Melodic) on either Raspberry Pi 4 B or Jetson Nano, which are both Single Board Computers (SBC) and are the brain of the robot. Modeling the Robot in Blender and URDF to simulate it in Gazebo. ROS packages and nodes: Hardware drivers to interact with the hardware components High level nodes for perception, navigation, localization and control. Use the menu to learn more about the ROS packages and other components of the robot. Note Using a Jetson Nano instead of a Raspberry Pi is also possible. See the Jetson Nano Setup section in this documentation for more details. To run ROS Noetic Docker is needed. Source Code The source code for this project can be found in the ros-mobile-robots/diffbot GitHub repository. Best Practices and REP The project tries to follow the ROS best practices as good as possible. This includes examples and patterns on producing and contributing high quality code, as well as on testing, and other quality oriented practices, like continuous integration. You can read more about it on the ROS Quality wiki . This includes also following the advices given in the ROS Enhancement Proposals (REPs) . Throughout the documentation links to corresponding REPs are given. The wiki section ROS developer's guide is a good starting point for getting used to the common practices for developing components to be shared with the community. It includes links to naming conventions (e.g. for packages) and ROS C++ and Python style guides. Other good resources to learn more about ROS best practices is the Autonomous Systems Lab of ETH Zurich. Note Your contributions to the code or documentation are most welcome but please try to follow the mentioned best pratices where possible. Testing, Debugging and CI For a ROS catkin workspace explaining gTest and rostest see Ros-Test-Example and its documentation . To run tests with catkin-tools, see Building and running tests . To get a workspace that allows a debugger to stop at breakpoints, it is required to build the catkin workspace with Debug Symbols. For this the command catkin build --save-config --cmake-args -DCMAKE_BUILD_TYPE=Debug is used, mentioned in the catkin-tools cheat sheet . This repository makes use of automated builds when new code is pushed or a pull reuqest is made to this repository. For this the Travis and GitHub actions configurations (yml files) from ROS Industrial CI are used. Documentation The documentation is using material design theme , which is based on MkDocs . Future code documentation will make use of doxygen and rosdoc_lite . References Helpful resources to bring your own robots into ROS are: Understand ROS Concepts Follow ROS Tutorials such as Using ROS on your custom Robot Books: Robot Operating System (ROS) for Absolute Beginners from Apress by Lentin Joseph Programming Robots with ROS A Practical Introduction to the Robot Operating System from O'Reilly Media Mastering ROS for Robotics Programming Second Edition from Packt Elements of Robotics Robots and Their Applications from Springer ROS Robot Programming Book for Free! Handbook from Robotis written by Turtlebot3 Developers Courses: Robocademy ROS Online Course for Beginner Udacity Robotics Software Engineer","title":"Home"},{"location":"#welcome-to-diffbot-documentation","text":"This project guides you on how to build an autonomous two wheel differential drive robot. The robot can operate on a Raspberry Pi 4 B or NVIDIA Jetson Nano Developer Kit running ROS Noetic or ROS Melodic middleware on Ubuntu Mate 20.04 and Ubuntu 18.04 respectively. With a motor driver and two actuators it can drive autonomously to a desired location while sensing its environment using sensors, such as a camera and an laser scanner to avoid obstacles. Odometry wheel encoders (also refered to as speed sensors) combined with an inertial measurement unit (IMU) are used for localization. The project is split into multiple parts, to adress the following main aspects of the robot. Bill of Materials (BOM) and the theory behind the parts. Theory of (mobile) robots . Assembly of the robot platform and the components. Setup of ROS (Noetic or Melodic) on either Raspberry Pi 4 B or Jetson Nano, which are both Single Board Computers (SBC) and are the brain of the robot. Modeling the Robot in Blender and URDF to simulate it in Gazebo. ROS packages and nodes: Hardware drivers to interact with the hardware components High level nodes for perception, navigation, localization and control. Use the menu to learn more about the ROS packages and other components of the robot. Note Using a Jetson Nano instead of a Raspberry Pi is also possible. See the Jetson Nano Setup section in this documentation for more details. To run ROS Noetic Docker is needed.","title":"Welcome to DiffBot Documentation"},{"location":"#source-code","text":"The source code for this project can be found in the ros-mobile-robots/diffbot GitHub repository.","title":"Source Code"},{"location":"#best-practices-and-rep","text":"The project tries to follow the ROS best practices as good as possible. This includes examples and patterns on producing and contributing high quality code, as well as on testing, and other quality oriented practices, like continuous integration. You can read more about it on the ROS Quality wiki . This includes also following the advices given in the ROS Enhancement Proposals (REPs) . Throughout the documentation links to corresponding REPs are given. The wiki section ROS developer's guide is a good starting point for getting used to the common practices for developing components to be shared with the community. It includes links to naming conventions (e.g. for packages) and ROS C++ and Python style guides. Other good resources to learn more about ROS best practices is the Autonomous Systems Lab of ETH Zurich. Note Your contributions to the code or documentation are most welcome but please try to follow the mentioned best pratices where possible.","title":"Best Practices and REP"},{"location":"#testing-debugging-and-ci","text":"For a ROS catkin workspace explaining gTest and rostest see Ros-Test-Example and its documentation . To run tests with catkin-tools, see Building and running tests . To get a workspace that allows a debugger to stop at breakpoints, it is required to build the catkin workspace with Debug Symbols. For this the command catkin build --save-config --cmake-args -DCMAKE_BUILD_TYPE=Debug is used, mentioned in the catkin-tools cheat sheet . This repository makes use of automated builds when new code is pushed or a pull reuqest is made to this repository. For this the Travis and GitHub actions configurations (yml files) from ROS Industrial CI are used.","title":"Testing, Debugging and CI"},{"location":"#documentation","text":"The documentation is using material design theme , which is based on MkDocs . Future code documentation will make use of doxygen and rosdoc_lite .","title":"Documentation"},{"location":"#references","text":"Helpful resources to bring your own robots into ROS are: Understand ROS Concepts Follow ROS Tutorials such as Using ROS on your custom Robot Books: Robot Operating System (ROS) for Absolute Beginners from Apress by Lentin Joseph Programming Robots with ROS A Practical Introduction to the Robot Operating System from O'Reilly Media Mastering ROS for Robotics Programming Second Edition from Packt Elements of Robotics Robots and Their Applications from Springer ROS Robot Programming Book for Free! Handbook from Robotis written by Turtlebot3 Developers Courses: Robocademy ROS Online Course for Beginner Udacity Robotics Software Engineer","title":"References"},{"location":"DG01D-E-motor-with-encoder/","text":"Motor with Wheel Encoder The DG01D-E is a single hobby motor with a hall speed encoder. This motor requires a voltage between 3-9V, has a gearbox ratio of 1:48 and a speed of 90RPM at 4.5V. The voltage between positive and negative is determined according to the power supply voltage of the single chip microcomputer used, generally 3.3V or 5V is used. The hall sensor can sense the North and South poles of its magnetic plate. When the hall sensor senses the South of the magnetic plate, the hall output will result in a high level. Meanwhile the North is the inverse and, when sensed, the hall output will result a low level. Terminal Pin Layout The pins on the product are as follows, when looking at the connector on the housing, motor down/connector up, from right to left. The colors correspond to the included cable when plugged in to the connection slot. G (Blue): hall power negative H1 (Green): hall H1 output signal, square wave H2 (Yellow): hall H2 output signal, square wave V (Orange): hall power positive M+ (Red): motor positive pole M- (Brown): motor negative pole The following image shows the motor from its side with the corresponding pin descriptions: DG01D-E Motor with encoder pin description. Wheel Encoder Measurements This section shows oscilloscope waveform measurements of the quadrature encoder in the DG01D-E motor. The motor is connected to the Grove I2C Motor Driver that is powerd with 10 VDC. The motor_example.py applies 50-100% of the 10 VDC which leads to the following output voltages on the motor: Voltage sweep measurements 0:00 Forward Speed 50: 6.5 VDC 0:12 Back Speed 50: 6.5 VDC 0:23 Forward Speed 60: 6.9 VDC 0:34 Back Speed 60: 6.9 VDC 0:46 Forward Speed 70: 7.2 VDC 0:56 Back Speed 70: 7.2 VDC 1:07 Forward 80: 7.3 VDC 1:18 Back 80: 7.3 VDC 1:29 Forward 90: 7.6 VDC 1:41 Back 90: 7.6 VDC 1:52 Forward 100: 7.9 VDC 2:02 Back 100: 7.9 VDC At the bottom of the pico scope window the cycle time, duty cycle, high and low pulse width measurements are shown for both encoder signals. Oscilloscope is the PicoScope 3000 Series with 2 Channels. To download and install Pico Scope software on Linux refer to the documentation . Summary of installation instructions Add repository to the updater 1 sudo bash -c 'echo \"deb https://labs.picotech.com/debian/ picoscope main\" >/etc/apt/sources.list.d/picoscope.list' Import public key 1 wget -O - https://labs.picotech.com/debian/dists/picoscope/Release.gpg.key | sudo apt-key add - Update package manager cache 1 sudo apt-get update Install PicoScope 1 sudo apt-get install picoscope","title":"Motor and Encoder"},{"location":"DG01D-E-motor-with-encoder/#motor-with-wheel-encoder","text":"The DG01D-E is a single hobby motor with a hall speed encoder. This motor requires a voltage between 3-9V, has a gearbox ratio of 1:48 and a speed of 90RPM at 4.5V. The voltage between positive and negative is determined according to the power supply voltage of the single chip microcomputer used, generally 3.3V or 5V is used. The hall sensor can sense the North and South poles of its magnetic plate. When the hall sensor senses the South of the magnetic plate, the hall output will result in a high level. Meanwhile the North is the inverse and, when sensed, the hall output will result a low level.","title":"Motor with Wheel Encoder"},{"location":"DG01D-E-motor-with-encoder/#terminal-pin-layout","text":"The pins on the product are as follows, when looking at the connector on the housing, motor down/connector up, from right to left. The colors correspond to the included cable when plugged in to the connection slot. G (Blue): hall power negative H1 (Green): hall H1 output signal, square wave H2 (Yellow): hall H2 output signal, square wave V (Orange): hall power positive M+ (Red): motor positive pole M- (Brown): motor negative pole The following image shows the motor from its side with the corresponding pin descriptions: DG01D-E Motor with encoder pin description.","title":"Terminal Pin Layout"},{"location":"DG01D-E-motor-with-encoder/#wheel-encoder-measurements","text":"This section shows oscilloscope waveform measurements of the quadrature encoder in the DG01D-E motor. The motor is connected to the Grove I2C Motor Driver that is powerd with 10 VDC. The motor_example.py applies 50-100% of the 10 VDC which leads to the following output voltages on the motor: Voltage sweep measurements 0:00 Forward Speed 50: 6.5 VDC 0:12 Back Speed 50: 6.5 VDC 0:23 Forward Speed 60: 6.9 VDC 0:34 Back Speed 60: 6.9 VDC 0:46 Forward Speed 70: 7.2 VDC 0:56 Back Speed 70: 7.2 VDC 1:07 Forward 80: 7.3 VDC 1:18 Back 80: 7.3 VDC 1:29 Forward 90: 7.6 VDC 1:41 Back 90: 7.6 VDC 1:52 Forward 100: 7.9 VDC 2:02 Back 100: 7.9 VDC At the bottom of the pico scope window the cycle time, duty cycle, high and low pulse width measurements are shown for both encoder signals. Oscilloscope is the PicoScope 3000 Series with 2 Channels. To download and install Pico Scope software on Linux refer to the documentation . Summary of installation instructions Add repository to the updater 1 sudo bash -c 'echo \"deb https://labs.picotech.com/debian/ picoscope main\" >/etc/apt/sources.list.d/picoscope.list' Import public key 1 wget -O - https://labs.picotech.com/debian/dists/picoscope/Release.gpg.key | sudo apt-key add - Update package manager cache 1 sudo apt-get update Install PicoScope 1 sudo apt-get install picoscope","title":"Wheel Encoder Measurements"},{"location":"components/","text":"Part list and assembly of the robot platform and the components. Category Hardware Part Number Data Sheet & Info Accessories Case for Raspberry Pi 4 B Slim acrylic case for Raspberry Pi 4, stackable, rainbow/transparent BerryBase Micro SD Card SanDisk 64GB Class 10 SanDisk , Ubuntu 18.04 Image Robot Car Kit 2WD robot05 Instructions manual Power bank Intenso Powerbank S10000 Intenso Actuator (Deprecated) Gearbox motor DC Gearbox motor - \"TT Motor\" - 200RPM - 3 to 6VDC Adafruit DG01E-E Motor with encoder DG01E-E Hobby motor with quadrature encoder Sparkfun Board Raspberry Pi 4 B Raspberry Pi 4 B - 4 GB OEM Website Cables Jumper - Female to Female Jumper - Male to Male Micro USB - USB Cable Camera extension cable I2C 4 pin cable Electronics Fan Fan 30x30x7mm 5V DC with Dupont connector BerryBase I2C motor driver Grove - I2C Motor Driver Seeed Studio I2C Hub Grove - I2C Hub Seeed Studio Human Machine Interface OLED Display Grove OLED Display 0.96\" Seeed Studio LED Ring NeoPixel Ring 12x5050 RGB LED Adafruit Sensors Camera module Raspberry Pi - camera module v2.1 Raspberry Pi Ultrasonic ranger Grove - Ultrasonic Ranger Seeed Studio IMU Adafruit 9-DOF Absolute Orientation IMU Fusion Breakout - BNO055 Adafruit Odometry Joy-IT - LM393 Speed Sensor with H206 slot-type opto interrupter Joy-IT Order list Part Store Raspberry Pi 4 B (4 Gb) Amazon.com , Amazon.de SanDisk 64 GB SD Card Class 10 Amazon.com , Amazon.de Robot Smart Chassis Kit Amazon.com , Amazon.de SLAMTEC RPLidar A2M8 (12 m) Amazon.com , Amazon.de Grove Ultrasonic Ranger Amazon.com , Amazon.de Raspi Camera Module V2, 8 MP, 1080p Amazon.com , Amazon.de Grove Motor Driver seeedstudio.com , Amazon.de I2C Hub seeedstudio.com , Amazon.de Additional (Optional) Equipment Part Store PicoScope 3000 Series Oscilloscope 2CH Amazon.de VOLTCRAFT PPS-16005 Amazon.de Board - Raspberry Pi 4 B The main processing unit of the robot is a Raspberry Pi 4 B with 4 GB of RAM. Raspberry Pi 4 B - 4 GB RAM variant. Accessories and Electronics Case and Cooling To protect the Rasbperry Pi 4 B we choose a case that provides access to all its ports. The following images show a stackable acrylic case in rainbow colors. Stackable Rainbow Case for Raspberry Pi 4 B. With this case it is possible to install four heatsinks and apply a fan as cooling equipment for the electronics of the Raspberry Pi 4 B such as its ARM processor. Heatsinks and cooling fan for Raspberry Pi 4 B. SD Card The Raspberry Pi requires a medium to boot from. For this we will use a micro sd card because it is lightweight and easy to flash new operating systems. SanDisk Micro SD Card Class 10. Although a micro sd card won't last that long compared to an hard disk drive (HDD) or solid state disk (SSD) it is well suited for testing. Because sd cards are slower when reading and writing data you should make sure to choose a micro sd card with high performance ratings. For the Raspberry Pi a Class 10 micro sd card is recommended. Regarding speed, the Pi has a limited bus speed of approximately 20 MB/s ( source ) Robot Base The Robot Car Kit 2WD from Joy-IT (article no.: robot05) is used as the base for the autonomous mobile robot. Parts of the 2WD Robot Car Kit 05 from Joy-IT. The acrylic chassis has many holes which allow to mount a mounting plate that can hold different development boards. It allows also to mount a Raspberry Pi 4 B, which will be used in this project. Two wheels, hence 2WD, are included in the kit which can be attached to the motors that are provided too. A third caster wheel is provided which allows the robot to spin on the spot. This means the robot can be described by a holonomic model. The motors operate in a range between 3 to 6 Volts DC and make it possible to mount a punched disk for speed measurements. With that punched disk and additional speed sensors it is possible to implement odometry in ROS. To power the motors a battery compartment is available together with a switch to turn the robot on or off. Power Supplies As mentioned the robot will be equipped with a 5V/2.1A USB-C powerbank to supply the Raspberry Pi 4 B with 5 V. Power bank with 10.000 mAh from Intenso. To power the motors the provided battery compartment will be used, which holds four AA batteries \\(4 \\cdot 1.5\\text{V} = 6\\text{V}\\) . I2C Hub The Raspberry Pi provides just two I2C ports, which is why we will use a I2C hub. With the four port I2C hub from Grove it is possible to connect three I2C devices to a single I2C port of the Raspberry Pi Grove I2C Hub. Breadboard and GPIO Extension Cable Optional but helpful for testing is a breadboard and a GPIO extension cable suitable for the Raspberry Pi 4 B. Breadboard with GPIO extension cable. Sensors Sensors are used to sense the environment and to collect information of the current state. For this 2WD robot the sensors are categorized into perception and localization which are explained in the following two sections. Perception Perception sensors of the 2WD robot will be used to avoid collisions using ultrasonic rangers. Another use case is to detect and follow objects using a camera. Ultrasonic Ranger To avoid obstacles the robot will carry a Grove - Ultrasonic Ranger at the front. Grove Ultrasonic Ranger for obstacle avoidance. It is a non-contact distance measurement module which works at 40KHz and can be interfaced via a single GPIO . For example physical pin 11 of the Raspberry Pi connected to the SIG pin on the sensor can provide the PWM communication. Parameter Value/Range Operating voltage 3.2~5.2V Operating current 8mA Ultrasonic frequency 40kHz Measuring range 2-350cm Resolution 1cm Output PWM Size 50mm X 25mm X 16mm Weight 13g Measurement angle 15 degree Working temperature -10~60 degree C Trigger signal 10uS TTL Echo signal TTL The code that will be used to wrap this sensor as a ROS node can be found in the Grove Raspberry Pi repository on GitHub. As an alternative we could use the HC SR04 . Camera RPi Camera v2. Localization Inertial Measurement Unit An intertial measurement unit (IMU) measures the acceleration and orientation through gyroscopes directly. Other states such as the velocity can then be calculated. For this the Adafruit 9-DOF Absolute Orientation IMU Fusion Breakout - BNO055 is used. 9-DOF Absolute Orientation IMU Fusion Breakout - BNO055 from Adafruit. Odometry For the used odometry sensor see the section below Motor and Wheel Encoder Alternative Optical Sensor To estimate the change in position over time ( odometry ) the robot will utilize an optical speed sensor . Specifically the Joy-IT Speed Sensor which combines a LM393 ( datasheet ) comperator and a H206 slot-type opto interrupter. LM393 Speed Sensor from Joy-IT. Technical Specifications: Dimensions: 32 x 14 x 7mm Operating voltage: 3.3V to 5V (we will use 3.3V) Two outputs: Digital (D0) and Analog (A0) References: https://dronebotworkshop.com/robot-car-with-speed-sensors/ Actuators Grove - I2C Motor Driver V1.3 Control To drive the two motors of the car kit we use the Grove - I2C Motor Driver V1.3 from Seeed Studio. Grove - I2C Motor Driver. Motor and Wheel Encoder The DG01E-E Hobby Motor has a quadrature encoder built in, which makes it easy to assemble the robot and saves space because of no additional (optical or magnetic) wheel encoders. DG01D-E Motor with wheel encoders. Alternative Brushed Gear Motor Brushed Gearbox Motor DC Gearbox motor - \"TT Motor\" - 200RPM - 3 to 6VDC. Human Machine Interface (HMI) The human machine interface is the layer between the user and the robot. OLED Display To update the user with status messages the robot has a 0.96 inch oled (organic ligth emitting diode) display. The oled display used is the Grove I2C 0.96 inch OLED display from Seeed Studio. Grove - I2C 0.96 inch OLED Display. The display is connected to the RPi via I2C on the physical pins 27 (scl) and 28 (sda), refere to the pinout . Library","title":"Components"},{"location":"components/#board-raspberry-pi-4-b","text":"The main processing unit of the robot is a Raspberry Pi 4 B with 4 GB of RAM. Raspberry Pi 4 B - 4 GB RAM variant.","title":"Board - Raspberry Pi 4 B"},{"location":"components/#accessories-and-electronics","text":"","title":"Accessories and Electronics"},{"location":"components/#case-and-cooling","text":"To protect the Rasbperry Pi 4 B we choose a case that provides access to all its ports. The following images show a stackable acrylic case in rainbow colors. Stackable Rainbow Case for Raspberry Pi 4 B. With this case it is possible to install four heatsinks and apply a fan as cooling equipment for the electronics of the Raspberry Pi 4 B such as its ARM processor. Heatsinks and cooling fan for Raspberry Pi 4 B.","title":"Case and Cooling"},{"location":"components/#sd-card","text":"The Raspberry Pi requires a medium to boot from. For this we will use a micro sd card because it is lightweight and easy to flash new operating systems. SanDisk Micro SD Card Class 10. Although a micro sd card won't last that long compared to an hard disk drive (HDD) or solid state disk (SSD) it is well suited for testing. Because sd cards are slower when reading and writing data you should make sure to choose a micro sd card with high performance ratings. For the Raspberry Pi a Class 10 micro sd card is recommended. Regarding speed, the Pi has a limited bus speed of approximately 20 MB/s ( source )","title":"SD Card"},{"location":"components/#robot-base","text":"The Robot Car Kit 2WD from Joy-IT (article no.: robot05) is used as the base for the autonomous mobile robot. Parts of the 2WD Robot Car Kit 05 from Joy-IT. The acrylic chassis has many holes which allow to mount a mounting plate that can hold different development boards. It allows also to mount a Raspberry Pi 4 B, which will be used in this project. Two wheels, hence 2WD, are included in the kit which can be attached to the motors that are provided too. A third caster wheel is provided which allows the robot to spin on the spot. This means the robot can be described by a holonomic model. The motors operate in a range between 3 to 6 Volts DC and make it possible to mount a punched disk for speed measurements. With that punched disk and additional speed sensors it is possible to implement odometry in ROS. To power the motors a battery compartment is available together with a switch to turn the robot on or off.","title":"Robot Base"},{"location":"components/#power-supplies","text":"As mentioned the robot will be equipped with a 5V/2.1A USB-C powerbank to supply the Raspberry Pi 4 B with 5 V. Power bank with 10.000 mAh from Intenso. To power the motors the provided battery compartment will be used, which holds four AA batteries \\(4 \\cdot 1.5\\text{V} = 6\\text{V}\\) .","title":"Power Supplies"},{"location":"components/#i2c-hub","text":"The Raspberry Pi provides just two I2C ports, which is why we will use a I2C hub. With the four port I2C hub from Grove it is possible to connect three I2C devices to a single I2C port of the Raspberry Pi Grove I2C Hub.","title":"I2C Hub"},{"location":"components/#breadboard-and-gpio-extension-cable","text":"Optional but helpful for testing is a breadboard and a GPIO extension cable suitable for the Raspberry Pi 4 B. Breadboard with GPIO extension cable.","title":"Breadboard and GPIO Extension Cable"},{"location":"components/#sensors","text":"Sensors are used to sense the environment and to collect information of the current state. For this 2WD robot the sensors are categorized into perception and localization which are explained in the following two sections.","title":"Sensors"},{"location":"components/#perception","text":"Perception sensors of the 2WD robot will be used to avoid collisions using ultrasonic rangers. Another use case is to detect and follow objects using a camera.","title":"Perception"},{"location":"components/#ultrasonic-ranger","text":"To avoid obstacles the robot will carry a Grove - Ultrasonic Ranger at the front. Grove Ultrasonic Ranger for obstacle avoidance. It is a non-contact distance measurement module which works at 40KHz and can be interfaced via a single GPIO . For example physical pin 11 of the Raspberry Pi connected to the SIG pin on the sensor can provide the PWM communication. Parameter Value/Range Operating voltage 3.2~5.2V Operating current 8mA Ultrasonic frequency 40kHz Measuring range 2-350cm Resolution 1cm Output PWM Size 50mm X 25mm X 16mm Weight 13g Measurement angle 15 degree Working temperature -10~60 degree C Trigger signal 10uS TTL Echo signal TTL The code that will be used to wrap this sensor as a ROS node can be found in the Grove Raspberry Pi repository on GitHub. As an alternative we could use the HC SR04 .","title":"Ultrasonic Ranger"},{"location":"components/#camera","text":"RPi Camera v2.","title":"Camera"},{"location":"components/#localization","text":"","title":"Localization"},{"location":"components/#inertial-measurement-unit","text":"An intertial measurement unit (IMU) measures the acceleration and orientation through gyroscopes directly. Other states such as the velocity can then be calculated. For this the Adafruit 9-DOF Absolute Orientation IMU Fusion Breakout - BNO055 is used. 9-DOF Absolute Orientation IMU Fusion Breakout - BNO055 from Adafruit.","title":"Inertial Measurement Unit"},{"location":"components/#odometry","text":"For the used odometry sensor see the section below Motor and Wheel Encoder Alternative Optical Sensor To estimate the change in position over time ( odometry ) the robot will utilize an optical speed sensor . Specifically the Joy-IT Speed Sensor which combines a LM393 ( datasheet ) comperator and a H206 slot-type opto interrupter. LM393 Speed Sensor from Joy-IT. Technical Specifications: Dimensions: 32 x 14 x 7mm Operating voltage: 3.3V to 5V (we will use 3.3V) Two outputs: Digital (D0) and Analog (A0) References: https://dronebotworkshop.com/robot-car-with-speed-sensors/","title":"Odometry"},{"location":"components/#actuators","text":"Grove - I2C Motor Driver V1.3","title":"Actuators"},{"location":"components/#control","text":"To drive the two motors of the car kit we use the Grove - I2C Motor Driver V1.3 from Seeed Studio. Grove - I2C Motor Driver.","title":"Control"},{"location":"components/#motor-and-wheel-encoder","text":"The DG01E-E Hobby Motor has a quadrature encoder built in, which makes it easy to assemble the robot and saves space because of no additional (optical or magnetic) wheel encoders. DG01D-E Motor with wheel encoders. Alternative Brushed Gear Motor","title":"Motor and Wheel Encoder"},{"location":"components/#brushed-gearbox-motor","text":"DC Gearbox motor - \"TT Motor\" - 200RPM - 3 to 6VDC.","title":"Brushed Gearbox Motor"},{"location":"components/#human-machine-interface-hmi","text":"The human machine interface is the layer between the user and the robot.","title":"Human Machine Interface (HMI)"},{"location":"components/#oled-display","text":"To update the user with status messages the robot has a 0.96 inch oled (organic ligth emitting diode) display. The oled display used is the Grove I2C 0.96 inch OLED display from Seeed Studio. Grove - I2C 0.96 inch OLED Display. The display is connected to the RPi via I2C on the physical pins 27 (scl) and 28 (sda), refere to the pinout . Library","title":"OLED Display"},{"location":"diffbot_base/","text":"DiffBot Base Package This package contains the so called hardware interface of DiffBot which represents the real hardware in software to work with ROS Control . ROS Control Overview. In the simpleste case all that is needed in this package is to write a class that inherits from hardware_interface::RobotHW and provide a launch file. The launch file will Load the robot description from diffbot_description to the paramter server Run the hardware interface of this package diffbot_base Load the controller configuration yaml from the diffbot_control package to the parameter server Load the controllers with the controller manager Load the value of the encoder resolution to the parameter server diffbot_base Package The diffbot_base package is created with catkin-tools : 1 2 3 4 5 6 7 fjp@diffbot:/home/fjp/catkin_ws/src$ catkin create pkg diffbot_base --catkin-deps diff_drive_controller hardware_interface roscpp sensor_msgs rosparam_shortcuts Creating package \"diffbot_base\" in \"/home/fjp/catkin_ws/src\"... Created file diffbot_base/package.xml Created file diffbot_base/CMakeLists.txt Created folder diffbot_base/include/diffbot_base Created folder diffbot_base/src Successfully created package files in /home/fjp/catkin_ws/src/diffbot_base. To work with this package the specified dependencies must be installed either using the available Ubuntu/Debian packages for ROS Noetic or they have to be built from source first. The following table lists the dependencies that we have to install because they are not already part of the ROS Noetic desktop full installation. Refer to the section ROS Noetic Setup for how this was done. Dependency Source Ubuntu/Debian Package rosparam_shortcuts https://github.com/PickNikRobotics/rosparam_shortcuts ros-noetic-rosparam-shortcuts hardware_interface https://github.com/ros-controls/ros_control ros-noetic-ros-control diff_drive_controller https://github.com/ros-controls/ros_controllers ros-noetic-ros-controllers To install a package from source clone (using git) or download the source files from where they are located (commonly hosted on GitHub) into the src folder of a ros catkin workspace and execute the catkin build command. Also make sure to source the workspace after building new packages with source devel/setup.bash . 1 2 3 4 cd /homw/fjp/git/diffbot/ros/ # Navigate to the workspace catkin build # Build all the packages in the workspace ls build # Show the resulting build space ls devel # Show the resulting devel space Make sure to clone/download the source files suitable for the ROS distribtion you are using. If the sources are not available for the distribution you are working with, it is worth to try building anyway. Chances are that the package you want to use is suitable for multiple ROS distros. For example if a package states in its docs, that it is only available for kinetic it is possible that it will work with a ROS noetic install. Hardware Interface See the include and src folders of this package and the details on the hardware interface implementation. For more details on the hardware interface also refer to the section ROS Integration: Control , it gives more details and also this overview article about ROS Control . The hardware interface provides an interface between the real robot hardware and the controllers provided by ROS Control (or even custom controllers). DiffBot works with the diff_drive_controller that is configured in the diffbot_control package, which is also relevant for the simulation in Gazebo. Remember that the simulation uses the gazebo_ros_control package to communicate with the diff_drive_controller . For the real robot hardware, ROS Control uses an instance of type hardware_interface::RobotHW that is passed to the controller_manager to handle the resources, meaning that the actuated robot joints are not in use by multiple controllers that might be loaded. The skeleton of DiffBot's hardware interface looks like following, where the constructor is used to read loaded configuration values from the robot's description from the ROS parameter server: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 namespace diffbot_base { DiffBotHWInterface :: DiffBotHWInterface ( ros :: NodeHandle & nh , urdf :: Model * urdf_model ) : name_ ( \"hardware_interface\" ) , nh_ ( nh ) { // Initialization of the robot's resources (joints, sensors, actuators) and // interfaces can be done here or inside init(). // E.g. parse the URDF for joint names & interfaces, then initialize them // Check if the URDF model needs to be loaded if ( urdf_model == NULL ) loadURDF ( nh , \"robot_description\" ); else urdf_model_ = urdf_model ; // Load rosparams ros :: NodeHandle rpnh ( nh_ , name_ ); std :: size_t error = 0 ; // Code API of rosparam_shortcuts: // http://docs.ros.org/en/noetic/api/rosparam_shortcuts/html/namespacerosparam__shortcuts.html#aa6536fe0130903960b1de4872df68d5d error += ! rosparam_shortcuts :: get ( name_ , rpnh , \"joints\" , joint_names_ ); error += ! rosparam_shortcuts :: get ( name_ , nh_ , \"mobile_base_controller/wheel_radius\" , wheel_radius_ ); error += ! rosparam_shortcuts :: get ( name_ , nh_ , \"mobile_base_controller/linear/x/max_velocity\" , max_velocity_ ); rosparam_shortcuts :: shutdownIfError ( name_ , error ); wheel_diameter_ = 2.0 * wheel_radius_ ; //max_velocity_ = 0.2; // m/s // ros_control RobotHW needs velocity in rad/s but in the config its given in m/s max_velocity_ = linearToAngular ( max_velocity_ ); // Setup publisher for the motor driver pub_left_motor_value_ = nh_ . advertise < std_msgs :: Int32 > ( \"motor_left\" , 1 ); pub_right_motor_value_ = nh_ . advertise < std_msgs :: Int32 > ( \"motor_right\" , 1 ); // Setup subscriber for the wheel encoders sub_left_encoder_ticks_ = nh_ . subscribe ( \"ticks_left\" , 1 , & DiffBotHWInterface :: leftEncoderTicksCallback , this ); sub_right_encoder_ticks_ = nh_ . subscribe ( \"ticks_right\" , 1 , & DiffBotHWInterface :: rightEncoderTicksCallback , this ); // Initialize the hardware interface init ( nh_ , nh_ ); } bool DiffBotHWInterface :: init ( ros :: NodeHandle & root_nh , ros :: NodeHandle & robot_hw_nh ) { ROS_INFO ( \"Initializing DiffBot Hardware Interface ...\" ); num_joints_ = joint_names_ . size (); ROS_INFO ( \"Number of joints: %d\" , ( int ) num_joints_ ); std :: array < std :: string , NUM_JOINTS > motor_names = { \"left_motor\" , \"right_motor\" }; for ( unsigned int i = 0 ; i < num_joints_ ; i ++ ) { // Create a JointStateHandle for each joint and register them with the // JointStateInterface. hardware_interface :: JointStateHandle joint_state_handle ( joint_names_ [ i ], & joint_positions_ [ i ], & joint_velocities_ [ i ], & joint_efforts_ [ i ]); joint_state_interface_ . registerHandle ( joint_state_handle ); // Create a JointHandle (read and write) for each controllable joint // using the read-only joint handles within the JointStateInterface and // register them with the JointVelocityInterface. hardware_interface :: JointHandle joint_handle ( joint_state_handle , & joint_velocity_commands_ [ i ]); velocity_joint_interface_ . registerHandle ( joint_handle ); // Initialize joint states with zero values joint_positions_ [ i ] = 0.0 ; joint_velocities_ [ i ] = 0.0 ; joint_efforts_ [ i ] = 0.0 ; // unused with diff_drive_controller joint_velocity_commands_ [ i ] = 0.0 ; // Initialize the pid controllers for the motors using the robot namespace std :: string pid_namespace = \"pid/\" + motor_names [ i ]; ROS_INFO_STREAM ( \"pid namespace: \" << pid_namespace ); ros :: NodeHandle nh ( root_nh , pid_namespace ); // TODO implement builder pattern to initialize values otherwise it is hard to see which parameter is what. pids_ [ i ]. init ( nh , 0.0 , 10.0 , 1.0 , 1.0 , 0.0 , 0.0 , false , - max_velocity_ , max_velocity_ ); pids_ [ i ]. setOutputLimits ( - max_velocity_ , max_velocity_ ); } // Register the JointStateInterface containing the read only joints // with this robot's hardware_interface::RobotHW. registerInterface ( & joint_state_interface_ ); // Register the JointVelocityInterface containing the read/write joints // with this robot's hardware_interface::RobotHW. registerInterface ( & velocity_joint_interface_ ); ROS_INFO ( \"... Done Initializing DiffBot Hardware Interface\" ); return true ; } // The read method is part of the control loop cycle (read, update, write) and is used to // populate the robot state from the robot's hardware resources (joints, sensors, actuators). // This method should be called before controller_manager::ControllerManager::update() and write. void DiffBotHWInterface :: read ( const ros :: Time & time , const ros :: Duration & period ) { ros :: Duration elapsed_time = period ; // Read from robot hw (motor encoders) // Fill joint_state_* members with read values double wheel_angles [ 2 ]; double wheel_angle_deltas [ 2 ]; for ( std :: size_t i = 0 ; i < num_joints_ ; ++ i ) { wheel_angles [ i ] = ticksToAngle ( encoder_ticks_ [ i ]); //double wheel_angle_normalized = normalizeAngle(wheel_angle); wheel_angle_deltas [ i ] = wheel_angles [ i ] - joint_positions_ [ i ]; joint_positions_ [ i ] += wheel_angle_deltas [ i ]; joint_velocities_ [ i ] = wheel_angle_deltas [ i ] / period . toSec (); joint_efforts_ [ i ] = 0.0 ; // unused with diff_drive_controller } } // The write method is part of the control loop cycle (read, update, write) and is used to // send out commands to the robot's hardware resources (joints, actuators). // This method should be called after read and controller_manager::ControllerManager::update. void DiffBotHWInterface :: write ( const ros :: Time & time , const ros :: Duration & period ) { ros :: Duration elapsed_time = period ; // Write to robot hw // joint velocity commands from ros_control's RobotHW are in rad/s // Convert the velocity command to a percentage value for the motor // This maps the velocity to a percentage value which is used to apply // a percentage of the highest possible battery voltage to each motor. std_msgs :: Int32 left_motor ; std_msgs :: Int32 right_motor ; double output_left = pids_ [ 0 ]( joint_velocities_ [ 0 ], joint_velocity_commands_ [ 0 ], period ); double output_right = pids_ [ 1 ]( joint_velocities_ [ 1 ], joint_velocity_commands_ [ 1 ], period ); left_motor . data = output_left / max_velocity_ * 100.0 ; right_motor . data = output_right / max_velocity_ * 100.0 ; // Publish the PID computed motor commands to the left and right motors pub_left_motor_value_ . publish ( left_motor ); pub_right_motor_value_ . publish ( right_motor ); } // Process updates from encoders using a subscriber void DiffBotHWInterface :: leftEncoderTicksCallback ( const std_msgs :: Int32 :: ConstPtr & msg ) { encoder_ticks_ [ 0 ] = msg -> data ; ROS_DEBUG_STREAM_THROTTLE ( 1 , \"Left encoder ticks: \" << msg -> data ); } void DiffBotHWInterface :: rightEncoderTicksCallback ( const std_msgs :: Int32 :: ConstPtr & msg ) { encoder_ticks_ [ 1 ] = msg -> data ; ROS_DEBUG_STREAM_THROTTLE ( 1 , \"Right encoder ticks: \" << msg -> data ); } double DiffBotHWInterface :: ticksToAngle ( const int & ticks ) const { // Convert number of encoder ticks to angle in radians double angle = ( double ) ticks * ( 2.0 * M_PI / 542.0 ); ROS_DEBUG_STREAM_THROTTLE ( 1 , ticks << \" ticks correspond to an angle of \" << angle ); return angle ; } }; The functions above are designed to give the controller manager (and the controllers inside the controller manager) access to the joint state of custom robot, and to command it. When the controller manager runs, the controllers will read from the pos, vel and eff variables of the custom robot hardware interface, and the controller will write the desired command into the cmd variable. It's mandatory to make sure the pos, vel and eff variables always have the latest joint state available, and to make sure that whatever is written into the cmd variable gets executed by the robot. This can be done by implementing hardware_interface::RobotHW::read() and a hardware_interface::RobotHW::write() methods. The main node that will be executed uses the controller_manager to operate the so called control loop. In the case of DiffBot a simple example looks like the following, refer to the diffbot_base.cpp for the complete implementation: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 #include <ros/ros.h> #include <diffbot_base/diffbot_hw_interface.h> #include <controller_manager/controller_manager.h> int main ( int argc , char ** argv ) { // Initialize the ROS node ros :: init ( argc , argv , \"diffbot_hw_interface\" ); ros :: NodeHandle nh ; // Create an instance of your robot so that this instance knows about all // the resources that are available. diffbot_base :: DiffBotHWInterface diffBot ( nh ); // Create an instance of the controller manager and pass it the robot, // so that it can handle its resources. controller_manager :: ControllerManager cm ( & diffBot ); // Setup a separate thread that will be used to service ROS callbacks. // NOTE: We run the ROS loop in a separate thread as external calls such // as service callbacks to load controllers can block the (main) control loop ros :: AsyncSpinner spinner ( 1 ); spinner . start (); // Setup for the control loop. ros :: Time prev_time = ros :: Time :: now (); ros :: Rate rate ( 10.0 ); // 10 Hz rate // Blocks until shutdown signal recieved while ( ros :: ok ()) { // Basic bookkeeping to get the system time in order to compute the control period. const ros :: Time time = ros :: Time :: now (); const ros :: Duration period = time - prev_time ; // Execution of the actual control loop. diffBot . read ( time , period ); // If needed, its possible to define transmissions in software by calling the // transmission_interface::ActuatorToJointPositionInterface::propagate() // after reading the joint states. cm . update ( time , period ); // In case of software transmissions, use // transmission_interface::JointToActuatorEffortHandle::propagate() // to convert from the joint space to the actuator space. diffBot . write ( time , period ); // All these steps keep getting repeated with the specified rate. rate . sleep (); } return 0 ; } As we can see, the basic steps are to initialize the node, instantiate the hardware interface, pass it to a new controller manager and run the control loop that does the following: Read joint states from the real robot hardware Update the diff_drive_controller with read values and compute the joint velocities using the target cmd_vel Write the computed values You may be wondering why the read values aren't returned from the diffbot.read() method and nothing is passed to the diffbot.write() . This is because the RobotHW::init() method, shown in the first code snippet, is used to register the actuated joint names (described in the diffbot_description ) to the joint_position , joint_velocity and joint_effort member variables of the custom robot hardware interface. The class that registers the variables of the controller with the hardware interface member variables and thereby gives read access to all joint values without conflicting with other controllers, is the hardware_interface::JointStateInterface . ROS Control uses the hardware_interface::VelocityJointInterface (part of the joint_command_interface.h ) that registers the command member variable of the controller with the hardware interface to provide it the command that should be written to the actuators. When the controller manager runs, the controllers will read from the joint_position , joint_velocity and joint_effort variables of the custom robot hardware interface, and the controller will write the desired command into the joint_velocity_command variable. It's mandatory to make sure the position, velocity and effort (effort is not needed in the case of the diff_drive_controller ) variables always have the latest joint state available, and to make sure that whatever is written into the joint_velocity_command variable gets executed by the robot. As mentioned this can be done by implementing hardware_interface::RobotHW::read() and a hardware_interface::RobotHW::write() methods. In the control loop the overriden hardware_interface::RobotHW::read() method of DiffBot is used to read the joint states. The diff_drive_controller works with a VelocityInterface which is why the joint_position , defined in rad, and joint_velocity , defined in rad/s, are calculated from the encoder ticks. PID Controller Note the two PID controllers inside the hardware interface, where each PID is passed the error between velocity measured by the encoders and the target velocity computed by the diff_drive_controller for a specific wheel joint. The diff_drive_controller doesn't have a PID controller integrated, and doesn't take care if the wheels of the robot are actually turning. As mentioned above, ROS Control expects that the commands sent by the controller are actually implemented on the real robot hardware and that the joint states are always up to date. This means that the diff_drive_controller just uses the twist_msg on the cmd_vel topic for example from the rqt_robot_steering and converts it to a velocity command for the motors. It doesn't take the actual velocity of the motors into account. See the code of diff_drive_controller where the joint_command_velocity is calculated. This is why a PID controller is needed to avoid situations like the following where the robot moves not straigth although it is commanded to do so: The PID used here inherits from the ROS Control control_toolbox::Pid that provides Dynamic Reconfigure out of the box to tune the proportional, integral and derivative gains. The behaviour when using only the P, I and D gains is that the output can overshoot and even change between positive and negative motor percent values because of a P gain that is too high. To avoid this, a feed forward gain can help to reach the setpoint faster. To add this feed forward gain to the dynamic reconfigure parameters it is necessary to add a new parameter configuration file in this package inside a cfg folder. For more details on ROS dynamic reconfigure see the official tutorials . With the use of the PID controller the robot is able to drive straight: In case of using inexpensive motors like the DG01D-E of DiffBot, you have to take inaccurate driving behaviour into account. The straight driving behaviour can be improved with motors that start spinning at the same voltage levels. To find suitable motors do a voltage sweep test by slightly increasing the voltage and note the voltage level where each motor starts to rotate. Such a test was done on DiffBot's motors. Using six DG01D-E motors the following values were recorded (sorted by increasing voltage): Motor Voltage (V) 01 2.5 02 2.8 - 3.0 03 3.1 04 3.2 05 3.2 06 3.3 In the videos above, motors numbered 01 and 03 were used coincidencely and I wasn't aware of the remarkable differences in voltage levels. Using the motors 04 and 05 improved the driving behaviour significantly. To deal with significant differences in the motors it would also help to tune the two PIDs individually, which is not shown in the video above . Make also sure that the motor driver outputs the same voltage level on both channels when the robot is commanded to move straight. The used Grove i2c motor driver was tested to do this. Another problem of not driving straight can be weight distribution or the orientation of the caster wheel. A good test to check the accuracy is to fix two meters of adhesive tape on the floor in a straight line. Then, place the robot on one end oriented in the direction to the other end. Now command it to move straight along the line and stop it when it reaches the end of the tape. Record the lateral displacement from the tape. Measuring a value below 10 cm is considered precise for these motors. CMakeLists.txt The diffbot_hw_interface target library of the diffbot_base package depends on the custom diffbot_msgs . To have them built first, add the following to the CMakeLists.txt of the diffbot_base package: 1 add_dependencies(diffbot_hw_interface diffbot_msgs_generate_messages_cpp) Note This makes sure message headers of this package are generated before being used. If you use messages from other packages inside your catkin workspace, you need to add dependencies to their respective generation targets as well, because catkin builds all projects in parallel. 1 Launch File To run a single controller_manager, the one from the diffbot_base package defined inside difbot_base.cpp use the launch file from diffbot_base/launch/diffbot.launch : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 <!-- https://github.com/ros-controls/ros_controllers/tree/kinetic-devel/diff_drive_controller/test --> <launch> <!-- Load DiffBot model --> <param name= \"robot_description\" command= \"$(find xacro)/xacro '$(find diffbot_description)/urdf/diffbot.xacro'\" /> <node name= \"diffbot_base\" pkg= \"diffbot_base\" type= \"diffbot_base\" /> <!-- Load controller config to the parameter server --> <rosparam command= \"load\" file= \"$(find diffbot_control)/config/diffbot_control.yaml\" /> <!-- Load base config to the parameter server --> <rosparam command= \"load\" file= \"$(find diffbot_base)/config/base.yaml\" /> <!-- Load the controllers --> <node name= \"controller_spawner\" pkg= \"controller_manager\" type= \"spawner\" respawn= \"false\" output= \"screen\" ns= \"diffbot\" args= \"joint_state_controller mobile_base_controller\" /> </launch> This will load the DiffBot robot description onto the parameter server which is required for the hardware interface that gets created inside the next node diffbot_base . It creates the hardware interface and instantiates a new controller manager in the diffbot_base.cpp . Finally the spawner from the controller_manager package is used to initialize and start the controllers defined in the diffbot_control/config/diffbot_control.yaml . This step of the launch file is required to get the controllers initialized and started. Another way would be to use controller_manager::ControllerManager::loadControllers() inside the diffbot_base.cpp . Additionally the launch file loads additional parameters, stored in the diffbot_base/config/base.yaml on the parameter server. These parameters are hardware related and used to tune the driving behaviour: 1 2 3 4 5 6 7 8 9 10 11 12 # Hardware related parameters # will be loaded onto the parameter server # See the diffbot.launch diffbot: encoder_resolution: 542 gain: 1.0 trim: 0.0 motor_constant: 27.0 pwm_limit: 1.0 debug: hardware_interface: false base_controller: true After launching this launch file on DiffBot's single board computer (e.g. Raspberry Pi or Jetson Nano) with 1 roslaunch diffbot_base diffbot.launch the following parameters are stored on the parameter server: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 $ rosparam list /diffbot/hardware_interface/joints /diffbot/joint_state_controller/extra_joints /diffbot/joint_state_controller/publish_rate /diffbot/joint_state_controller/type /diffbot/mobile_base_controller/base_frame_id /diffbot/mobile_base_controller/left_wheel /diffbot/mobile_base_controller/pose_covariance_diagonal /diffbot/mobile_base_controller/publish_rate /diffbot/mobile_base_controller/right_wheel /diffbot/mobile_base_controller/twist_covariance_diagonal /diffbot/mobile_base_controller/type /diffbot/mobile_base_controller/wheel_radius /diffbot/mobile_base_controller/wheel_separation /robot_description /rosdistro /roslaunch/uris/host_tensorbook__46157 /roslaunch/uris/host_ubuntu__33729 /rosversion /run_id Additional Requirements Because the hardware interface subscribes to the encoders, that are connected to the Teensy MCU, and publishes to the motors via the motr driver node, another launch will be required to run these additional nodes. See the diffbot_bringup package for this setup. Simulation To have a simulation showing DiffBot, the second step is to use the diffbot_gazebo/launch/diffbot_base.launch on the work pc: 1 $ roslaunch diffbot_gazebo diffbot_base.launch This will launch the gazebo simulation, which can make use of the running controllers inside the controller manager too: ROS Control with Gazebo Overview. After launching the Gazebo simulation the controllers got uninitialized. (It is assumed that the gazebo_ros_control plugin that gets launched). Because of this the controllers have to be initialized and started again. For this the diffbot_base/launch/controllers.launch should be used. This launch file is just loading and starting all controllers again. Note that using the spawner from the controller_manager package, like in the diffbot_base/launch/diffbot.launch results in an error. (TODO this needs some more testing). References ROS Tutorials: Writing Publisher Subscriber, Building your nodes \u21a9","title":"Base Hardware Interface"},{"location":"diffbot_base/#diffbot-base-package","text":"This package contains the so called hardware interface of DiffBot which represents the real hardware in software to work with ROS Control . ROS Control Overview. In the simpleste case all that is needed in this package is to write a class that inherits from hardware_interface::RobotHW and provide a launch file. The launch file will Load the robot description from diffbot_description to the paramter server Run the hardware interface of this package diffbot_base Load the controller configuration yaml from the diffbot_control package to the parameter server Load the controllers with the controller manager Load the value of the encoder resolution to the parameter server","title":"DiffBot Base Package"},{"location":"diffbot_base/#diffbot_base-package","text":"The diffbot_base package is created with catkin-tools : 1 2 3 4 5 6 7 fjp@diffbot:/home/fjp/catkin_ws/src$ catkin create pkg diffbot_base --catkin-deps diff_drive_controller hardware_interface roscpp sensor_msgs rosparam_shortcuts Creating package \"diffbot_base\" in \"/home/fjp/catkin_ws/src\"... Created file diffbot_base/package.xml Created file diffbot_base/CMakeLists.txt Created folder diffbot_base/include/diffbot_base Created folder diffbot_base/src Successfully created package files in /home/fjp/catkin_ws/src/diffbot_base. To work with this package the specified dependencies must be installed either using the available Ubuntu/Debian packages for ROS Noetic or they have to be built from source first. The following table lists the dependencies that we have to install because they are not already part of the ROS Noetic desktop full installation. Refer to the section ROS Noetic Setup for how this was done. Dependency Source Ubuntu/Debian Package rosparam_shortcuts https://github.com/PickNikRobotics/rosparam_shortcuts ros-noetic-rosparam-shortcuts hardware_interface https://github.com/ros-controls/ros_control ros-noetic-ros-control diff_drive_controller https://github.com/ros-controls/ros_controllers ros-noetic-ros-controllers To install a package from source clone (using git) or download the source files from where they are located (commonly hosted on GitHub) into the src folder of a ros catkin workspace and execute the catkin build command. Also make sure to source the workspace after building new packages with source devel/setup.bash . 1 2 3 4 cd /homw/fjp/git/diffbot/ros/ # Navigate to the workspace catkin build # Build all the packages in the workspace ls build # Show the resulting build space ls devel # Show the resulting devel space Make sure to clone/download the source files suitable for the ROS distribtion you are using. If the sources are not available for the distribution you are working with, it is worth to try building anyway. Chances are that the package you want to use is suitable for multiple ROS distros. For example if a package states in its docs, that it is only available for kinetic it is possible that it will work with a ROS noetic install.","title":"diffbot_base Package"},{"location":"diffbot_base/#hardware-interface","text":"See the include and src folders of this package and the details on the hardware interface implementation. For more details on the hardware interface also refer to the section ROS Integration: Control , it gives more details and also this overview article about ROS Control . The hardware interface provides an interface between the real robot hardware and the controllers provided by ROS Control (or even custom controllers). DiffBot works with the diff_drive_controller that is configured in the diffbot_control package, which is also relevant for the simulation in Gazebo. Remember that the simulation uses the gazebo_ros_control package to communicate with the diff_drive_controller . For the real robot hardware, ROS Control uses an instance of type hardware_interface::RobotHW that is passed to the controller_manager to handle the resources, meaning that the actuated robot joints are not in use by multiple controllers that might be loaded. The skeleton of DiffBot's hardware interface looks like following, where the constructor is used to read loaded configuration values from the robot's description from the ROS parameter server: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 namespace diffbot_base { DiffBotHWInterface :: DiffBotHWInterface ( ros :: NodeHandle & nh , urdf :: Model * urdf_model ) : name_ ( \"hardware_interface\" ) , nh_ ( nh ) { // Initialization of the robot's resources (joints, sensors, actuators) and // interfaces can be done here or inside init(). // E.g. parse the URDF for joint names & interfaces, then initialize them // Check if the URDF model needs to be loaded if ( urdf_model == NULL ) loadURDF ( nh , \"robot_description\" ); else urdf_model_ = urdf_model ; // Load rosparams ros :: NodeHandle rpnh ( nh_ , name_ ); std :: size_t error = 0 ; // Code API of rosparam_shortcuts: // http://docs.ros.org/en/noetic/api/rosparam_shortcuts/html/namespacerosparam__shortcuts.html#aa6536fe0130903960b1de4872df68d5d error += ! rosparam_shortcuts :: get ( name_ , rpnh , \"joints\" , joint_names_ ); error += ! rosparam_shortcuts :: get ( name_ , nh_ , \"mobile_base_controller/wheel_radius\" , wheel_radius_ ); error += ! rosparam_shortcuts :: get ( name_ , nh_ , \"mobile_base_controller/linear/x/max_velocity\" , max_velocity_ ); rosparam_shortcuts :: shutdownIfError ( name_ , error ); wheel_diameter_ = 2.0 * wheel_radius_ ; //max_velocity_ = 0.2; // m/s // ros_control RobotHW needs velocity in rad/s but in the config its given in m/s max_velocity_ = linearToAngular ( max_velocity_ ); // Setup publisher for the motor driver pub_left_motor_value_ = nh_ . advertise < std_msgs :: Int32 > ( \"motor_left\" , 1 ); pub_right_motor_value_ = nh_ . advertise < std_msgs :: Int32 > ( \"motor_right\" , 1 ); // Setup subscriber for the wheel encoders sub_left_encoder_ticks_ = nh_ . subscribe ( \"ticks_left\" , 1 , & DiffBotHWInterface :: leftEncoderTicksCallback , this ); sub_right_encoder_ticks_ = nh_ . subscribe ( \"ticks_right\" , 1 , & DiffBotHWInterface :: rightEncoderTicksCallback , this ); // Initialize the hardware interface init ( nh_ , nh_ ); } bool DiffBotHWInterface :: init ( ros :: NodeHandle & root_nh , ros :: NodeHandle & robot_hw_nh ) { ROS_INFO ( \"Initializing DiffBot Hardware Interface ...\" ); num_joints_ = joint_names_ . size (); ROS_INFO ( \"Number of joints: %d\" , ( int ) num_joints_ ); std :: array < std :: string , NUM_JOINTS > motor_names = { \"left_motor\" , \"right_motor\" }; for ( unsigned int i = 0 ; i < num_joints_ ; i ++ ) { // Create a JointStateHandle for each joint and register them with the // JointStateInterface. hardware_interface :: JointStateHandle joint_state_handle ( joint_names_ [ i ], & joint_positions_ [ i ], & joint_velocities_ [ i ], & joint_efforts_ [ i ]); joint_state_interface_ . registerHandle ( joint_state_handle ); // Create a JointHandle (read and write) for each controllable joint // using the read-only joint handles within the JointStateInterface and // register them with the JointVelocityInterface. hardware_interface :: JointHandle joint_handle ( joint_state_handle , & joint_velocity_commands_ [ i ]); velocity_joint_interface_ . registerHandle ( joint_handle ); // Initialize joint states with zero values joint_positions_ [ i ] = 0.0 ; joint_velocities_ [ i ] = 0.0 ; joint_efforts_ [ i ] = 0.0 ; // unused with diff_drive_controller joint_velocity_commands_ [ i ] = 0.0 ; // Initialize the pid controllers for the motors using the robot namespace std :: string pid_namespace = \"pid/\" + motor_names [ i ]; ROS_INFO_STREAM ( \"pid namespace: \" << pid_namespace ); ros :: NodeHandle nh ( root_nh , pid_namespace ); // TODO implement builder pattern to initialize values otherwise it is hard to see which parameter is what. pids_ [ i ]. init ( nh , 0.0 , 10.0 , 1.0 , 1.0 , 0.0 , 0.0 , false , - max_velocity_ , max_velocity_ ); pids_ [ i ]. setOutputLimits ( - max_velocity_ , max_velocity_ ); } // Register the JointStateInterface containing the read only joints // with this robot's hardware_interface::RobotHW. registerInterface ( & joint_state_interface_ ); // Register the JointVelocityInterface containing the read/write joints // with this robot's hardware_interface::RobotHW. registerInterface ( & velocity_joint_interface_ ); ROS_INFO ( \"... Done Initializing DiffBot Hardware Interface\" ); return true ; } // The read method is part of the control loop cycle (read, update, write) and is used to // populate the robot state from the robot's hardware resources (joints, sensors, actuators). // This method should be called before controller_manager::ControllerManager::update() and write. void DiffBotHWInterface :: read ( const ros :: Time & time , const ros :: Duration & period ) { ros :: Duration elapsed_time = period ; // Read from robot hw (motor encoders) // Fill joint_state_* members with read values double wheel_angles [ 2 ]; double wheel_angle_deltas [ 2 ]; for ( std :: size_t i = 0 ; i < num_joints_ ; ++ i ) { wheel_angles [ i ] = ticksToAngle ( encoder_ticks_ [ i ]); //double wheel_angle_normalized = normalizeAngle(wheel_angle); wheel_angle_deltas [ i ] = wheel_angles [ i ] - joint_positions_ [ i ]; joint_positions_ [ i ] += wheel_angle_deltas [ i ]; joint_velocities_ [ i ] = wheel_angle_deltas [ i ] / period . toSec (); joint_efforts_ [ i ] = 0.0 ; // unused with diff_drive_controller } } // The write method is part of the control loop cycle (read, update, write) and is used to // send out commands to the robot's hardware resources (joints, actuators). // This method should be called after read and controller_manager::ControllerManager::update. void DiffBotHWInterface :: write ( const ros :: Time & time , const ros :: Duration & period ) { ros :: Duration elapsed_time = period ; // Write to robot hw // joint velocity commands from ros_control's RobotHW are in rad/s // Convert the velocity command to a percentage value for the motor // This maps the velocity to a percentage value which is used to apply // a percentage of the highest possible battery voltage to each motor. std_msgs :: Int32 left_motor ; std_msgs :: Int32 right_motor ; double output_left = pids_ [ 0 ]( joint_velocities_ [ 0 ], joint_velocity_commands_ [ 0 ], period ); double output_right = pids_ [ 1 ]( joint_velocities_ [ 1 ], joint_velocity_commands_ [ 1 ], period ); left_motor . data = output_left / max_velocity_ * 100.0 ; right_motor . data = output_right / max_velocity_ * 100.0 ; // Publish the PID computed motor commands to the left and right motors pub_left_motor_value_ . publish ( left_motor ); pub_right_motor_value_ . publish ( right_motor ); } // Process updates from encoders using a subscriber void DiffBotHWInterface :: leftEncoderTicksCallback ( const std_msgs :: Int32 :: ConstPtr & msg ) { encoder_ticks_ [ 0 ] = msg -> data ; ROS_DEBUG_STREAM_THROTTLE ( 1 , \"Left encoder ticks: \" << msg -> data ); } void DiffBotHWInterface :: rightEncoderTicksCallback ( const std_msgs :: Int32 :: ConstPtr & msg ) { encoder_ticks_ [ 1 ] = msg -> data ; ROS_DEBUG_STREAM_THROTTLE ( 1 , \"Right encoder ticks: \" << msg -> data ); } double DiffBotHWInterface :: ticksToAngle ( const int & ticks ) const { // Convert number of encoder ticks to angle in radians double angle = ( double ) ticks * ( 2.0 * M_PI / 542.0 ); ROS_DEBUG_STREAM_THROTTLE ( 1 , ticks << \" ticks correspond to an angle of \" << angle ); return angle ; } }; The functions above are designed to give the controller manager (and the controllers inside the controller manager) access to the joint state of custom robot, and to command it. When the controller manager runs, the controllers will read from the pos, vel and eff variables of the custom robot hardware interface, and the controller will write the desired command into the cmd variable. It's mandatory to make sure the pos, vel and eff variables always have the latest joint state available, and to make sure that whatever is written into the cmd variable gets executed by the robot. This can be done by implementing hardware_interface::RobotHW::read() and a hardware_interface::RobotHW::write() methods. The main node that will be executed uses the controller_manager to operate the so called control loop. In the case of DiffBot a simple example looks like the following, refer to the diffbot_base.cpp for the complete implementation: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 #include <ros/ros.h> #include <diffbot_base/diffbot_hw_interface.h> #include <controller_manager/controller_manager.h> int main ( int argc , char ** argv ) { // Initialize the ROS node ros :: init ( argc , argv , \"diffbot_hw_interface\" ); ros :: NodeHandle nh ; // Create an instance of your robot so that this instance knows about all // the resources that are available. diffbot_base :: DiffBotHWInterface diffBot ( nh ); // Create an instance of the controller manager and pass it the robot, // so that it can handle its resources. controller_manager :: ControllerManager cm ( & diffBot ); // Setup a separate thread that will be used to service ROS callbacks. // NOTE: We run the ROS loop in a separate thread as external calls such // as service callbacks to load controllers can block the (main) control loop ros :: AsyncSpinner spinner ( 1 ); spinner . start (); // Setup for the control loop. ros :: Time prev_time = ros :: Time :: now (); ros :: Rate rate ( 10.0 ); // 10 Hz rate // Blocks until shutdown signal recieved while ( ros :: ok ()) { // Basic bookkeeping to get the system time in order to compute the control period. const ros :: Time time = ros :: Time :: now (); const ros :: Duration period = time - prev_time ; // Execution of the actual control loop. diffBot . read ( time , period ); // If needed, its possible to define transmissions in software by calling the // transmission_interface::ActuatorToJointPositionInterface::propagate() // after reading the joint states. cm . update ( time , period ); // In case of software transmissions, use // transmission_interface::JointToActuatorEffortHandle::propagate() // to convert from the joint space to the actuator space. diffBot . write ( time , period ); // All these steps keep getting repeated with the specified rate. rate . sleep (); } return 0 ; } As we can see, the basic steps are to initialize the node, instantiate the hardware interface, pass it to a new controller manager and run the control loop that does the following: Read joint states from the real robot hardware Update the diff_drive_controller with read values and compute the joint velocities using the target cmd_vel Write the computed values You may be wondering why the read values aren't returned from the diffbot.read() method and nothing is passed to the diffbot.write() . This is because the RobotHW::init() method, shown in the first code snippet, is used to register the actuated joint names (described in the diffbot_description ) to the joint_position , joint_velocity and joint_effort member variables of the custom robot hardware interface. The class that registers the variables of the controller with the hardware interface member variables and thereby gives read access to all joint values without conflicting with other controllers, is the hardware_interface::JointStateInterface . ROS Control uses the hardware_interface::VelocityJointInterface (part of the joint_command_interface.h ) that registers the command member variable of the controller with the hardware interface to provide it the command that should be written to the actuators. When the controller manager runs, the controllers will read from the joint_position , joint_velocity and joint_effort variables of the custom robot hardware interface, and the controller will write the desired command into the joint_velocity_command variable. It's mandatory to make sure the position, velocity and effort (effort is not needed in the case of the diff_drive_controller ) variables always have the latest joint state available, and to make sure that whatever is written into the joint_velocity_command variable gets executed by the robot. As mentioned this can be done by implementing hardware_interface::RobotHW::read() and a hardware_interface::RobotHW::write() methods. In the control loop the overriden hardware_interface::RobotHW::read() method of DiffBot is used to read the joint states. The diff_drive_controller works with a VelocityInterface which is why the joint_position , defined in rad, and joint_velocity , defined in rad/s, are calculated from the encoder ticks.","title":"Hardware Interface"},{"location":"diffbot_base/#pid-controller","text":"Note the two PID controllers inside the hardware interface, where each PID is passed the error between velocity measured by the encoders and the target velocity computed by the diff_drive_controller for a specific wheel joint. The diff_drive_controller doesn't have a PID controller integrated, and doesn't take care if the wheels of the robot are actually turning. As mentioned above, ROS Control expects that the commands sent by the controller are actually implemented on the real robot hardware and that the joint states are always up to date. This means that the diff_drive_controller just uses the twist_msg on the cmd_vel topic for example from the rqt_robot_steering and converts it to a velocity command for the motors. It doesn't take the actual velocity of the motors into account. See the code of diff_drive_controller where the joint_command_velocity is calculated. This is why a PID controller is needed to avoid situations like the following where the robot moves not straigth although it is commanded to do so: The PID used here inherits from the ROS Control control_toolbox::Pid that provides Dynamic Reconfigure out of the box to tune the proportional, integral and derivative gains. The behaviour when using only the P, I and D gains is that the output can overshoot and even change between positive and negative motor percent values because of a P gain that is too high. To avoid this, a feed forward gain can help to reach the setpoint faster. To add this feed forward gain to the dynamic reconfigure parameters it is necessary to add a new parameter configuration file in this package inside a cfg folder. For more details on ROS dynamic reconfigure see the official tutorials . With the use of the PID controller the robot is able to drive straight: In case of using inexpensive motors like the DG01D-E of DiffBot, you have to take inaccurate driving behaviour into account. The straight driving behaviour can be improved with motors that start spinning at the same voltage levels. To find suitable motors do a voltage sweep test by slightly increasing the voltage and note the voltage level where each motor starts to rotate. Such a test was done on DiffBot's motors. Using six DG01D-E motors the following values were recorded (sorted by increasing voltage): Motor Voltage (V) 01 2.5 02 2.8 - 3.0 03 3.1 04 3.2 05 3.2 06 3.3 In the videos above, motors numbered 01 and 03 were used coincidencely and I wasn't aware of the remarkable differences in voltage levels. Using the motors 04 and 05 improved the driving behaviour significantly. To deal with significant differences in the motors it would also help to tune the two PIDs individually, which is not shown in the video above . Make also sure that the motor driver outputs the same voltage level on both channels when the robot is commanded to move straight. The used Grove i2c motor driver was tested to do this. Another problem of not driving straight can be weight distribution or the orientation of the caster wheel. A good test to check the accuracy is to fix two meters of adhesive tape on the floor in a straight line. Then, place the robot on one end oriented in the direction to the other end. Now command it to move straight along the line and stop it when it reaches the end of the tape. Record the lateral displacement from the tape. Measuring a value below 10 cm is considered precise for these motors.","title":"PID Controller"},{"location":"diffbot_base/#cmakeliststxt","text":"The diffbot_hw_interface target library of the diffbot_base package depends on the custom diffbot_msgs . To have them built first, add the following to the CMakeLists.txt of the diffbot_base package: 1 add_dependencies(diffbot_hw_interface diffbot_msgs_generate_messages_cpp) Note This makes sure message headers of this package are generated before being used. If you use messages from other packages inside your catkin workspace, you need to add dependencies to their respective generation targets as well, because catkin builds all projects in parallel. 1","title":"CMakeLists.txt"},{"location":"diffbot_base/#launch-file","text":"To run a single controller_manager, the one from the diffbot_base package defined inside difbot_base.cpp use the launch file from diffbot_base/launch/diffbot.launch : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 <!-- https://github.com/ros-controls/ros_controllers/tree/kinetic-devel/diff_drive_controller/test --> <launch> <!-- Load DiffBot model --> <param name= \"robot_description\" command= \"$(find xacro)/xacro '$(find diffbot_description)/urdf/diffbot.xacro'\" /> <node name= \"diffbot_base\" pkg= \"diffbot_base\" type= \"diffbot_base\" /> <!-- Load controller config to the parameter server --> <rosparam command= \"load\" file= \"$(find diffbot_control)/config/diffbot_control.yaml\" /> <!-- Load base config to the parameter server --> <rosparam command= \"load\" file= \"$(find diffbot_base)/config/base.yaml\" /> <!-- Load the controllers --> <node name= \"controller_spawner\" pkg= \"controller_manager\" type= \"spawner\" respawn= \"false\" output= \"screen\" ns= \"diffbot\" args= \"joint_state_controller mobile_base_controller\" /> </launch> This will load the DiffBot robot description onto the parameter server which is required for the hardware interface that gets created inside the next node diffbot_base . It creates the hardware interface and instantiates a new controller manager in the diffbot_base.cpp . Finally the spawner from the controller_manager package is used to initialize and start the controllers defined in the diffbot_control/config/diffbot_control.yaml . This step of the launch file is required to get the controllers initialized and started. Another way would be to use controller_manager::ControllerManager::loadControllers() inside the diffbot_base.cpp . Additionally the launch file loads additional parameters, stored in the diffbot_base/config/base.yaml on the parameter server. These parameters are hardware related and used to tune the driving behaviour: 1 2 3 4 5 6 7 8 9 10 11 12 # Hardware related parameters # will be loaded onto the parameter server # See the diffbot.launch diffbot: encoder_resolution: 542 gain: 1.0 trim: 0.0 motor_constant: 27.0 pwm_limit: 1.0 debug: hardware_interface: false base_controller: true After launching this launch file on DiffBot's single board computer (e.g. Raspberry Pi or Jetson Nano) with 1 roslaunch diffbot_base diffbot.launch the following parameters are stored on the parameter server: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 $ rosparam list /diffbot/hardware_interface/joints /diffbot/joint_state_controller/extra_joints /diffbot/joint_state_controller/publish_rate /diffbot/joint_state_controller/type /diffbot/mobile_base_controller/base_frame_id /diffbot/mobile_base_controller/left_wheel /diffbot/mobile_base_controller/pose_covariance_diagonal /diffbot/mobile_base_controller/publish_rate /diffbot/mobile_base_controller/right_wheel /diffbot/mobile_base_controller/twist_covariance_diagonal /diffbot/mobile_base_controller/type /diffbot/mobile_base_controller/wheel_radius /diffbot/mobile_base_controller/wheel_separation /robot_description /rosdistro /roslaunch/uris/host_tensorbook__46157 /roslaunch/uris/host_ubuntu__33729 /rosversion /run_id","title":"Launch File"},{"location":"diffbot_base/#additional-requirements","text":"Because the hardware interface subscribes to the encoders, that are connected to the Teensy MCU, and publishes to the motors via the motr driver node, another launch will be required to run these additional nodes. See the diffbot_bringup package for this setup.","title":"Additional Requirements"},{"location":"diffbot_base/#simulation","text":"To have a simulation showing DiffBot, the second step is to use the diffbot_gazebo/launch/diffbot_base.launch on the work pc: 1 $ roslaunch diffbot_gazebo diffbot_base.launch This will launch the gazebo simulation, which can make use of the running controllers inside the controller manager too: ROS Control with Gazebo Overview. After launching the Gazebo simulation the controllers got uninitialized. (It is assumed that the gazebo_ros_control plugin that gets launched). Because of this the controllers have to be initialized and started again. For this the diffbot_base/launch/controllers.launch should be used. This launch file is just loading and starting all controllers again. Note that using the spawner from the controller_manager package, like in the diffbot_base/launch/diffbot.launch results in an error. (TODO this needs some more testing).","title":"Simulation"},{"location":"diffbot_base/#references","text":"ROS Tutorials: Writing Publisher Subscriber, Building your nodes \u21a9","title":"References"},{"location":"diffbot_bringup/","text":"DiffBot Bring Up Package The bringup package is used to initialize the real hardware of DiffBot and to actually drive the robot around. First the package is created using catkin-tools : 1 2 3 4 5 fjp@diffbot:~/git/diffbot/ros/src$ catkin create pkg diffbot_bringup Creating package \"diffbot_bringup\" in \"/home/fjp/git/diffbot/ros/src\"... Created file diffbot_bringup/package.xml Created file diffbot_bringup/CMakeLists.txt Successfully created package files in /home/fjp/git/diffbot/ros/src/diffbot_bringup. The package provides a launch folder which includes minimal.launch and bringup.launch . The minimal.launch is used to load DiffBot's robot descripton and the controller configuration onto the ROS parameter server using the launch file from the diffbot_base package . It will also setup the ROS controller manager with DiffBot's hardware interface . For the motor driver the node motor_driver.py from the grove_motor_driver package is started. And for the encoders rosserial communicates with the Teensy microcontroller to publish the encoder ticks. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 <launch> <!-- Including the following launch file from diffbot_base package will --> <!-- Load the robot description onto the parameter server --> <!-- Run the controller manager with DiffBot's hardware interface --> <!-- Load the controller config onto the parameter server --> <include file= \"$(find diffbot_base)/launch/diffbot.launch\" > <!-- arg name=\"model\" value=\"$(arg model)\" /--> </include> <!-- Motors --> <!-- --> <node name= \"motor_driver\" pkg= \"grove_motor_driver\" type= \"motor_driver.py\" respawn= \"false\" output= \"screen\" ns= \"diffbot\" /> <!-- Encoders --> <!-- Run rosserial to connect with the Teensy 3.2 board connected to the motor encoders --> <node name= \"rosserial_teensy\" pkg= \"rosserial_python\" type= \"serial_node.py\" respawn= \"false\" output= \"screen\" ns= \"diffbot\" args= \"_port:=/dev/ttyACM0 _baud:=115200\" /> </launch> As mentioned, the ROS controller used for DiffBot is the diff_drive_controller . This controller publishes a transform message (see its published topics ), via the /tf topic, between the odom frame and the frame configured in the controller's configuration specified by the base_frame_id . In the case of DiffBot this is the base_footprint , a conventional link, defined in REP-120 , for mobile robots that specifies the robot's footprint. Because this is the only transform published by diff_drive_controller another node is needed to publish rest of the link transformations. It is the well known robot_state_publisher , which uses the joint states published by the ROS controller joint_state_controller (not to be confused with joint_state_publisher - it is not used here, see this answer for the difference) to create the transforms between the links. To do this the bringup.launch includes the minimal.launch and then runs the robot_state_publisher : 1 2 3 4 5 6 7 8 9 10 11 12 <launch> <include file= \"$(find diffbot_bringup)/launch/minimal.launch\" > <!-- arg name=\"model\" value=\"$(arg model)\" /--> </include> <!-- Starting robot state publish which will publish tf --> <!-- This is needed to publish transforms between all links --> <!-- diff_drive_controller publishes only a single transfrom between odom and base_footprint --> <!-- The robot_state_publisher reads the joint states published by ros control's joint_state_controller --> <node name= \"robot_state_publisher\" pkg= \"robot_state_publisher\" type= \"robot_state_publisher\" output= \"screen\" ns= \"diffbot\" /> </launch>","title":"Hardware Bringup"},{"location":"diffbot_bringup/#diffbot-bring-up-package","text":"The bringup package is used to initialize the real hardware of DiffBot and to actually drive the robot around. First the package is created using catkin-tools : 1 2 3 4 5 fjp@diffbot:~/git/diffbot/ros/src$ catkin create pkg diffbot_bringup Creating package \"diffbot_bringup\" in \"/home/fjp/git/diffbot/ros/src\"... Created file diffbot_bringup/package.xml Created file diffbot_bringup/CMakeLists.txt Successfully created package files in /home/fjp/git/diffbot/ros/src/diffbot_bringup. The package provides a launch folder which includes minimal.launch and bringup.launch . The minimal.launch is used to load DiffBot's robot descripton and the controller configuration onto the ROS parameter server using the launch file from the diffbot_base package . It will also setup the ROS controller manager with DiffBot's hardware interface . For the motor driver the node motor_driver.py from the grove_motor_driver package is started. And for the encoders rosserial communicates with the Teensy microcontroller to publish the encoder ticks. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 <launch> <!-- Including the following launch file from diffbot_base package will --> <!-- Load the robot description onto the parameter server --> <!-- Run the controller manager with DiffBot's hardware interface --> <!-- Load the controller config onto the parameter server --> <include file= \"$(find diffbot_base)/launch/diffbot.launch\" > <!-- arg name=\"model\" value=\"$(arg model)\" /--> </include> <!-- Motors --> <!-- --> <node name= \"motor_driver\" pkg= \"grove_motor_driver\" type= \"motor_driver.py\" respawn= \"false\" output= \"screen\" ns= \"diffbot\" /> <!-- Encoders --> <!-- Run rosserial to connect with the Teensy 3.2 board connected to the motor encoders --> <node name= \"rosserial_teensy\" pkg= \"rosserial_python\" type= \"serial_node.py\" respawn= \"false\" output= \"screen\" ns= \"diffbot\" args= \"_port:=/dev/ttyACM0 _baud:=115200\" /> </launch> As mentioned, the ROS controller used for DiffBot is the diff_drive_controller . This controller publishes a transform message (see its published topics ), via the /tf topic, between the odom frame and the frame configured in the controller's configuration specified by the base_frame_id . In the case of DiffBot this is the base_footprint , a conventional link, defined in REP-120 , for mobile robots that specifies the robot's footprint. Because this is the only transform published by diff_drive_controller another node is needed to publish rest of the link transformations. It is the well known robot_state_publisher , which uses the joint states published by the ROS controller joint_state_controller (not to be confused with joint_state_publisher - it is not used here, see this answer for the difference) to create the transforms between the links. To do this the bringup.launch includes the minimal.launch and then runs the robot_state_publisher : 1 2 3 4 5 6 7 8 9 10 11 12 <launch> <include file= \"$(find diffbot_bringup)/launch/minimal.launch\" > <!-- arg name=\"model\" value=\"$(arg model)\" /--> </include> <!-- Starting robot state publish which will publish tf --> <!-- This is needed to publish transforms between all links --> <!-- diff_drive_controller publishes only a single transfrom between odom and base_footprint --> <!-- The robot_state_publisher reads the joint states published by ros control's joint_state_controller --> <node name= \"robot_state_publisher\" pkg= \"robot_state_publisher\" type= \"robot_state_publisher\" output= \"screen\" ns= \"diffbot\" /> </launch>","title":"DiffBot Bring Up Package"},{"location":"diffbot_control/","text":"DiffBot Control Package As described in the ROS Integration and Gazebo Simulation sections, DiffBot makes use of ROS Control repositories. Specifically the diff_drive_controller package from the ros_controllers meta package. To leverage ROS Control for the simulation with Gazebo the robot description and the controller configuration (usually a MYROBOT_control.yaml file) is required. For the real hardware its required to implement a class derived from hardware_interface::RobotHW . The convention to control a robot (in simulation and in the real world) is to have a package named MYROBOT_control . In case of DiffBot its called diffbot_control and created with catkin create pkg PKG_NAME [--catkin-deps [DEP [DEP ...]]] : 1 2 3 4 5 6 7 catkin create pkg diffbot_control --catkin-deps diff_drive_controller roscpp sensor_msgs Creating package \"diffbot_control\" in \"/home/fjp/git/diffbot/ros/src\"... Created file diffbot_control/CMakeLists.txt Created file diffbot_control/package.xml Created folder diffbot_control/include/diffbot_control Created folder diffbot_control/src Successfully created package files in /home/fjp/git/diffbot/ros/src/diffbot_control. To work with this package the specified dependencies must be installed either using the available Ubuntu/Debian packages for ROS Noetic or have to be built from source first. The following table lists the dependencies that we have to install because they are not already part of the ROS Noetic desktop full installation. Refer to the section ROS Noetic Setup for how this was done. Dependency Source Ubuntu/Debian Package diff_drive_controller https://github.com/ros-controls/ros_controllers ros-noetic-ros-controllers To install a package from source clone (using git) or download the source files from where they are located (commonly hosted on GitHub) into the src folder of a ros catkin workspace and execute the catkin build command. Also make sure to source the workspace after building new packages with source devel/setup.bash . 1 2 3 4 cd /home/fjp/git/diffbot/ros/ # Navigate to the workspace catkin build # Build all the packages in the workspace ls build # Show the resulting build space ls devel # Show the resulting devel space Make sure to clone/download the source files suitable for the ROS distribtion you are using. If the sources are not available for the distribution you are working with, it is worth to try building anyway. Chances are that the package you want to use is suitable for multiple ROS distros. For example if a package states in its docs, that it is only available for kinetic it is possible that it will work with a ROS noetic install. ROS Control in Gazebo Two great resources to get the diff_drive_controller working inside Gazebo is the Gazebo ROS Control Tutorial of rrbot and the R2D2 ROS URDF Tutorial , especially the last section, The Wheels on the Droid Go Round and Round . To spawn DiffBot inside Gazebo, RViz and control it with the rqt_robot_steering plugin, launch the diffbot.launch inside the diffbot_control package: 1 roslaunch diffbot_control diffbot.launch This launch file makes use of diffbot_gazebo/launch/diffbot.launch , diffbot_control/launch/diffbot_control.launch to run gazebo and the diff_drive_controller . It also opens RViz with the configuration stored in diffbot_control/rviz/diffbot.rviz . The following video shows the result of launching. Note the video may be outdated when you read this and the model has improved. ROS Control on the Real Hardware As mentioned above the its required to implement a class derived from hardware_interface::RobotHW . Let's call it DiffBotHW and create it inside the diffbot_control/src folder.","title":"Control"},{"location":"diffbot_control/#diffbot-control-package","text":"As described in the ROS Integration and Gazebo Simulation sections, DiffBot makes use of ROS Control repositories. Specifically the diff_drive_controller package from the ros_controllers meta package. To leverage ROS Control for the simulation with Gazebo the robot description and the controller configuration (usually a MYROBOT_control.yaml file) is required. For the real hardware its required to implement a class derived from hardware_interface::RobotHW . The convention to control a robot (in simulation and in the real world) is to have a package named MYROBOT_control . In case of DiffBot its called diffbot_control and created with catkin create pkg PKG_NAME [--catkin-deps [DEP [DEP ...]]] : 1 2 3 4 5 6 7 catkin create pkg diffbot_control --catkin-deps diff_drive_controller roscpp sensor_msgs Creating package \"diffbot_control\" in \"/home/fjp/git/diffbot/ros/src\"... Created file diffbot_control/CMakeLists.txt Created file diffbot_control/package.xml Created folder diffbot_control/include/diffbot_control Created folder diffbot_control/src Successfully created package files in /home/fjp/git/diffbot/ros/src/diffbot_control. To work with this package the specified dependencies must be installed either using the available Ubuntu/Debian packages for ROS Noetic or have to be built from source first. The following table lists the dependencies that we have to install because they are not already part of the ROS Noetic desktop full installation. Refer to the section ROS Noetic Setup for how this was done. Dependency Source Ubuntu/Debian Package diff_drive_controller https://github.com/ros-controls/ros_controllers ros-noetic-ros-controllers To install a package from source clone (using git) or download the source files from where they are located (commonly hosted on GitHub) into the src folder of a ros catkin workspace and execute the catkin build command. Also make sure to source the workspace after building new packages with source devel/setup.bash . 1 2 3 4 cd /home/fjp/git/diffbot/ros/ # Navigate to the workspace catkin build # Build all the packages in the workspace ls build # Show the resulting build space ls devel # Show the resulting devel space Make sure to clone/download the source files suitable for the ROS distribtion you are using. If the sources are not available for the distribution you are working with, it is worth to try building anyway. Chances are that the package you want to use is suitable for multiple ROS distros. For example if a package states in its docs, that it is only available for kinetic it is possible that it will work with a ROS noetic install.","title":"DiffBot Control Package"},{"location":"diffbot_control/#ros-control-in-gazebo","text":"Two great resources to get the diff_drive_controller working inside Gazebo is the Gazebo ROS Control Tutorial of rrbot and the R2D2 ROS URDF Tutorial , especially the last section, The Wheels on the Droid Go Round and Round . To spawn DiffBot inside Gazebo, RViz and control it with the rqt_robot_steering plugin, launch the diffbot.launch inside the diffbot_control package: 1 roslaunch diffbot_control diffbot.launch This launch file makes use of diffbot_gazebo/launch/diffbot.launch , diffbot_control/launch/diffbot_control.launch to run gazebo and the diff_drive_controller . It also opens RViz with the configuration stored in diffbot_control/rviz/diffbot.rviz . The following video shows the result of launching. Note the video may be outdated when you read this and the model has improved.","title":"ROS Control in Gazebo"},{"location":"diffbot_control/#ros-control-on-the-real-hardware","text":"As mentioned above the its required to implement a class derived from hardware_interface::RobotHW . Let's call it DiffBotHW and create it inside the diffbot_control/src folder.","title":"ROS Control on the Real Hardware"},{"location":"diffbot_gazebo/","text":"Simulate DiffBot in Gazebo As described in the Creating your own Gazebo ROS Package , it is common in ROS to create a package that contains all the world files and launch files used with Gazebo. These files are located in a ROS package named /MYROBOT_gazebo . For DiffBot the package is named diffbot_gazebo . Another example that follows best pratices is rrbot which can be found in the gazebo_ros_demos repository. 1 2 3 4 5 fjp@ubuntu:~/git/diffbot/ros/src$ catkin create pkg diffbot_gazebo Creating package \"diffbot_gazebo\" in \"/home/fjp/git/diffbot/ros/src\"... Created file diffbot_gazebo/package.xml Created file diffbot_gazebo/CMakeLists.txt Successfully created package files in /home/fjp/git/diffbot/ros/src/diffbot_gazebo. The diffbot_gazebo package contains a launch file to lauch a world in Gazebo and spawn the robot model, which is defined in the previously created diffbot_description package. For the launch files the convention is to have a folder named launch and for Gazebo world files a folder named world inside a package. 1 fjp@ubuntu:~/git/diffbot/ros/src/diffbot_gazebo$ mkdir launch world Inside the launch folder is the diffbot.launch . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 <launch> <!-- these are the arguments you can pass this launch file, for example paused:=true --> <arg name= \"paused\" default= \"false\" /> <arg name= \"use_sim_time\" default= \"true\" /> <arg name= \"gui\" default= \"true\" /> <arg name= \"headless\" default= \"false\" /> <arg name= \"debug\" default= \"false\" /> <!-- We resume the logic in empty_world.launch, changing only the name of the world to be launched --> <include file= \"$(find gazebo_ros)/launch/empty_world.launch\" > <arg name= \"world_name\" value= \"$(find diffbot_gazebo)/worlds/diffbot.world\" /> <arg name= \"debug\" value= \"$(arg debug)\" /> <arg name= \"gui\" value= \"$(arg gui)\" /> <arg name= \"paused\" value= \"$(arg paused)\" /> <arg name= \"use_sim_time\" value= \"$(arg use_sim_time)\" /> <arg name= \"headless\" value= \"$(arg headless)\" /> </include> </launch> In the world folder of the diffbot_gazebo package is the diffbot.world file: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 <?xml version=\"1.0\" ?> <sdf version= \"1.4\" > <world name= \"default\" > <include> <uri> model://ground_plane </uri> </include> <include> <uri> model://sun </uri> </include> <include> <uri> model://gas_station </uri> <name> gas_station </name> <pose> -2.0 7.0 0 0 0 0 </pose> </include> </world> </sdf> With these files build the catkin workspace and source it to make the new diffbot_gazebo package visible to roslaunch : 1 2 catkin build source devel/setup.zsh Then its possible to launch the diffbot.launch with: 1 roslaunch diffbot_gazebo diffbot.launch This will lead to the following output: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 roslaunch diffbot_gazebo diffbot.launch ... logging to /home/fjp/.ros/log/6be90ef2-fdd8-11ea-9cb3-317fd602d5f2/roslaunch-tensorbook-393333.log Checking log directory for disk usage. This may take a while. Press Ctrl-C to interrupt Done checking log file disk usage. Usage is <1GB. started roslaunch server http://tensorbook:32837/ SUMMARY ======== PARAMETERS * /gazebo/enable_ros_network: True * /rosdistro: noetic * /rosversion: 1.15.8 * /use_sim_time: True NODES / gazebo (gazebo_ros/gzserver) gazebo_gui (gazebo_ros/gzclient) ROS_MASTER_URI=http://localhost:11311 process[gazebo-1]: started with pid [393352] process[gazebo_gui-2]: started with pid [393357] [ INFO] [1600950165.494721382]: Finished loading Gazebo ROS API Plugin. [ INFO] [1600950165.495515766]: waitForService: Service [/gazebo/set_physics_properties] has not been advertised, waiting... [ INFO] [1600950165.649461740]: Finished loading Gazebo ROS API Plugin. [ INFO] [1600950165.650277038]: waitForService: Service [/gazebo_gui/set_physics_properties] has not been advertised, waiting... [ INFO] [1600950166.640929113]: waitForService: Service [/gazebo/set_physics_properties] is now available. [ INFO] [1600950166.659917502, 0.007000000]: Physics dynamic reconfigure ready. Also, the Gazebo simulator will open a new window with the objects defined in diffbot.world except for the Gas station because it is a model that has to be downloaded first, which is happening in the background. TODO: gas station is not showing this way. Empty world of DiffBot. To get the Gas station or other available models it is possible to clone the gazebo models repository into your /home/your_username/.gazebo folder, e.g.: 1 /home/fjp/.gazeb$ git clone osrf/gazebo_models Then add this path inside Gazebo to insert these models into your world file. Using ROS launch to Spawn URDF Robots According to the Gazebo roslaunch tutorial the recommended way to spawn a robot into Gazebo is to use a launch file. Therefore, edit the diffbot.launch inside the diffbot_gazebo package by adding the following inside the <launch> </launch tag: 1 2 3 4 5 6 7 <!-- Load the URDF into the ROS Parameter Server --> <param name=\"robot_description\" command=\"$(find xacro)/xacro --inorder '$(find diffbot_description)/urdf/diffbot.xacro'\" /> <!-- Run a python script to the send a service call to gazebo_ros to spawn a URDF robot --> <node name=\"urdf_spawner\" pkg=\"gazebo_ros\" type=\"spawn_model\" respawn=\"false\" output=\"screen\" args=\"-urdf -model diffbot -param robot_description\"/> See also the complete diffbot.launch file. This will open Gazebo simulator and show the DiffBot model: Empty world including DiffBot. Moving the Robot Note that the robot cannot be moved without having either a Gazebo plugin loaded or making use of ROS Control and its Gazebo plugin gazebo_ros_control , see also the Gazebo ROS Control Tutorial . Using the ROS Control and its Gazebo plugin is done in case of DiffBot. An alternative would be to use the existing differential_drive_controller Gazebo plugin without having to rely on ROS Control. The next section explains the diffbot_control package in more detail and how to setup the diff_drive_controller from the ros_controllers package. Adding Sensors To add sensors to a robot model make use of link and joint tags to define the desired location and shape, possibly using meshes. For the simulation of these sensor there exist common Gazebo plugins that can be used. See, the [Tutorial: Using Gazebo plugins with ROS] for existing plugins and more details how to use them. For a full list of plugins see also gazebo_ros_pkgs which is a package or interface for using ROS with the Gazebo simulator. Camera This section follows Gazebo tutorial Adding a Camera . Laser (Lidar) This section follows Gazebo tutorial Adding a Laser GPU . Ultrasonic Ranger See the source of the gazebo_ros_range plugin. Inertial Measurement Unit (IMU) This section follows Gazebo tutorial [Adding an IMU](http://gazebosim.org/tutorials?tut=ros_gzplugins#IMU(GazeboRosImu). Note use GazeboRosImuSensor? Troubleshooting A quick way to verify if the conversion from xacro to urdf to sdf is working is the following ( source: Tutorial URDF in Gazebo ): First convert the xacro model to a urdf model with the xacro command: 1 xacro src/diffbot_description/urdf/diffbot.xacro -o diffbot.urdf This will output the urdf into a file named diffbot.urdf in the current working directory. Then use the gz command to create a sdf: 1 2 # gazebo3 and above gz sdf -p MODEL.urdf DiffBot sdf. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 <sdf version= '1.7' > <model name= 'diffbot' > <link name= 'base_footprint' > <inertial> <pose> -0.012273 0 0.040818 0 -0 0 </pose> <mass> 5.5 </mass> <inertia> <ixx> 0.0387035 </ixx> <ixy> 0 </ixy> <ixz> 0.000552273 </ixz> <iyy> 0.0188626 </iyy> <iyz> 0 </iyz> <izz> 0.0561591 </izz> </inertia> </inertial> <collision name= 'base_footprint_collision' > <pose> 0 0 0 0 -0 0 </pose> <geometry> <box> <size> 0.001 0.001 0.001 </size> </box> </geometry> <surface> <contact> <ode/> </contact> <friction> <ode/> </friction> </surface> </collision> <collision name= 'base_footprint_fixed_joint_lump__base_link_collision_1' > <pose> 0 0 0.04 0 -0 0 </pose> <geometry> <box> <size> 0.3 0.15 0.02 </size> </box> </geometry> <surface> <contact> <ode/> </contact> <friction> <ode/> </friction> </surface> </collision> <collision name= 'base_footprint_fixed_joint_lump__caster_link_collision_2' > <pose> -0.135 0 0.029 0 -0 0 </pose> <geometry> <sphere> <radius> 0.025 </radius> </sphere> </geometry> <surface> <contact> <ode/> </contact> <friction> <ode/> </friction> </surface> </collision> <visual name= 'base_footprint_fixed_joint_lump__base_link_visual' > <pose> 0 0 0.04 0 -0 0 </pose> <geometry> <box> <size> 0.3 0.15 0.02 </size> </box> </geometry> <material> <script> <name> Gazebo/White </name> <uri> file://media/materials/scripts/gazebo.material </uri> </script> </material> </visual> <visual name= 'base_footprint_fixed_joint_lump__caster_link_visual_1' > <pose> -0.115 0 0.029 0 -0 0 </pose> <geometry> <sphere> <radius> 0.025 </radius> </sphere> </geometry> </visual> <velocity_decay/> <velocity_decay/> <gravity> 1 </gravity> <velocity_decay/> </link> <joint name= 'front_left_wheel_joint' type= 'revolute' > <pose relative_to= 'base_footprint' > 0.105 -0.085 0.04 0 -0 0 </pose> <parent> base_footprint </parent> <child> front_left_wheel </child> <axis> <xyz> 0 1 0 </xyz> <limit> <lower> -1e+16 </lower> <upper> 1e+16 </upper> </limit> <dynamics> <spring_reference> 0 </spring_reference> <spring_stiffness> 0 </spring_stiffness> </dynamics> </axis> </joint> <link name= 'front_left_wheel' > <pose relative_to= 'front_left_wheel_joint' > 0 0 0 0 -0 0 </pose> <inertial> <pose> 0 0 0 0 -0 0 </pose> <mass> 2.5 </mass> <inertia> <ixx> 0.00108333 </ixx> <ixy> 0 </ixy> <ixz> 0 </ixz> <iyy> 0.00108333 </iyy> <iyz> 0 </iyz> <izz> 0.002 </izz> </inertia> </inertial> <collision name= 'front_left_wheel_collision' > <pose> 0 0 0 1.5708 -0 0 </pose> <geometry> <cylinder> <length> 0.02 </length> <radius> 0.04 </radius> </cylinder> </geometry> <surface> <contact> <ode> <kp> 1e+07 </kp> <kd> 1 </kd> </ode> </contact> <friction> <ode> <mu> 1 </mu> <mu2> 1 </mu2> <fdir1> 1 0 0 </fdir1> </ode> </friction> </surface> </collision> <visual name= 'front_left_wheel_visual' > <pose> 0 0 0 1.5708 -0 0 </pose> <geometry> <cylinder> <length> 0.02 </length> <radius> 0.04 </radius> </cylinder> </geometry> <material> <script> <name> Gazebo/Grey </name> <uri> file://media/materials/scripts/gazebo.material </uri> </script> </material> </visual> <gravity> 1 </gravity> <velocity_decay/> </link> <joint name= 'front_right_wheel_joint' type= 'revolute' > <pose relative_to= 'base_footprint' > 0.105 0.085 0.04 0 -0 0 </pose> <parent> base_footprint </parent> <child> front_right_wheel </child> <axis> <xyz> 0 1 0 </xyz> <limit> <lower> -1e+16 </lower> <upper> 1e+16 </upper> </limit> <dynamics> <spring_reference> 0 </spring_reference> <spring_stiffness> 0 </spring_stiffness> </dynamics> </axis> </joint> <link name= 'front_right_wheel' > <pose relative_to= 'front_right_wheel_joint' > 0 0 0 0 -0 0 </pose> <inertial> <pose> 0 0 0 0 -0 0 </pose> <mass> 2.5 </mass> <inertia> <ixx> 0.00108333 </ixx> <ixy> 0 </ixy> <ixz> 0 </ixz> <iyy> 0.00108333 </iyy> <iyz> 0 </iyz> <izz> 0.002 </izz> </inertia> </inertial> <collision name= 'front_right_wheel_collision' > <pose> 0 0 0 1.5708 -0 0 </pose> <geometry> <cylinder> <length> 0.02 </length> <radius> 0.04 </radius> </cylinder> </geometry> <surface> <contact> <ode> <kp> 1e+07 </kp> <kd> 1 </kd> </ode> </contact> <friction> <ode> <mu> 1 </mu> <mu2> 1 </mu2> <fdir1> 1 0 0 </fdir1> </ode> </friction> </surface> </collision> <visual name= 'front_right_wheel_visual' > <pose> 0 0 0 1.5708 -0 0 </pose> <geometry> <cylinder> <length> 0.02 </length> <radius> 0.04 </radius> </cylinder> </geometry> <material> <script> <name> Gazebo/Grey </name> <uri> file://media/materials/scripts/gazebo.material </uri> </script> </material> </visual> <gravity> 1 </gravity> <velocity_decay/> </link> <plugin name= 'gazebo_ros_control' filename= 'libgazebo_ros_control.so' > <robotNamespace> /diffbot </robotNamespace> <robotSimType> gazebo_ros_control/DefaultRobotHWSim </robotSimType> </plugin> <static> 0 </static> <plugin name= 'differential_drive_controller' filename= 'libgazebo_ros_diff_drive.so' > <legacyMode> 1 </legacyMode> <rosDebugLevel> Debug </rosDebugLevel> <publishWheelTF> 0 </publishWheelTF> <robotNamespace> / </robotNamespace> <publishTf> 1 </publishTf> <publishWheelJointState> 0 </publishWheelJointState> <alwaysOn> 1 </alwaysOn> <updateRate> 100.0 </updateRate> <leftJoint> front_left_wheel_joint </leftJoint> <rightJoint> front_right_wheel_joint </rightJoint> <wheelSeparation> 0.3 </wheelSeparation> <wheelDiameter> 0.08 </wheelDiameter> <broadcastTF> 1 </broadcastTF> <wheelTorque> 30 </wheelTorque> <wheelAcceleration> 1.8 </wheelAcceleration> <commandTopic> cmd_vel </commandTopic> <odometryFrame> odom </odometryFrame> <odometryTopic> odom </odometryTopic> <robotBaseFrame> base_footprint </robotBaseFrame> </plugin> </model> </sdf> In case the output looks like the following, there are most certainly missing <inertial> tags in the <link> tag. For the Gazebo simulator the <inertial> must be present, in order to simulate the dynamics of the robot. See also http://wiki.ros.org/urdf/XML/link and the Gazebo tutorials on URDF . 1 2 3 <sdf version= '1.7' > <model name= 'diffbot' /> </sdf>","title":"Simulation"},{"location":"diffbot_gazebo/#simulate-diffbot-in-gazebo","text":"As described in the Creating your own Gazebo ROS Package , it is common in ROS to create a package that contains all the world files and launch files used with Gazebo. These files are located in a ROS package named /MYROBOT_gazebo . For DiffBot the package is named diffbot_gazebo . Another example that follows best pratices is rrbot which can be found in the gazebo_ros_demos repository. 1 2 3 4 5 fjp@ubuntu:~/git/diffbot/ros/src$ catkin create pkg diffbot_gazebo Creating package \"diffbot_gazebo\" in \"/home/fjp/git/diffbot/ros/src\"... Created file diffbot_gazebo/package.xml Created file diffbot_gazebo/CMakeLists.txt Successfully created package files in /home/fjp/git/diffbot/ros/src/diffbot_gazebo. The diffbot_gazebo package contains a launch file to lauch a world in Gazebo and spawn the robot model, which is defined in the previously created diffbot_description package. For the launch files the convention is to have a folder named launch and for Gazebo world files a folder named world inside a package. 1 fjp@ubuntu:~/git/diffbot/ros/src/diffbot_gazebo$ mkdir launch world Inside the launch folder is the diffbot.launch . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 <launch> <!-- these are the arguments you can pass this launch file, for example paused:=true --> <arg name= \"paused\" default= \"false\" /> <arg name= \"use_sim_time\" default= \"true\" /> <arg name= \"gui\" default= \"true\" /> <arg name= \"headless\" default= \"false\" /> <arg name= \"debug\" default= \"false\" /> <!-- We resume the logic in empty_world.launch, changing only the name of the world to be launched --> <include file= \"$(find gazebo_ros)/launch/empty_world.launch\" > <arg name= \"world_name\" value= \"$(find diffbot_gazebo)/worlds/diffbot.world\" /> <arg name= \"debug\" value= \"$(arg debug)\" /> <arg name= \"gui\" value= \"$(arg gui)\" /> <arg name= \"paused\" value= \"$(arg paused)\" /> <arg name= \"use_sim_time\" value= \"$(arg use_sim_time)\" /> <arg name= \"headless\" value= \"$(arg headless)\" /> </include> </launch> In the world folder of the diffbot_gazebo package is the diffbot.world file: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 <?xml version=\"1.0\" ?> <sdf version= \"1.4\" > <world name= \"default\" > <include> <uri> model://ground_plane </uri> </include> <include> <uri> model://sun </uri> </include> <include> <uri> model://gas_station </uri> <name> gas_station </name> <pose> -2.0 7.0 0 0 0 0 </pose> </include> </world> </sdf> With these files build the catkin workspace and source it to make the new diffbot_gazebo package visible to roslaunch : 1 2 catkin build source devel/setup.zsh Then its possible to launch the diffbot.launch with: 1 roslaunch diffbot_gazebo diffbot.launch This will lead to the following output: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 roslaunch diffbot_gazebo diffbot.launch ... logging to /home/fjp/.ros/log/6be90ef2-fdd8-11ea-9cb3-317fd602d5f2/roslaunch-tensorbook-393333.log Checking log directory for disk usage. This may take a while. Press Ctrl-C to interrupt Done checking log file disk usage. Usage is <1GB. started roslaunch server http://tensorbook:32837/ SUMMARY ======== PARAMETERS * /gazebo/enable_ros_network: True * /rosdistro: noetic * /rosversion: 1.15.8 * /use_sim_time: True NODES / gazebo (gazebo_ros/gzserver) gazebo_gui (gazebo_ros/gzclient) ROS_MASTER_URI=http://localhost:11311 process[gazebo-1]: started with pid [393352] process[gazebo_gui-2]: started with pid [393357] [ INFO] [1600950165.494721382]: Finished loading Gazebo ROS API Plugin. [ INFO] [1600950165.495515766]: waitForService: Service [/gazebo/set_physics_properties] has not been advertised, waiting... [ INFO] [1600950165.649461740]: Finished loading Gazebo ROS API Plugin. [ INFO] [1600950165.650277038]: waitForService: Service [/gazebo_gui/set_physics_properties] has not been advertised, waiting... [ INFO] [1600950166.640929113]: waitForService: Service [/gazebo/set_physics_properties] is now available. [ INFO] [1600950166.659917502, 0.007000000]: Physics dynamic reconfigure ready. Also, the Gazebo simulator will open a new window with the objects defined in diffbot.world except for the Gas station because it is a model that has to be downloaded first, which is happening in the background. TODO: gas station is not showing this way. Empty world of DiffBot. To get the Gas station or other available models it is possible to clone the gazebo models repository into your /home/your_username/.gazebo folder, e.g.: 1 /home/fjp/.gazeb$ git clone osrf/gazebo_models Then add this path inside Gazebo to insert these models into your world file.","title":"Simulate DiffBot in Gazebo"},{"location":"diffbot_gazebo/#using-ros-launch-to-spawn-urdf-robots","text":"According to the Gazebo roslaunch tutorial the recommended way to spawn a robot into Gazebo is to use a launch file. Therefore, edit the diffbot.launch inside the diffbot_gazebo package by adding the following inside the <launch> </launch tag: 1 2 3 4 5 6 7 <!-- Load the URDF into the ROS Parameter Server --> <param name=\"robot_description\" command=\"$(find xacro)/xacro --inorder '$(find diffbot_description)/urdf/diffbot.xacro'\" /> <!-- Run a python script to the send a service call to gazebo_ros to spawn a URDF robot --> <node name=\"urdf_spawner\" pkg=\"gazebo_ros\" type=\"spawn_model\" respawn=\"false\" output=\"screen\" args=\"-urdf -model diffbot -param robot_description\"/> See also the complete diffbot.launch file. This will open Gazebo simulator and show the DiffBot model: Empty world including DiffBot.","title":"Using ROS launch to Spawn URDF Robots"},{"location":"diffbot_gazebo/#moving-the-robot","text":"Note that the robot cannot be moved without having either a Gazebo plugin loaded or making use of ROS Control and its Gazebo plugin gazebo_ros_control , see also the Gazebo ROS Control Tutorial . Using the ROS Control and its Gazebo plugin is done in case of DiffBot. An alternative would be to use the existing differential_drive_controller Gazebo plugin without having to rely on ROS Control. The next section explains the diffbot_control package in more detail and how to setup the diff_drive_controller from the ros_controllers package.","title":"Moving the Robot"},{"location":"diffbot_gazebo/#adding-sensors","text":"To add sensors to a robot model make use of link and joint tags to define the desired location and shape, possibly using meshes. For the simulation of these sensor there exist common Gazebo plugins that can be used. See, the [Tutorial: Using Gazebo plugins with ROS] for existing plugins and more details how to use them. For a full list of plugins see also gazebo_ros_pkgs which is a package or interface for using ROS with the Gazebo simulator.","title":"Adding Sensors"},{"location":"diffbot_gazebo/#camera","text":"This section follows Gazebo tutorial Adding a Camera .","title":"Camera"},{"location":"diffbot_gazebo/#laser-lidar","text":"This section follows Gazebo tutorial Adding a Laser GPU .","title":"Laser (Lidar)"},{"location":"diffbot_gazebo/#ultrasonic-ranger","text":"See the source of the gazebo_ros_range plugin.","title":"Ultrasonic Ranger"},{"location":"diffbot_gazebo/#inertial-measurement-unit-imu","text":"This section follows Gazebo tutorial [Adding an IMU](http://gazebosim.org/tutorials?tut=ros_gzplugins#IMU(GazeboRosImu). Note use GazeboRosImuSensor?","title":"Inertial Measurement Unit (IMU)"},{"location":"diffbot_gazebo/#troubleshooting","text":"A quick way to verify if the conversion from xacro to urdf to sdf is working is the following ( source: Tutorial URDF in Gazebo ): First convert the xacro model to a urdf model with the xacro command: 1 xacro src/diffbot_description/urdf/diffbot.xacro -o diffbot.urdf This will output the urdf into a file named diffbot.urdf in the current working directory. Then use the gz command to create a sdf: 1 2 # gazebo3 and above gz sdf -p MODEL.urdf DiffBot sdf. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 <sdf version= '1.7' > <model name= 'diffbot' > <link name= 'base_footprint' > <inertial> <pose> -0.012273 0 0.040818 0 -0 0 </pose> <mass> 5.5 </mass> <inertia> <ixx> 0.0387035 </ixx> <ixy> 0 </ixy> <ixz> 0.000552273 </ixz> <iyy> 0.0188626 </iyy> <iyz> 0 </iyz> <izz> 0.0561591 </izz> </inertia> </inertial> <collision name= 'base_footprint_collision' > <pose> 0 0 0 0 -0 0 </pose> <geometry> <box> <size> 0.001 0.001 0.001 </size> </box> </geometry> <surface> <contact> <ode/> </contact> <friction> <ode/> </friction> </surface> </collision> <collision name= 'base_footprint_fixed_joint_lump__base_link_collision_1' > <pose> 0 0 0.04 0 -0 0 </pose> <geometry> <box> <size> 0.3 0.15 0.02 </size> </box> </geometry> <surface> <contact> <ode/> </contact> <friction> <ode/> </friction> </surface> </collision> <collision name= 'base_footprint_fixed_joint_lump__caster_link_collision_2' > <pose> -0.135 0 0.029 0 -0 0 </pose> <geometry> <sphere> <radius> 0.025 </radius> </sphere> </geometry> <surface> <contact> <ode/> </contact> <friction> <ode/> </friction> </surface> </collision> <visual name= 'base_footprint_fixed_joint_lump__base_link_visual' > <pose> 0 0 0.04 0 -0 0 </pose> <geometry> <box> <size> 0.3 0.15 0.02 </size> </box> </geometry> <material> <script> <name> Gazebo/White </name> <uri> file://media/materials/scripts/gazebo.material </uri> </script> </material> </visual> <visual name= 'base_footprint_fixed_joint_lump__caster_link_visual_1' > <pose> -0.115 0 0.029 0 -0 0 </pose> <geometry> <sphere> <radius> 0.025 </radius> </sphere> </geometry> </visual> <velocity_decay/> <velocity_decay/> <gravity> 1 </gravity> <velocity_decay/> </link> <joint name= 'front_left_wheel_joint' type= 'revolute' > <pose relative_to= 'base_footprint' > 0.105 -0.085 0.04 0 -0 0 </pose> <parent> base_footprint </parent> <child> front_left_wheel </child> <axis> <xyz> 0 1 0 </xyz> <limit> <lower> -1e+16 </lower> <upper> 1e+16 </upper> </limit> <dynamics> <spring_reference> 0 </spring_reference> <spring_stiffness> 0 </spring_stiffness> </dynamics> </axis> </joint> <link name= 'front_left_wheel' > <pose relative_to= 'front_left_wheel_joint' > 0 0 0 0 -0 0 </pose> <inertial> <pose> 0 0 0 0 -0 0 </pose> <mass> 2.5 </mass> <inertia> <ixx> 0.00108333 </ixx> <ixy> 0 </ixy> <ixz> 0 </ixz> <iyy> 0.00108333 </iyy> <iyz> 0 </iyz> <izz> 0.002 </izz> </inertia> </inertial> <collision name= 'front_left_wheel_collision' > <pose> 0 0 0 1.5708 -0 0 </pose> <geometry> <cylinder> <length> 0.02 </length> <radius> 0.04 </radius> </cylinder> </geometry> <surface> <contact> <ode> <kp> 1e+07 </kp> <kd> 1 </kd> </ode> </contact> <friction> <ode> <mu> 1 </mu> <mu2> 1 </mu2> <fdir1> 1 0 0 </fdir1> </ode> </friction> </surface> </collision> <visual name= 'front_left_wheel_visual' > <pose> 0 0 0 1.5708 -0 0 </pose> <geometry> <cylinder> <length> 0.02 </length> <radius> 0.04 </radius> </cylinder> </geometry> <material> <script> <name> Gazebo/Grey </name> <uri> file://media/materials/scripts/gazebo.material </uri> </script> </material> </visual> <gravity> 1 </gravity> <velocity_decay/> </link> <joint name= 'front_right_wheel_joint' type= 'revolute' > <pose relative_to= 'base_footprint' > 0.105 0.085 0.04 0 -0 0 </pose> <parent> base_footprint </parent> <child> front_right_wheel </child> <axis> <xyz> 0 1 0 </xyz> <limit> <lower> -1e+16 </lower> <upper> 1e+16 </upper> </limit> <dynamics> <spring_reference> 0 </spring_reference> <spring_stiffness> 0 </spring_stiffness> </dynamics> </axis> </joint> <link name= 'front_right_wheel' > <pose relative_to= 'front_right_wheel_joint' > 0 0 0 0 -0 0 </pose> <inertial> <pose> 0 0 0 0 -0 0 </pose> <mass> 2.5 </mass> <inertia> <ixx> 0.00108333 </ixx> <ixy> 0 </ixy> <ixz> 0 </ixz> <iyy> 0.00108333 </iyy> <iyz> 0 </iyz> <izz> 0.002 </izz> </inertia> </inertial> <collision name= 'front_right_wheel_collision' > <pose> 0 0 0 1.5708 -0 0 </pose> <geometry> <cylinder> <length> 0.02 </length> <radius> 0.04 </radius> </cylinder> </geometry> <surface> <contact> <ode> <kp> 1e+07 </kp> <kd> 1 </kd> </ode> </contact> <friction> <ode> <mu> 1 </mu> <mu2> 1 </mu2> <fdir1> 1 0 0 </fdir1> </ode> </friction> </surface> </collision> <visual name= 'front_right_wheel_visual' > <pose> 0 0 0 1.5708 -0 0 </pose> <geometry> <cylinder> <length> 0.02 </length> <radius> 0.04 </radius> </cylinder> </geometry> <material> <script> <name> Gazebo/Grey </name> <uri> file://media/materials/scripts/gazebo.material </uri> </script> </material> </visual> <gravity> 1 </gravity> <velocity_decay/> </link> <plugin name= 'gazebo_ros_control' filename= 'libgazebo_ros_control.so' > <robotNamespace> /diffbot </robotNamespace> <robotSimType> gazebo_ros_control/DefaultRobotHWSim </robotSimType> </plugin> <static> 0 </static> <plugin name= 'differential_drive_controller' filename= 'libgazebo_ros_diff_drive.so' > <legacyMode> 1 </legacyMode> <rosDebugLevel> Debug </rosDebugLevel> <publishWheelTF> 0 </publishWheelTF> <robotNamespace> / </robotNamespace> <publishTf> 1 </publishTf> <publishWheelJointState> 0 </publishWheelJointState> <alwaysOn> 1 </alwaysOn> <updateRate> 100.0 </updateRate> <leftJoint> front_left_wheel_joint </leftJoint> <rightJoint> front_right_wheel_joint </rightJoint> <wheelSeparation> 0.3 </wheelSeparation> <wheelDiameter> 0.08 </wheelDiameter> <broadcastTF> 1 </broadcastTF> <wheelTorque> 30 </wheelTorque> <wheelAcceleration> 1.8 </wheelAcceleration> <commandTopic> cmd_vel </commandTopic> <odometryFrame> odom </odometryFrame> <odometryTopic> odom </odometryTopic> <robotBaseFrame> base_footprint </robotBaseFrame> </plugin> </model> </sdf> In case the output looks like the following, there are most certainly missing <inertial> tags in the <link> tag. For the Gazebo simulator the <inertial> must be present, in order to simulate the dynamics of the robot. See also http://wiki.ros.org/urdf/XML/link and the Gazebo tutorials on URDF . 1 2 3 <sdf version= '1.7' > <model name= 'diffbot' /> </sdf>","title":"Troubleshooting"},{"location":"diffbot_mbf/","text":"DiffBot Move Base Flex As described in the move_base_flex ROS wiki : Move Base Flex (MBF) is a backwards-compatible replacement for move_base. MBF can use existing plugins for move_base, and provides an enhanced version of the planner, controller and recovery plugin ROS interfaces. It exposes action servers for planning, controlling and recovering, providing detailed information of the current state and the plugin\u2019s feedback. An external executive logic can use MBF and its actions to perform smart and flexible navigation strategies. Furthermore, MBF enables the use of other map representations, e.g. meshes or grid_map This package is a meta package and refers to the Move Base Flex stack packages.The abstract core of MBF \u2013 without any binding to a map representation \u2013 is represented by the mbf_abstract_nav and the mbf_abstract_core . For navigation on costmaps see mbf_costmap_nav and mbf_costmap_core . This diffbot_mbf package was created using catkin-tools using the following command: 1 2 3 4 5 catkin create pkg diffbot_mbf Creating package \"diffbot_mbf\" in \"/home/fjp/git/ros_ws/src/diffbot\"... Created file diffbot_mbf/package.xml Created file diffbot_mbf/CMakeLists.txt Successfully created package files in /home/fjp/git/ros_ws/src/diffbot/diffbot_mbf. Additionally the following Ubuntu packages are required dependencies of move_base_flex : 1 sudo apt install ros-noetic-mbf-costmap-nav Another working example for turtlebot3 can be found in the turtlebot3_mbf package.","title":"Move Base Flex"},{"location":"diffbot_mbf/#diffbot-move-base-flex","text":"As described in the move_base_flex ROS wiki : Move Base Flex (MBF) is a backwards-compatible replacement for move_base. MBF can use existing plugins for move_base, and provides an enhanced version of the planner, controller and recovery plugin ROS interfaces. It exposes action servers for planning, controlling and recovering, providing detailed information of the current state and the plugin\u2019s feedback. An external executive logic can use MBF and its actions to perform smart and flexible navigation strategies. Furthermore, MBF enables the use of other map representations, e.g. meshes or grid_map This package is a meta package and refers to the Move Base Flex stack packages.The abstract core of MBF \u2013 without any binding to a map representation \u2013 is represented by the mbf_abstract_nav and the mbf_abstract_core . For navigation on costmaps see mbf_costmap_nav and mbf_costmap_core . This diffbot_mbf package was created using catkin-tools using the following command: 1 2 3 4 5 catkin create pkg diffbot_mbf Creating package \"diffbot_mbf\" in \"/home/fjp/git/ros_ws/src/diffbot\"... Created file diffbot_mbf/package.xml Created file diffbot_mbf/CMakeLists.txt Successfully created package files in /home/fjp/git/ros_ws/src/diffbot/diffbot_mbf. Additionally the following Ubuntu packages are required dependencies of move_base_flex : 1 sudo apt install ros-noetic-mbf-costmap-nav Another working example for turtlebot3 can be found in the turtlebot3_mbf package.","title":"DiffBot Move Base Flex"},{"location":"diffbot_msgs/","text":"DiffBot Messages Package As mentioned before, the nodes in ROS communicate with each other by publishing messages to topics . ROS provides the std_msgs package that includes ROS' common message types to represent primitive data types (see the ROS msg specification for primitive types) and other basic message constructs, such as multiarrays. Note howerver, the following from the std_msgs documentation : Quote The types in std_msgs do not convey semantic meaning about their contents: every message simply has a field called \"data\". Therefore, while the messages in this package can be useful for quick prototyping, they are NOT intended for \"long-term\" usage. For ease of documentation and collaboration, we recommend that existing messages be used, or new messages created, that provide meaningful field name(s). Diffbot Messages Therefore, we create a package that contains message definitions specific to DiffBot. The following command uses catkin-tools to create the diffbot_msgs package: 1 2 3 4 5 6 ros_ws/src$ catkin create pkg diffbot_msgs --catkin-deps message_generation std_msgs Creating package \"diffbot_msgs\" in \"/home/fjp/git/ros_ws/src\"... WARNING: Packages with messages or services should depend on both message_generation and message_runtime Created file diffbot_msgs/package.xml Created file diffbot_msgs/CMakeLists.txt Successfully created package files in /home/fjp/git/ros_ws/src/diffbot_msgs. Note The following is based on ROS Tutorials Creating Msg And Srv . In this tutorial you can find the required configurations for the package.xml and CMakeLists.txt . Encoders Currently there is no encoder message definition in ROS (see the sensor_msgs package) which is why a dedicated message is created for the encoders. For this, a simple msg description file, named Encoders.msg is created in the msg/ subdirectory of this diffbot_msgs package: 1 2 3 4 5 6 7 8 # This is a message to hold number of ticks from Encoders Header header # Use an array of size two of type int32 for the two encoders. # int32 is used instead of int64 because it is not supporte by Arduino/Teensy. # An overflow is also unlikely with the encoders of the DG01D-E # motor with its encoder because of its low encoder resolution int32[2] ticks The message includes the message type Header (see also Header msg ) which includes common metadata fileds such as timestamp that is automatically set by ROS client libraries . Having this encoder message description gives semantic meaning to the encoder messages and for example avoids having two separate int32 publishers for each encoder. Combining the encoder message into a single one alleviates additional timing problems. There exists also the common_msgs meta package for common, generic robot-specific message types. From the common_msgs DiffBot uses for example the nav_msgs for navigation with the navigation stack . Other relevant message definitions are the sensor_msgs/Imu and sensor_msgs/LaserScan , where both are definitions from the sensor_msgs package. Wheel Commands To command a joint velocity for each wheel diffbot_msgs provides the WheelCmd.msg . This specifies the Header and a float64 array for the angular wheel joint velocities. 1 2 3 4 5 6 # This is a message that holds commanded angular joint velocity Header header # Use an array of type float32 for the two wheel joint velocities. # float32 is used instead of float64 because it is not supporte by Arduino/Teensy. float32[] velocities Using rosmsg After building the package and its messages using catkin build let's make sure that ROS can see it using the rosmsg show command. 1 2 3 4 5 6 $ rosmsg show diffbot_msgs/Encoders std_msgs/Header header uint32 seq time stamp string frame_id int32[2] encoders Tip When using the a ros command such as rosmsg make use of the Tab key to auto complete the message name. ROSSerial The generated messages in this packages are used on the Teensy microcontroller, which is using rosserial . Integrating these messages requires the following steps. Generate rosserial libraries in a temporary folder using the make_libraries script: 1 rosrun rosserial_client make_libraries ~/Arduino/tmp/ This will generate all messages for ALL installed packages, but in our case only the diffbot_msgs package is needed to avoid missing includes. Copy the generated ~/Arduino/tmp/diffbot_msgs message folder to the src folder of the rosserial Arduino library. When rosserial was installed with the Arduino Library Manager, the location is ~/Arduino/libraries/Rosserial_Arduino_Library/ . Usage The new messages, specific to DiffBot, can be used by including the generated header, for example #include <diffbot_msgs/Encoders.h> . References Tutorials Arduino IDE Setup , specifically Install ros_lib into the Arduino Environment rosserial limitations : float64 is not supported on Arduino.","title":"Messages"},{"location":"diffbot_msgs/#diffbot-messages-package","text":"As mentioned before, the nodes in ROS communicate with each other by publishing messages to topics . ROS provides the std_msgs package that includes ROS' common message types to represent primitive data types (see the ROS msg specification for primitive types) and other basic message constructs, such as multiarrays. Note howerver, the following from the std_msgs documentation : Quote The types in std_msgs do not convey semantic meaning about their contents: every message simply has a field called \"data\". Therefore, while the messages in this package can be useful for quick prototyping, they are NOT intended for \"long-term\" usage. For ease of documentation and collaboration, we recommend that existing messages be used, or new messages created, that provide meaningful field name(s).","title":"DiffBot Messages Package"},{"location":"diffbot_msgs/#diffbot-messages","text":"Therefore, we create a package that contains message definitions specific to DiffBot. The following command uses catkin-tools to create the diffbot_msgs package: 1 2 3 4 5 6 ros_ws/src$ catkin create pkg diffbot_msgs --catkin-deps message_generation std_msgs Creating package \"diffbot_msgs\" in \"/home/fjp/git/ros_ws/src\"... WARNING: Packages with messages or services should depend on both message_generation and message_runtime Created file diffbot_msgs/package.xml Created file diffbot_msgs/CMakeLists.txt Successfully created package files in /home/fjp/git/ros_ws/src/diffbot_msgs. Note The following is based on ROS Tutorials Creating Msg And Srv . In this tutorial you can find the required configurations for the package.xml and CMakeLists.txt .","title":"Diffbot Messages"},{"location":"diffbot_msgs/#encoders","text":"Currently there is no encoder message definition in ROS (see the sensor_msgs package) which is why a dedicated message is created for the encoders. For this, a simple msg description file, named Encoders.msg is created in the msg/ subdirectory of this diffbot_msgs package: 1 2 3 4 5 6 7 8 # This is a message to hold number of ticks from Encoders Header header # Use an array of size two of type int32 for the two encoders. # int32 is used instead of int64 because it is not supporte by Arduino/Teensy. # An overflow is also unlikely with the encoders of the DG01D-E # motor with its encoder because of its low encoder resolution int32[2] ticks The message includes the message type Header (see also Header msg ) which includes common metadata fileds such as timestamp that is automatically set by ROS client libraries . Having this encoder message description gives semantic meaning to the encoder messages and for example avoids having two separate int32 publishers for each encoder. Combining the encoder message into a single one alleviates additional timing problems. There exists also the common_msgs meta package for common, generic robot-specific message types. From the common_msgs DiffBot uses for example the nav_msgs for navigation with the navigation stack . Other relevant message definitions are the sensor_msgs/Imu and sensor_msgs/LaserScan , where both are definitions from the sensor_msgs package.","title":"Encoders"},{"location":"diffbot_msgs/#wheel-commands","text":"To command a joint velocity for each wheel diffbot_msgs provides the WheelCmd.msg . This specifies the Header and a float64 array for the angular wheel joint velocities. 1 2 3 4 5 6 # This is a message that holds commanded angular joint velocity Header header # Use an array of type float32 for the two wheel joint velocities. # float32 is used instead of float64 because it is not supporte by Arduino/Teensy. float32[] velocities","title":"Wheel Commands"},{"location":"diffbot_msgs/#using-rosmsg","text":"After building the package and its messages using catkin build let's make sure that ROS can see it using the rosmsg show command. 1 2 3 4 5 6 $ rosmsg show diffbot_msgs/Encoders std_msgs/Header header uint32 seq time stamp string frame_id int32[2] encoders Tip When using the a ros command such as rosmsg make use of the Tab key to auto complete the message name.","title":"Using rosmsg"},{"location":"diffbot_msgs/#rosserial","text":"The generated messages in this packages are used on the Teensy microcontroller, which is using rosserial . Integrating these messages requires the following steps. Generate rosserial libraries in a temporary folder using the make_libraries script: 1 rosrun rosserial_client make_libraries ~/Arduino/tmp/ This will generate all messages for ALL installed packages, but in our case only the diffbot_msgs package is needed to avoid missing includes. Copy the generated ~/Arduino/tmp/diffbot_msgs message folder to the src folder of the rosserial Arduino library. When rosserial was installed with the Arduino Library Manager, the location is ~/Arduino/libraries/Rosserial_Arduino_Library/ .","title":"ROSSerial"},{"location":"diffbot_msgs/#usage","text":"The new messages, specific to DiffBot, can be used by including the generated header, for example #include <diffbot_msgs/Encoders.h> .","title":"Usage"},{"location":"diffbot_msgs/#references","text":"Tutorials Arduino IDE Setup , specifically Install ros_lib into the Arduino Environment rosserial limitations : float64 is not supported on Arduino.","title":"References"},{"location":"diffbot_navigation/","text":"DiffBot Navigation Package Navigation Stack Overview. 1 2 3 4 5 fjp@diffbot:~/catkin_ws/src/diffbot$ catkin create pkg diffbot_navigation --catkin-deps amcl map_server move_base diffbot_bringup Creating package \"diffbot_navigation\" in \"/home/fjp/git/ros_ws/src/diffbot\"... Created file diffbot_navigation/package.xml Created file diffbot_navigation/CMakeLists.txt Successfully created package files in /home/fjp/git/ros_ws/src/diffbot/diffbot_navigation. We also need the following ROS packages that can be installed from the ROS Ubuntu packages: 1 $ sudo apt install ros-noetic-dwa-local-planner ros-noetic-amcl ros-noetic-map-server ros-noetic-move-base After this we create the required launch files and parameter configurations. These will be used for the simulation and the real robot. First we focus on the simulation in Gazebo. Launch files All launch files are in the folder named launch of the diffbot_navigation package. Inside the move_base.launch it is important to remap the following topics: 1 2 3 4 5 6 7 <!-- Arguments --> <arg name= \"cmd_vel_topic\" default= \"/diffbot/mobile_base_controller/cmd_vel\" /> <arg name= \"odom_topic\" default= \"/diffbot/mobile_base_controller/odom\" /> ... <!-- remappings of move_base node --> <remap from= \"cmd_vel\" to= \"$(arg cmd_vel_topic)\" /> <remap from= \"odom\" to= \"$(arg odom_topic)\" /> Parameter Configuration The parameters for the navigation package go into the config (for some robots named param ) folder. Most of them can be changed during runtime using dynamic reconfigure with the rqt_reconfigure gui . Setup and Configuration of the Navigation Stack on a Robot amcl : amcl is a probabilistic localization system for a robot moving in 2D. It implements the adaptive (or KLD-sampling) Monte Carlo localization approach (as described by Dieter Fox), which uses a particle filter to track the pose of a robot against a known map. map_server : provides the map_server ROS Node, which offers map data as a ROS Service. It also provides the map_saver command-line utility, which allows dynamically generated maps to be saved to file. move_base : The move_base package provides an implementation of an action (see the actionlib package) that, given a goal in the world, will attempt to reach it with a mobile base. The move_base node links together a global and local planner to accomplish its global navigation task. It supports any global planner adhering to the nav_core::BaseGlobalPlanner interface specified in the nav_core package and any local planner adhering to the nav_core::BaseLocalPlanner interface specified in the nav_core package. The move_base node also maintains two costmaps, one for the global planner, and one for a local planner (see the costmap_2d package) that are used to accomplish navigation tasks. gmapping : This package contains a ROS wrapper for OpenSlam's Gmapping. The gmapping package provides laser-based SLAM (Simultaneous Localization and Mapping), as a ROS node called slam_gmapping. Using slam_gmapping, you can create a 2-D occupancy grid map (like a building floorplan) from laser and pose data collected by a mobile robot. ROS cartographer slam_toolbox Examples - TurtleBot3 Navigation Navigation in Gazebo with available Map To navigate the robot in the simulation run the following command but make sure to first download the turtlebot3_world to your ~/.gazebo/models/ folder. This is required because the turtlebot3_world.world file references the turtlebot3_world model. 1 roslaunch diffbot_navigation diffbot.launch world_name:='$(find diffbot_gazebo)/worlds/turtlebot3_world.world' This will spawn DiffBot inside the turtlebot3 world inside Gazebo and visualize the elements of the navigation stack in RViz. Navigation demo of DiffBot (click to view the demo on Youtube). To navigate the robot using the default DWA planner in the known map, coming from the running map_server , you can use the 2D Nav Goal in RViz . Just select the navigation arrow to where the robot should move as shown in the animation above. Note The DWA local planner is working for differential drive robots, like DiffBot. For other robots such as non-holonomic robots or other types of mobile robots (also differential drive robots) other planners can be used. See for example teb_local_planner . Resources Global Planners: - global_planner Local Planners: - Difference between DWA and Base Local Planner - Difference between DWA and TEB Local Planner","title":"Navigation"},{"location":"diffbot_navigation/#diffbot-navigation-package","text":"Navigation Stack Overview. 1 2 3 4 5 fjp@diffbot:~/catkin_ws/src/diffbot$ catkin create pkg diffbot_navigation --catkin-deps amcl map_server move_base diffbot_bringup Creating package \"diffbot_navigation\" in \"/home/fjp/git/ros_ws/src/diffbot\"... Created file diffbot_navigation/package.xml Created file diffbot_navigation/CMakeLists.txt Successfully created package files in /home/fjp/git/ros_ws/src/diffbot/diffbot_navigation. We also need the following ROS packages that can be installed from the ROS Ubuntu packages: 1 $ sudo apt install ros-noetic-dwa-local-planner ros-noetic-amcl ros-noetic-map-server ros-noetic-move-base After this we create the required launch files and parameter configurations. These will be used for the simulation and the real robot. First we focus on the simulation in Gazebo.","title":"DiffBot Navigation Package"},{"location":"diffbot_navigation/#launch-files","text":"All launch files are in the folder named launch of the diffbot_navigation package. Inside the move_base.launch it is important to remap the following topics: 1 2 3 4 5 6 7 <!-- Arguments --> <arg name= \"cmd_vel_topic\" default= \"/diffbot/mobile_base_controller/cmd_vel\" /> <arg name= \"odom_topic\" default= \"/diffbot/mobile_base_controller/odom\" /> ... <!-- remappings of move_base node --> <remap from= \"cmd_vel\" to= \"$(arg cmd_vel_topic)\" /> <remap from= \"odom\" to= \"$(arg odom_topic)\" />","title":"Launch files"},{"location":"diffbot_navigation/#parameter-configuration","text":"The parameters for the navigation package go into the config (for some robots named param ) folder. Most of them can be changed during runtime using dynamic reconfigure with the rqt_reconfigure gui . Setup and Configuration of the Navigation Stack on a Robot amcl : amcl is a probabilistic localization system for a robot moving in 2D. It implements the adaptive (or KLD-sampling) Monte Carlo localization approach (as described by Dieter Fox), which uses a particle filter to track the pose of a robot against a known map. map_server : provides the map_server ROS Node, which offers map data as a ROS Service. It also provides the map_saver command-line utility, which allows dynamically generated maps to be saved to file. move_base : The move_base package provides an implementation of an action (see the actionlib package) that, given a goal in the world, will attempt to reach it with a mobile base. The move_base node links together a global and local planner to accomplish its global navigation task. It supports any global planner adhering to the nav_core::BaseGlobalPlanner interface specified in the nav_core package and any local planner adhering to the nav_core::BaseLocalPlanner interface specified in the nav_core package. The move_base node also maintains two costmaps, one for the global planner, and one for a local planner (see the costmap_2d package) that are used to accomplish navigation tasks. gmapping : This package contains a ROS wrapper for OpenSlam's Gmapping. The gmapping package provides laser-based SLAM (Simultaneous Localization and Mapping), as a ROS node called slam_gmapping. Using slam_gmapping, you can create a 2-D occupancy grid map (like a building floorplan) from laser and pose data collected by a mobile robot. ROS cartographer slam_toolbox Examples - TurtleBot3 Navigation","title":"Parameter Configuration"},{"location":"diffbot_navigation/#navigation-in-gazebo-with-available-map","text":"To navigate the robot in the simulation run the following command but make sure to first download the turtlebot3_world to your ~/.gazebo/models/ folder. This is required because the turtlebot3_world.world file references the turtlebot3_world model. 1 roslaunch diffbot_navigation diffbot.launch world_name:='$(find diffbot_gazebo)/worlds/turtlebot3_world.world' This will spawn DiffBot inside the turtlebot3 world inside Gazebo and visualize the elements of the navigation stack in RViz. Navigation demo of DiffBot (click to view the demo on Youtube). To navigate the robot using the default DWA planner in the known map, coming from the running map_server , you can use the 2D Nav Goal in RViz . Just select the navigation arrow to where the robot should move as shown in the animation above. Note The DWA local planner is working for differential drive robots, like DiffBot. For other robots such as non-holonomic robots or other types of mobile robots (also differential drive robots) other planners can be used. See for example teb_local_planner .","title":"Navigation in Gazebo with available Map"},{"location":"diffbot_navigation/#resources","text":"Global Planners: - global_planner Local Planners: - Difference between DWA and Base Local Planner - Difference between DWA and TEB Local Planner","title":"Resources"},{"location":"diffbot_perception/","text":"Perception To provide perception capabilities to your mobile robot you need a sensor like a RGB or RGB-D camera as hardware and software like OpenCV or PCL . Quote OpenCV (Open Source Computer Vision Library) is a library of programming functions mainly aimed at real-time computer vision. Originally developed by Intel, it was later supported by Willow Garage then Itseez (which was later acquired by Intel). The library is cross-platform and free for use under the open-source Apache 2 License. Starting with 2011, OpenCV features GPU acceleration for real-time operations. Quote The Point Cloud Library (PCL) is a standalone, large scale, open project for 2D/3D image and point cloud processing. PCL is released under the terms of the BSD license, and thus free for commercial and research use. When working with OpenCV and ROS we have to install required dependency using ros-noetic-vision-opencv . This will install additional dependencies like OpenCV ( libopencv-dv ) and ros-noetic-cv-bridge . There is no need to install OpenCV from source or the Ubuntu binaries (deb package). If you want to use OpenCV without ROS you should consider installing it from source using CMake . This will allow you to configure what features should be installed, e.g., example code. Otherwise you can install a pre compiled binary libopencv-dev . Similar to work with PCL and ROS you need to install the dependency ros-noetic-perception-pcl , which is the interface between ROS and PCL. To work with PCL without ROS you should consider installing it from source using CMake . This will allow you to configure what features should be installed, e.g., example code. Otherwise you can install a pre compiled binary libpcl-dev . perception_pcl is a meta package for PCL (Point Cloud Library) ROS interface stack. PCL-ROS is the preferred bridge for 3D applications involving n-D Point Clouds and 3D geometry processing in ROS.","title":"Diffbot perception"},{"location":"diffbot_perception/#perception","text":"To provide perception capabilities to your mobile robot you need a sensor like a RGB or RGB-D camera as hardware and software like OpenCV or PCL . Quote OpenCV (Open Source Computer Vision Library) is a library of programming functions mainly aimed at real-time computer vision. Originally developed by Intel, it was later supported by Willow Garage then Itseez (which was later acquired by Intel). The library is cross-platform and free for use under the open-source Apache 2 License. Starting with 2011, OpenCV features GPU acceleration for real-time operations. Quote The Point Cloud Library (PCL) is a standalone, large scale, open project for 2D/3D image and point cloud processing. PCL is released under the terms of the BSD license, and thus free for commercial and research use. When working with OpenCV and ROS we have to install required dependency using ros-noetic-vision-opencv . This will install additional dependencies like OpenCV ( libopencv-dv ) and ros-noetic-cv-bridge . There is no need to install OpenCV from source or the Ubuntu binaries (deb package). If you want to use OpenCV without ROS you should consider installing it from source using CMake . This will allow you to configure what features should be installed, e.g., example code. Otherwise you can install a pre compiled binary libopencv-dev . Similar to work with PCL and ROS you need to install the dependency ros-noetic-perception-pcl , which is the interface between ROS and PCL. To work with PCL without ROS you should consider installing it from source using CMake . This will allow you to configure what features should be installed, e.g., example code. Otherwise you can install a pre compiled binary libpcl-dev . perception_pcl is a meta package for PCL (Point Cloud Library) ROS interface stack. PCL-ROS is the preferred bridge for 3D applications involving n-D Point Clouds and 3D geometry processing in ROS.","title":"Perception"},{"location":"diffbot_robot/","text":"DiffBot Robot The diffbot_robot package is a ROS metapackage that references all related packages to DiffBot. A metapackage can be created with: 1 2 3 4 fjp@ubuntu:ros_ws/src/$ catkin_create_pkg diffbot_robot --meta Created file diffbot_robot/package.xml Created file diffbot_robot/CMakeLists.txt Successfully created files in /home/fjp/git/ros_ws/src/diffbot/diffbot_robot. Please adjust the values in package.xml. To release a package see the bloom page and the listed tutorials there. Specifically the following ones: To index the package follow the Indexing Your ROS Repository for Documentation Generation . Release a package using bloom, see First Time Release tutorial .","title":"Robot Package"},{"location":"diffbot_robot/#diffbot-robot","text":"The diffbot_robot package is a ROS metapackage that references all related packages to DiffBot. A metapackage can be created with: 1 2 3 4 fjp@ubuntu:ros_ws/src/$ catkin_create_pkg diffbot_robot --meta Created file diffbot_robot/package.xml Created file diffbot_robot/CMakeLists.txt Successfully created files in /home/fjp/git/ros_ws/src/diffbot/diffbot_robot. Please adjust the values in package.xml. To release a package see the bloom page and the listed tutorials there. Specifically the following ones: To index the package follow the Indexing Your ROS Repository for Documentation Generation . Release a package using bloom, see First Time Release tutorial .","title":"DiffBot Robot"},{"location":"diffbot_slam/","text":"DiffBot Slam Package This package contains launch files and configurations for different simultaneous localization and mapping (SLAM) algorithms to map the environment of the robot in 2D, although some of these algorithms can be used to map in 3D. 1 2 3 4 5 fjp@diffbot:~/catkin_ws/src/diffbot$ catkin create pkg diffbot_slam --catkin-deps diffbot_navigation gmapping Creating package \"diffbot_slam\" in \"/home/fjp/git/ros_ws/src/diffbot\"... Created file diffbot_slam/package.xml Created file diffbot_slam/CMakeLists.txt Successfully created package files in /home/fjp/git/ros_ws/src/diffbot/diffbot_slam. Additional runtime dependencies are: cartographer_ros , hector_slam , frontier_exploration and explore_lite . These are added to this workspace using vcstool (TODO). As you can see this package has lots of dependencies to test different slam implementations and frontier exploration approaches. To run this package these dependencies need to be installed and are set as exec_depend in the package.xml . Currently only gmapping provides a ROS Noetic Ubuntu package that can be installed directly with: 1 sudo apt install ros-noetic-gmapping In case you want to try more advanced SLAM algorithms, such as karto_slam or cartographer_ros you need the following Ubuntu package dependencies. Alternatively you can install from source by building the cloned git repository in your catkin workspace. Take the required installation size into account. For example karto_slam needs approximately 125MB because it will also install ros-noetic-open-karto . 1 sudo apt install ros-noetic-slam-karto SLAM SLAM stands for Simultaneous Localization and Mapping sometimes refered to as Concurrent Localization and Mappping (CLAM). The SLAM algorithm combines localization and mapping, where a robot has access only to its own movement and sensory data. The robot must build a map while simultaneously localizing itself relative to the map. See also this blog post on FastSLAM . To use the following slam algorithms, we need a mobile robot that provides odometry data and is equipped with a horizontally-mounted, fixed, laser range-finder. Viewed on a higher level, every specific slam node of these algorithms will attempt to transform each incoming scan into the odom (odometry) tf frame. Therefore the node will subscribe to the laser /scan and the /tf topics. Transforms are necessary to relate frames for laser, base, and odometry. The only exception is hector_slam which doesn't require odometry for mapping. The following SLAM implementations are offered using the launch files explained in the next section. It is suggested to start with gmapping which is used by default. gmapping : This package contains a ROS wrapper for OpenSlam's Gmapping . The gmapping package provides laser-based SLAM (Simultaneous Localization and Mapping), as a ROS node called slam_gmapping . Using slam_gmapping , you can create a 2-D occupancy grid map (like a building floorplan) from laser and pose data collected by a mobile robot. cartographer : Cartographer is a system that provides real-time simultaneous localization and mapping (SLAM) in 2D and 3D across multiple platforms and sensor configurations. See the documentation for an algorithm walkthrough . karto : This package pulls in the Karto mapping library, and provides a ROS wrapper for using it. Karto is considered more accurate than, for example gmapping (note: for ROS noetic, see slam_karto ) and became open source in 2010 . hector_slam : metapackage that installs hector_mapping and related packages. The hector_mapping is a SLAM approach that can be used without odometry as well as on platforms that exhibit roll/pitch motion (of the sensor, the platform or both), such as drones. It leverages the high update rate of modern LIDAR systems like the Hokuyo UTM-30LX and provides 2D pose estimates at scan rate of the sensors (40Hz for the UTM-30LX). While the system does not provide explicit loop closing ability, it is sufficiently accurate for many real world scenarios. The system has successfully been used on Unmanned Ground Robots, Unmanned Surface Vehicles, Handheld Mapping Devices and logged data from quadrotor UAVs. Unlike gmapping which uses a particle filter , karto , cartographer and hector_slam are all graph-based SLAM algorithms . The least accurate SLAM algorithm is gmapping but it works fine for smaller maps. Use other algorithms, such as karto if you operate your robot in larger environments or you want more accuracy. Another interesing package is slam_toolbox which provides ROS1 and ROS2 support and is based on the easy to use karto algorithm. karto is the basis for many companies because it provides an excellent scan matcher and can operate in large environments. Additionally, slam_toolbox provides tools to edit a generated map and even create a high quality map using stored data (offline). The cartographer package is currently supported by OpenRobotics and not by Google where it was originally developed. It is currently also not setup correctly for DiffBot. Using it will result in errors. Launch files This package provides a main launch file named diffbot_slam.launch which accepts an argument slam_method . Depending on its value, different launch files will be included that execute the specified SLAM algorithm using its configuration in the config folder. As mentioned above, every ROS slam package requries messages from the laser-range finder topic. Usually this topic is named /scan . To distinguish possible multiple lidars, the topic of DiffBot resides in its namespace /diffbot/scan . Therefore, its necessary to remap the /scan topic to /diffbot/scan . The following shows how this was done for the gmapping launch file. Inside this package in the launch/gmapping.launch it is important to map the scan topic to laser scanner topic published by Diffbot. Remappings are done in the node tag . Here, for the gmapping.launch in the gmapping node: 1 2 3 4 5 6 7 8 9 10 11 <launch> <!-- Arguments --> <arg name= \"scan_topic\" default= \"diffbot/scan\" /> ... <!-- Gmapping --> <node pkg= \"gmapping\" type= \"slam_gmapping\" name= \"diffbot_slam_gmapping\" output= \"screen\" > ... <!-- remapping of gmapping node --> <remap from= \"scan\" to= \"$(arg scan_topic)\" /> </node> </launch> Parameter Configurations Most of the configrations are the same as turtlebot3_slam/config . For detailed description of what each parameter does, please check the individual package documentation of the different SLAM methods. Gazebo Simulation Tests To test SLAM in the Gazebo simulator run the following two launch files in separate terminals. First run the simulation with: 1 roslaunch diffbot_gazebo diffbot.launch world_name:='$(find diffbot_gazebo)/worlds/turtlebot3_world.world' and in a second terminal execute the SLAM algorithm: 1 roslaunch diffbot_slam diffbot_slam.launch slam_method:=gmapping Here you can choose between different algorithms by changing the value of the slam_method argument. Possible values are gmapping (the default), karto , hector and cartographer . The ROS node graph will look like the following: ROS Node graph after launching Gazebo and gmapping. In the figure we can see that gmapping subscribes and publishes to tf . It requires the transformation from <the frame attached to incoming scans> to the base_link , which is usually a fixed value, broadcast periodically by the robot_state_publisher . Aditionally, it requires the transform from base_link to odom . This is provided by the odometry system (e.g., the driver for the mobile base). In the case of DiffBot the odometry system consists of EKF fusion data from the motor encoders and the IMU. The provided tf transforms are map to odom that describes the current estimate of the robot's pose within the map frame. You can read more about the required and provided transforms in the documentation. Field Tests In case you get inaccurate maps follow the official ROS troubleshooting guide for navigation . Frontier Exploration The so far described mapping approaches require manually steering the robot in the unknown environment. Frontier exploration is an approach to move a mobile robot on its own to new frontiers to extend its map into new territory until the entire environment has been explored. The ROS wiki provides a good tutorial using Husky robot how to use the frontier_exploration package. A lightweight alternative is the explore_lite package. Other SLAM Packages (for 3D Mapping) hdl_graph_slam : Open source ROS package for real-time 6DOF SLAM using a 3D LIDAR. It is based on 3D Graph SLAM with NDT scan matching-based odometry estimation and loop detection. This method is useful for outdoor. RTAB-Map : stands for Real-Time Appearance-Based Mapping and is a RGB-D SLAM approach based on a global loop closure detector with real-time constraints. This package can be used to generate a 3D point clouds of the environment and/or to create a 2D occupancy grid map for navigation. To do this it requires only a stereo or RGB-D camera for visual odometry. Additional wheel odometry is not required but can improve the result. Loam Velodyne : Laser Odometry and Mapping (Loam) is a realtime method for state estimation and mapping using a 3D lidar, see also the forked Github repository for loam_velodyne . Note that this is not supported officially anymore because it became closed source. ORB-SLAM2 : Real-Time SLAM for Monocular, Stereo and RGB-D Cameras, with Loop Detection and Relocalization Capabilities. See orb_slam2_ros for the ROS wrapper. slam_toolbox : This package provides a sped up improved slam karto with updated SDK and visualization and modification toolsets. It is a ROS drop in replacement to gmapping, cartographer, karto, hector, etc. This package supports ROS1 and ROS2 and is suitable for use in commercial products because it can map large environments. And it provides tools to edit the generated maps. See also the related ROSCon 2019 video . References slam_toolbox , Slam Toolbox ROSCon 2019 pdf Papers: A Tutorial on Graph-Based SLAM cartographer Real-Time Loop Closure in 2D LIDAR SLAM hector_slam A flexible and scalable SLAM system with full 3D motion estimation . A practical introduction to to pose graph slam The Normal Distributions Transform: A New Approach to Laser Scan Matching RTAB-Map as an Open-Source Lidar and Visual SLAM Library for Large-Scale and Long-Term Online Operation LOAM: Lidar Odometry and Mapping in Real-time ORB-SLAM2: an Open-Source SLAM System for Monocular, Stereo and RGB-D Cameras","title":"SLAM"},{"location":"diffbot_slam/#diffbot-slam-package","text":"This package contains launch files and configurations for different simultaneous localization and mapping (SLAM) algorithms to map the environment of the robot in 2D, although some of these algorithms can be used to map in 3D. 1 2 3 4 5 fjp@diffbot:~/catkin_ws/src/diffbot$ catkin create pkg diffbot_slam --catkin-deps diffbot_navigation gmapping Creating package \"diffbot_slam\" in \"/home/fjp/git/ros_ws/src/diffbot\"... Created file diffbot_slam/package.xml Created file diffbot_slam/CMakeLists.txt Successfully created package files in /home/fjp/git/ros_ws/src/diffbot/diffbot_slam. Additional runtime dependencies are: cartographer_ros , hector_slam , frontier_exploration and explore_lite . These are added to this workspace using vcstool (TODO). As you can see this package has lots of dependencies to test different slam implementations and frontier exploration approaches. To run this package these dependencies need to be installed and are set as exec_depend in the package.xml . Currently only gmapping provides a ROS Noetic Ubuntu package that can be installed directly with: 1 sudo apt install ros-noetic-gmapping In case you want to try more advanced SLAM algorithms, such as karto_slam or cartographer_ros you need the following Ubuntu package dependencies. Alternatively you can install from source by building the cloned git repository in your catkin workspace. Take the required installation size into account. For example karto_slam needs approximately 125MB because it will also install ros-noetic-open-karto . 1 sudo apt install ros-noetic-slam-karto","title":"DiffBot Slam Package"},{"location":"diffbot_slam/#slam","text":"SLAM stands for Simultaneous Localization and Mapping sometimes refered to as Concurrent Localization and Mappping (CLAM). The SLAM algorithm combines localization and mapping, where a robot has access only to its own movement and sensory data. The robot must build a map while simultaneously localizing itself relative to the map. See also this blog post on FastSLAM . To use the following slam algorithms, we need a mobile robot that provides odometry data and is equipped with a horizontally-mounted, fixed, laser range-finder. Viewed on a higher level, every specific slam node of these algorithms will attempt to transform each incoming scan into the odom (odometry) tf frame. Therefore the node will subscribe to the laser /scan and the /tf topics. Transforms are necessary to relate frames for laser, base, and odometry. The only exception is hector_slam which doesn't require odometry for mapping. The following SLAM implementations are offered using the launch files explained in the next section. It is suggested to start with gmapping which is used by default. gmapping : This package contains a ROS wrapper for OpenSlam's Gmapping . The gmapping package provides laser-based SLAM (Simultaneous Localization and Mapping), as a ROS node called slam_gmapping . Using slam_gmapping , you can create a 2-D occupancy grid map (like a building floorplan) from laser and pose data collected by a mobile robot. cartographer : Cartographer is a system that provides real-time simultaneous localization and mapping (SLAM) in 2D and 3D across multiple platforms and sensor configurations. See the documentation for an algorithm walkthrough . karto : This package pulls in the Karto mapping library, and provides a ROS wrapper for using it. Karto is considered more accurate than, for example gmapping (note: for ROS noetic, see slam_karto ) and became open source in 2010 . hector_slam : metapackage that installs hector_mapping and related packages. The hector_mapping is a SLAM approach that can be used without odometry as well as on platforms that exhibit roll/pitch motion (of the sensor, the platform or both), such as drones. It leverages the high update rate of modern LIDAR systems like the Hokuyo UTM-30LX and provides 2D pose estimates at scan rate of the sensors (40Hz for the UTM-30LX). While the system does not provide explicit loop closing ability, it is sufficiently accurate for many real world scenarios. The system has successfully been used on Unmanned Ground Robots, Unmanned Surface Vehicles, Handheld Mapping Devices and logged data from quadrotor UAVs. Unlike gmapping which uses a particle filter , karto , cartographer and hector_slam are all graph-based SLAM algorithms . The least accurate SLAM algorithm is gmapping but it works fine for smaller maps. Use other algorithms, such as karto if you operate your robot in larger environments or you want more accuracy. Another interesing package is slam_toolbox which provides ROS1 and ROS2 support and is based on the easy to use karto algorithm. karto is the basis for many companies because it provides an excellent scan matcher and can operate in large environments. Additionally, slam_toolbox provides tools to edit a generated map and even create a high quality map using stored data (offline). The cartographer package is currently supported by OpenRobotics and not by Google where it was originally developed. It is currently also not setup correctly for DiffBot. Using it will result in errors.","title":"SLAM"},{"location":"diffbot_slam/#launch-files","text":"This package provides a main launch file named diffbot_slam.launch which accepts an argument slam_method . Depending on its value, different launch files will be included that execute the specified SLAM algorithm using its configuration in the config folder. As mentioned above, every ROS slam package requries messages from the laser-range finder topic. Usually this topic is named /scan . To distinguish possible multiple lidars, the topic of DiffBot resides in its namespace /diffbot/scan . Therefore, its necessary to remap the /scan topic to /diffbot/scan . The following shows how this was done for the gmapping launch file. Inside this package in the launch/gmapping.launch it is important to map the scan topic to laser scanner topic published by Diffbot. Remappings are done in the node tag . Here, for the gmapping.launch in the gmapping node: 1 2 3 4 5 6 7 8 9 10 11 <launch> <!-- Arguments --> <arg name= \"scan_topic\" default= \"diffbot/scan\" /> ... <!-- Gmapping --> <node pkg= \"gmapping\" type= \"slam_gmapping\" name= \"diffbot_slam_gmapping\" output= \"screen\" > ... <!-- remapping of gmapping node --> <remap from= \"scan\" to= \"$(arg scan_topic)\" /> </node> </launch>","title":"Launch files"},{"location":"diffbot_slam/#parameter-configurations","text":"Most of the configrations are the same as turtlebot3_slam/config . For detailed description of what each parameter does, please check the individual package documentation of the different SLAM methods.","title":"Parameter Configurations"},{"location":"diffbot_slam/#gazebo-simulation-tests","text":"To test SLAM in the Gazebo simulator run the following two launch files in separate terminals. First run the simulation with: 1 roslaunch diffbot_gazebo diffbot.launch world_name:='$(find diffbot_gazebo)/worlds/turtlebot3_world.world' and in a second terminal execute the SLAM algorithm: 1 roslaunch diffbot_slam diffbot_slam.launch slam_method:=gmapping Here you can choose between different algorithms by changing the value of the slam_method argument. Possible values are gmapping (the default), karto , hector and cartographer . The ROS node graph will look like the following: ROS Node graph after launching Gazebo and gmapping. In the figure we can see that gmapping subscribes and publishes to tf . It requires the transformation from <the frame attached to incoming scans> to the base_link , which is usually a fixed value, broadcast periodically by the robot_state_publisher . Aditionally, it requires the transform from base_link to odom . This is provided by the odometry system (e.g., the driver for the mobile base). In the case of DiffBot the odometry system consists of EKF fusion data from the motor encoders and the IMU. The provided tf transforms are map to odom that describes the current estimate of the robot's pose within the map frame. You can read more about the required and provided transforms in the documentation.","title":"Gazebo Simulation Tests"},{"location":"diffbot_slam/#field-tests","text":"In case you get inaccurate maps follow the official ROS troubleshooting guide for navigation .","title":"Field Tests"},{"location":"diffbot_slam/#frontier-exploration","text":"The so far described mapping approaches require manually steering the robot in the unknown environment. Frontier exploration is an approach to move a mobile robot on its own to new frontiers to extend its map into new territory until the entire environment has been explored. The ROS wiki provides a good tutorial using Husky robot how to use the frontier_exploration package. A lightweight alternative is the explore_lite package.","title":"Frontier Exploration"},{"location":"diffbot_slam/#other-slam-packages-for-3d-mapping","text":"hdl_graph_slam : Open source ROS package for real-time 6DOF SLAM using a 3D LIDAR. It is based on 3D Graph SLAM with NDT scan matching-based odometry estimation and loop detection. This method is useful for outdoor. RTAB-Map : stands for Real-Time Appearance-Based Mapping and is a RGB-D SLAM approach based on a global loop closure detector with real-time constraints. This package can be used to generate a 3D point clouds of the environment and/or to create a 2D occupancy grid map for navigation. To do this it requires only a stereo or RGB-D camera for visual odometry. Additional wheel odometry is not required but can improve the result. Loam Velodyne : Laser Odometry and Mapping (Loam) is a realtime method for state estimation and mapping using a 3D lidar, see also the forked Github repository for loam_velodyne . Note that this is not supported officially anymore because it became closed source. ORB-SLAM2 : Real-Time SLAM for Monocular, Stereo and RGB-D Cameras, with Loop Detection and Relocalization Capabilities. See orb_slam2_ros for the ROS wrapper. slam_toolbox : This package provides a sped up improved slam karto with updated SDK and visualization and modification toolsets. It is a ROS drop in replacement to gmapping, cartographer, karto, hector, etc. This package supports ROS1 and ROS2 and is suitable for use in commercial products because it can map large environments. And it provides tools to edit the generated maps. See also the related ROSCon 2019 video .","title":"Other SLAM Packages (for 3D Mapping)"},{"location":"diffbot_slam/#references","text":"slam_toolbox , Slam Toolbox ROSCon 2019 pdf Papers: A Tutorial on Graph-Based SLAM cartographer Real-Time Loop Closure in 2D LIDAR SLAM hector_slam A flexible and scalable SLAM system with full 3D motion estimation . A practical introduction to to pose graph slam The Normal Distributions Transform: A New Approach to Laser Scan Matching RTAB-Map as an Open-Source Lidar and Visual SLAM Library for Large-Scale and Long-Term Online Operation LOAM: Lidar Odometry and Mapping in Real-time ORB-SLAM2: an Open-Source SLAM System for Monocular, Stereo and RGB-D Cameras","title":"References"},{"location":"git-setup/","text":"git setup Install git on Ubuntu via the following command: fjp@ubuntu : ~/git/2wd-robot $ sudo apt install git Set your username and email address that you use on github (when using github to host your repository): fjp@ubuntu : ~ $ git config --global user.name \"fjp\" fjp@ubuntu : ~ $ git config --global user.email \"franz.pucher@gmail.com\" To store your password credentials when pushing and pulling to the remote repository use the following commands: fjp@ubuntu : ~/git/2wd-robot $ git config --global credential.helper store fjp@ubuntu : ~/git/2wd-robot $ git push Username for 'https://github.com': fjp Password for 'https://fjp@github.com': Everything up-to-date fjp@ubuntu : ~/git/2wd-robot $ git push Everything up-to-date","title":"Git Setup"},{"location":"git-setup/#git-setup","text":"Install git on Ubuntu via the following command: fjp@ubuntu : ~/git/2wd-robot $ sudo apt install git Set your username and email address that you use on github (when using github to host your repository): fjp@ubuntu : ~ $ git config --global user.name \"fjp\" fjp@ubuntu : ~ $ git config --global user.email \"franz.pucher@gmail.com\" To store your password credentials when pushing and pulling to the remote repository use the following commands: fjp@ubuntu : ~/git/2wd-robot $ git config --global credential.helper store fjp@ubuntu : ~/git/2wd-robot $ git push Username for 'https://github.com': fjp Password for 'https://fjp@github.com': Everything up-to-date fjp@ubuntu : ~/git/2wd-robot $ git push Everything up-to-date","title":"git setup"},{"location":"grove_motor_driver/","text":"Grove - I2C Motor Driver V1.3 The package for the Grove I2C Motor Driver V1.3 from Seeed Studio is created with catkin create pkg PKG_NAME [--catkin-deps [DEP [DEP ...]]] : 1 2 3 4 5 6 7 fjp@ubuntu:~/git/diffbot/ros/src$ catkin create pkg grove_motor_driver --catkin-deps rospy roscpp geometry_msgs Creating package \"grove_motor_driver\" in \"/home/fjp/git/diffbot/ros/src\"... Created file grove_motor_driver/CMakeLists.txt Created file grove_motor_driver/package.xml Created folder grove_motor_driver/include/grove_motor_driver Created folder grove_motor_driver/src Successfully created package files in /home/fjp/git/diffbot/ros/src/grove_motor_driver. The package depends on the two ROS client libraries rospy and roscpp . To control the two motors the package will use the geometry_msgs/Twist message. The interface to the motor driver is done with the Python library from DexterInd which is a rewerite of the Seeed Studio Arduino library . This library requires the following two python libraries RPi.GPIO smbus SMBus (System Management Bus) is a subset from the I2C protocol These libraries should be installed with pip3 , Python's package manager: 1 2 pip3 install RPi.GPIO pip3 install smbus Note that this will install these packages system wide. This is ok because they are installed on the Raspberry Pi which is dedicated to operate for this purpose. For a development environment it is best practice to use a python virtual environment like venv and install the packages inside it. Connection Afterwards the I2C port of the motor should be connected to the I2C port 1 of the Raspberry Pi 4 B. Don't forget to remove the jumper on the motor driver board which would provide power to the Pi. However, it is also not required to connect VCC and GND of the I2C connector. Only the SDA (data) and SCL (clock) wires are required. Make sure to set the address with the dip switches on the motor driver to 0x0f because this is the default address used in the library files. To test the physical I2C connection use i2cdetect described in Hardware Interfaces : The output should list 0f in the address table: 1 2 3 4 5 6 7 8 9 10 $ i2cdetect -y 1 0 1 2 3 4 5 6 7 8 9 a b c d e f 00: -- -- -- -- -- -- -- -- -- -- -- -- 0f 10: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 20: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 30: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 40: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 50: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 60: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 70: -- -- -- -- -- -- -- -- Test Motor Driver Test the motor driver by running one of the python files: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 fjp@ubuntu:~/git/diffbot/ros/src/grove_motor_driver/src$ python3 motor_example.py Forward Back Stop Speed: 0 Speed: 1 Speed: 2 Speed: 3 Speed: 4 Speed: 5 Speed: 6 Speed: 7 Speed: 8 Speed: 9 Speed: 10 Speed: 11 Speed: 12 ... Speed: 25 Speed: 26 Speed: 27 Speed: 28 Speed: 29 Speed: 30 Speed: 31 Speed: 32 Speed: 33 ... Speed: 55 Speed: 56 Speed: 57 Speed: 58 Speed: 59 Speed: 60 Speed: 61 Speed: 62 Speed: 63 Speed: 64 Speed: 65 Speed: 66 ... Speed: 75 Speed: 76 Speed: 77 Speed: 78 Speed: 79 Speed: 80 Speed: 81 Speed: 82 Speed: 83 Speed: 84 Speed: 85 Speed: 86 Speed: 87 Speed: 88 ... Speed: 97 Speed: 98 Speed: 99 Stop Troubleshooting If you get errors like the following, make sure the I2C cables from the motor driver to the Raspberry Pi are connected (see Hardware Interfaces for more infos) and use the RESET button on the motor driver. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 fjp@ubuntu:~/git/2wd-robot/ros/src/control$ sudo python grove_i2c_motor_driver.py Traceback (most recent call last): File \"grove_i2c_motor_driver.py\", line 68, in <module> m.MotorSpeedSetAB(100,100) File \"grove_i2c_motor_driver.py\", line 57, in MotorSpeedSetAB bus.write_i2c_block_data(self.I2CMotorDriverAdd, self.MotorSpeedSet, [MotorSpeedA,MotorSpeedB]) IOError: [Errno 110] Connection timed out fjp@ubuntu:~/git/2wd-robot/ros/src/control$ sudo python grove_i2c_motor_driver.py Traceback (most recent call last): File \"grove_i2c_motor_driver.py\", line 68, in <module> m.MotorSpeedSetAB(100,100) File \"grove_i2c_motor_driver.py\", line 57, in MotorSpeedSetAB bus.write_i2c_block_data(self.I2CMotorDriverAdd, self.MotorSpeedSet, [MotorSpeedA,MotorSpeedB]) IOError: [Errno 121] Remote I/O error Try pressing the RESET button and release it right before executing one of the scripts. You should also be able to detect the motor driver with i2cdetect -y 1 : 1 2 3 4 5 6 7 8 9 10 fjp@ubuntu:~/git/2wd-robot/ros/src/control/src$ sudo i2cdetect -y 1 0 1 2 3 4 5 6 7 8 9 a b c d e f 00: -- -- -- -- -- -- -- -- -- -- -- -- 0f 10: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 20: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 30: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 40: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 50: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 60: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 70: -- -- -- -- -- -- -- -- As you can see the address of the motor driver is detected at 0x0f . In case of the following output, where every address of the I2C bus seems to be taken, it is most likely that the SDA (data) and SCL (clock) signal cables are switched: 1 2 3 4 5 6 7 8 9 10 i2cdetect -y 1 0 1 2 3 4 5 6 7 8 9 a b c d e f 00: 03 04 05 06 07 08 09 0a 0b 0c 0d 0e 0f 10: 10 11 12 13 14 15 16 17 18 19 1a 1b 1c 1d 1e 1f 20: 20 21 22 23 24 25 26 27 28 29 2a 2b 2c 2d 2e 2f 30: 30 31 32 33 34 35 36 37 38 39 3a 3b 3c 3d 3e 3f 40: 40 41 42 43 44 45 46 47 48 49 4a 4b 4c 4d 4e 4f 50: 50 51 52 53 54 55 56 57 58 59 5a 5b 5c 5d 5e 5f 60: 60 61 62 63 64 65 66 67 68 69 6a 6b 6c 6d 6e 6f 70: 70 71 72 73 74 75 76 77 To solve this make sure the SDA and SCL cables are connected to the right pins on the Raspberry Pi. See Hardware Interfaces for more infos. Another solution is to restart the Raspberry Pi while making sure that the motor driver is powerd on by connecting it to the battery pack. ROS Node for Motor Driver To use the available library of the Grove I2C motor driver in ROS we need to create a wrapper node, called motor_driver . The node subscribes to the topic /2wd_robot/cmd_vel which is of type Twist message from the geometry_msgs header. To send commands to the motor the . These topics can be published with nodes from the navigation stack or with rostopic pub for test purposes. https://en.wikipedia.org/wiki/Differential_wheeled_robot http://wiki.ros.org/differential_drive After the node has been implemented, we need to build the workspace with catkin build : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 fjp@ubuntu:~/git/2wd-robot/ros$ catkin build ---------------------------------------------------------------- Profile: default Extending: [cached] /opt/ros/melodic Workspace: /home/fjp/git/2wd-robot/ros ---------------------------------------------------------------- Build Space: [exists] /home/fjp/git/2wd-robot/ros/build Devel Space: [exists] /home/fjp/git/2wd-robot/ros/devel Install Space: [unused] /home/fjp/git/2wd-robot/ros/install Log Space: [exists] /home/fjp/git/2wd-robot/ros/logs Source Space: [exists] /home/fjp/git/2wd-robot/ros/src DESTDIR: [unused] None ---------------------------------------------------------------- Devel Space Layout: linked Install Space Layout: None ---------------------------------------------------------------- Additional CMake Args: None Additional Make Args: None Additional catkin Make Args: None Internal Make Job Server: True Cache Job Environments: False ---------------------------------------------------------------- Whitelisted Packages: None Blacklisted Packages: None ---------------------------------------------------------------- Workspace configuration appears valid. ---------------------------------------------------------------- [build] Found '2' packages in 0.0 seconds. [build] Updating package table. Starting >>> grove_motor_driver Starting >>> grove_ultrasonic_ranger Finished <<< grove_motor_driver [ 1.0 seconds ] Finished <<< grove_ultrasonic_ranger [ 1.0 seconds ] [build] Summary: All 2 packages succeeded! [build] Ignored: None. [build] Warnings: None. [build] Abandoned: None. [build] Failed: None. [build] Runtime: 2.0 seconds total. As the final note of the build output suggests, we have to source the setup.bash files in the devel space. 1 fjp@ubuntu:~/git/2wd-robot/ros$ source devel/setup.bash To make the ranger node executable we have to modify the ranger.py file: 1 fjp@ubuntu:~/git/2wd-robot/ros/src/grove_ultrasonic_ranger/src$ sudo chmod a+x ranger.py","title":"Motor Driver"},{"location":"grove_motor_driver/#grove-i2c-motor-driver-v13","text":"The package for the Grove I2C Motor Driver V1.3 from Seeed Studio is created with catkin create pkg PKG_NAME [--catkin-deps [DEP [DEP ...]]] : 1 2 3 4 5 6 7 fjp@ubuntu:~/git/diffbot/ros/src$ catkin create pkg grove_motor_driver --catkin-deps rospy roscpp geometry_msgs Creating package \"grove_motor_driver\" in \"/home/fjp/git/diffbot/ros/src\"... Created file grove_motor_driver/CMakeLists.txt Created file grove_motor_driver/package.xml Created folder grove_motor_driver/include/grove_motor_driver Created folder grove_motor_driver/src Successfully created package files in /home/fjp/git/diffbot/ros/src/grove_motor_driver. The package depends on the two ROS client libraries rospy and roscpp . To control the two motors the package will use the geometry_msgs/Twist message. The interface to the motor driver is done with the Python library from DexterInd which is a rewerite of the Seeed Studio Arduino library . This library requires the following two python libraries RPi.GPIO smbus SMBus (System Management Bus) is a subset from the I2C protocol These libraries should be installed with pip3 , Python's package manager: 1 2 pip3 install RPi.GPIO pip3 install smbus Note that this will install these packages system wide. This is ok because they are installed on the Raspberry Pi which is dedicated to operate for this purpose. For a development environment it is best practice to use a python virtual environment like venv and install the packages inside it.","title":"Grove - I2C Motor Driver V1.3"},{"location":"grove_motor_driver/#connection","text":"Afterwards the I2C port of the motor should be connected to the I2C port 1 of the Raspberry Pi 4 B. Don't forget to remove the jumper on the motor driver board which would provide power to the Pi. However, it is also not required to connect VCC and GND of the I2C connector. Only the SDA (data) and SCL (clock) wires are required. Make sure to set the address with the dip switches on the motor driver to 0x0f because this is the default address used in the library files. To test the physical I2C connection use i2cdetect described in Hardware Interfaces : The output should list 0f in the address table: 1 2 3 4 5 6 7 8 9 10 $ i2cdetect -y 1 0 1 2 3 4 5 6 7 8 9 a b c d e f 00: -- -- -- -- -- -- -- -- -- -- -- -- 0f 10: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 20: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 30: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 40: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 50: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 60: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 70: -- -- -- -- -- -- -- --","title":"Connection"},{"location":"grove_motor_driver/#test-motor-driver","text":"Test the motor driver by running one of the python files: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 fjp@ubuntu:~/git/diffbot/ros/src/grove_motor_driver/src$ python3 motor_example.py Forward Back Stop Speed: 0 Speed: 1 Speed: 2 Speed: 3 Speed: 4 Speed: 5 Speed: 6 Speed: 7 Speed: 8 Speed: 9 Speed: 10 Speed: 11 Speed: 12 ... Speed: 25 Speed: 26 Speed: 27 Speed: 28 Speed: 29 Speed: 30 Speed: 31 Speed: 32 Speed: 33 ... Speed: 55 Speed: 56 Speed: 57 Speed: 58 Speed: 59 Speed: 60 Speed: 61 Speed: 62 Speed: 63 Speed: 64 Speed: 65 Speed: 66 ... Speed: 75 Speed: 76 Speed: 77 Speed: 78 Speed: 79 Speed: 80 Speed: 81 Speed: 82 Speed: 83 Speed: 84 Speed: 85 Speed: 86 Speed: 87 Speed: 88 ... Speed: 97 Speed: 98 Speed: 99 Stop","title":"Test Motor Driver"},{"location":"grove_motor_driver/#troubleshooting","text":"If you get errors like the following, make sure the I2C cables from the motor driver to the Raspberry Pi are connected (see Hardware Interfaces for more infos) and use the RESET button on the motor driver. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 fjp@ubuntu:~/git/2wd-robot/ros/src/control$ sudo python grove_i2c_motor_driver.py Traceback (most recent call last): File \"grove_i2c_motor_driver.py\", line 68, in <module> m.MotorSpeedSetAB(100,100) File \"grove_i2c_motor_driver.py\", line 57, in MotorSpeedSetAB bus.write_i2c_block_data(self.I2CMotorDriverAdd, self.MotorSpeedSet, [MotorSpeedA,MotorSpeedB]) IOError: [Errno 110] Connection timed out fjp@ubuntu:~/git/2wd-robot/ros/src/control$ sudo python grove_i2c_motor_driver.py Traceback (most recent call last): File \"grove_i2c_motor_driver.py\", line 68, in <module> m.MotorSpeedSetAB(100,100) File \"grove_i2c_motor_driver.py\", line 57, in MotorSpeedSetAB bus.write_i2c_block_data(self.I2CMotorDriverAdd, self.MotorSpeedSet, [MotorSpeedA,MotorSpeedB]) IOError: [Errno 121] Remote I/O error Try pressing the RESET button and release it right before executing one of the scripts. You should also be able to detect the motor driver with i2cdetect -y 1 : 1 2 3 4 5 6 7 8 9 10 fjp@ubuntu:~/git/2wd-robot/ros/src/control/src$ sudo i2cdetect -y 1 0 1 2 3 4 5 6 7 8 9 a b c d e f 00: -- -- -- -- -- -- -- -- -- -- -- -- 0f 10: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 20: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 30: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 40: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 50: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 60: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 70: -- -- -- -- -- -- -- -- As you can see the address of the motor driver is detected at 0x0f . In case of the following output, where every address of the I2C bus seems to be taken, it is most likely that the SDA (data) and SCL (clock) signal cables are switched: 1 2 3 4 5 6 7 8 9 10 i2cdetect -y 1 0 1 2 3 4 5 6 7 8 9 a b c d e f 00: 03 04 05 06 07 08 09 0a 0b 0c 0d 0e 0f 10: 10 11 12 13 14 15 16 17 18 19 1a 1b 1c 1d 1e 1f 20: 20 21 22 23 24 25 26 27 28 29 2a 2b 2c 2d 2e 2f 30: 30 31 32 33 34 35 36 37 38 39 3a 3b 3c 3d 3e 3f 40: 40 41 42 43 44 45 46 47 48 49 4a 4b 4c 4d 4e 4f 50: 50 51 52 53 54 55 56 57 58 59 5a 5b 5c 5d 5e 5f 60: 60 61 62 63 64 65 66 67 68 69 6a 6b 6c 6d 6e 6f 70: 70 71 72 73 74 75 76 77 To solve this make sure the SDA and SCL cables are connected to the right pins on the Raspberry Pi. See Hardware Interfaces for more infos. Another solution is to restart the Raspberry Pi while making sure that the motor driver is powerd on by connecting it to the battery pack.","title":"Troubleshooting"},{"location":"grove_motor_driver/#ros-node-for-motor-driver","text":"To use the available library of the Grove I2C motor driver in ROS we need to create a wrapper node, called motor_driver . The node subscribes to the topic /2wd_robot/cmd_vel which is of type Twist message from the geometry_msgs header. To send commands to the motor the . These topics can be published with nodes from the navigation stack or with rostopic pub for test purposes. https://en.wikipedia.org/wiki/Differential_wheeled_robot http://wiki.ros.org/differential_drive After the node has been implemented, we need to build the workspace with catkin build : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 fjp@ubuntu:~/git/2wd-robot/ros$ catkin build ---------------------------------------------------------------- Profile: default Extending: [cached] /opt/ros/melodic Workspace: /home/fjp/git/2wd-robot/ros ---------------------------------------------------------------- Build Space: [exists] /home/fjp/git/2wd-robot/ros/build Devel Space: [exists] /home/fjp/git/2wd-robot/ros/devel Install Space: [unused] /home/fjp/git/2wd-robot/ros/install Log Space: [exists] /home/fjp/git/2wd-robot/ros/logs Source Space: [exists] /home/fjp/git/2wd-robot/ros/src DESTDIR: [unused] None ---------------------------------------------------------------- Devel Space Layout: linked Install Space Layout: None ---------------------------------------------------------------- Additional CMake Args: None Additional Make Args: None Additional catkin Make Args: None Internal Make Job Server: True Cache Job Environments: False ---------------------------------------------------------------- Whitelisted Packages: None Blacklisted Packages: None ---------------------------------------------------------------- Workspace configuration appears valid. ---------------------------------------------------------------- [build] Found '2' packages in 0.0 seconds. [build] Updating package table. Starting >>> grove_motor_driver Starting >>> grove_ultrasonic_ranger Finished <<< grove_motor_driver [ 1.0 seconds ] Finished <<< grove_ultrasonic_ranger [ 1.0 seconds ] [build] Summary: All 2 packages succeeded! [build] Ignored: None. [build] Warnings: None. [build] Abandoned: None. [build] Failed: None. [build] Runtime: 2.0 seconds total. As the final note of the build output suggests, we have to source the setup.bash files in the devel space. 1 fjp@ubuntu:~/git/2wd-robot/ros$ source devel/setup.bash To make the ranger node executable we have to modify the ranger.py file: 1 fjp@ubuntu:~/git/2wd-robot/ros/src/grove_ultrasonic_ranger/src$ sudo chmod a+x ranger.py","title":"ROS Node for Motor Driver"},{"location":"grove_ultrasonic_ranger/","text":"Grove - Ultrasonic Ranger To avoid obstacles the Grove ultrasonic ranger from Seeed Studio is used. We will create a ROS package with catkin create pkg PKG_NAME [--catkin-deps [DEP [DEP ...]]] : 1 2 3 4 5 6 fjp@ubuntu:~/git/2wd-robot/ros/src$ catkin create pkg grove_ultrasonic_ranger --catkin-deps rospy roscpp sensor_msgs Creating package \"grove_ultrasonic_ranger\" in \"/home/fjp/git/2wd-robot/ros/src\" ... Created file grove_ultrasonic_ranger/CMakeLists.txt Created file grove_ultrasonic_ranger/package.xml Created folder grove_ultrasonic_ranger/include/grove_ultrasonic_ranger Created folder grove_ultrasonic_ranger/src The package depends on the two ROS client libraries rospy and roscpp . To signalise the current distance to obstacles the sensor_msgs/Range message is used. After connecting the signal pin of the sensor to (physical) GPIO 11 of the Raspberry Pi 4 B and power it with 5V and ground, we can test its functionality with the available python script ultrasonic.py from Seed Studio. The following shows the truncated output of the ultrasonic.py script when moving an obstacle in front of the sensor. We see that the distance value changes as expected. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 fjp@ubuntu:~/git/2wd-robot/ros/src/grove_ultrasonic_ranger/src$ sudo python ultrasonic.py SeeedStudio Grove Ultrasonic get data and print Ultrasonic Measurement Distance : 2 .0 CM Ultrasonic Measurement Distance : 2 .1 CM Ultrasonic Measurement Distance : 2 .0 CM Ultrasonic Measurement Distance : 3 .5 CM Ultrasonic Measurement Distance : 3 .5 CM Ultrasonic Measurement Distance : 2 .4 CM Ultrasonic Measurement Distance : 12 .9 CM ... Ultrasonic Measurement Distance : 45 .8 CM Ultrasonic Measurement Distance : 510 .7 CM Ultrasonic Measurement Distance : 510 .7 CM Ultrasonic Measurement Distance : 45 .7 CM Ultrasonic Measurement Distance : 161 .8 CM Ultrasonic Measurement Distance : 26 .9 CM Ultrasonic Measurement Distance : 18 .3 CM Ultrasonic Measurement Distance : 160 .9 CM Ultrasonic Measurement Distance : 158 .3 CM Ultrasonic Measurement Distance : 159 .4 CM ... Modified GroveUltrasonicRanger Library To use the ultrasonic ranger as a ROS node it is convenient to wrap the sensor functionality in a class. This provides an easy to extend interface for the ultrasonic ranger ( API ) Therefore I copied the core functionality of the ultrasonic.py script from Seeed Studio in a class named GroveUltrasonicRanger . Executing the grove_ultrasonic_ranger.py will result in the following output: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 fjp@ubuntu:~/git/2wd-robot/ros/src/grove_ultrasonic_ranger/src$ sudo python grove_ultrasonic_ranger.py SeeedStudio Grove Ultrasonic get data and print Distance : 0 .051 m Distance : 0 .069 m Distance : 0 .098 m Distance : 0 .131 m Distance : 0 .153 m Distance : 0 .172 m Distance : 0 .207 m Distance : 0 .210 m Distance : 0 .234 m Distance : 0 .256 m GPIO.cleanup () GPIO.cleanup () done ROS Node for Ultrasonic Ranger ROS provides the Range Message in the sensor_msgs header . This message type can be used to write a wrapper that will act as a ROS node for the Grove ultrasonic sensor. To design this node we will send out measurements periodically over a topic of type sensor_msgs/Range . The code for this node is in ranger.py . After writing the node we need to build the packages in the workspace with catkin build . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 fjp@ubuntu:~/git/2wd-robot/ros$ catkin build ---------------------------------------------------------------- Profile: default Extending: [ env ] /opt/ros/melodic Workspace: /home/fjp/git/2wd-robot/ros ---------------------------------------------------------------- Build Space: [ exists ] /home/fjp/git/2wd-robot/ros/build Devel Space: [ exists ] /home/fjp/git/2wd-robot/ros/devel Install Space: [ unused ] /home/fjp/git/2wd-robot/ros/install Log Space: [ missing ] /home/fjp/git/2wd-robot/ros/logs Source Space: [ exists ] /home/fjp/git/2wd-robot/ros/src DESTDIR: [ unused ] None ---------------------------------------------------------------- Devel Space Layout: linked Install Space Layout: None ---------------------------------------------------------------- Additional CMake Args: None Additional Make Args: None Additional catkin Make Args: None Internal Make Job Server: True Cache Job Environments: False ---------------------------------------------------------------- Whitelisted Packages: None Blacklisted Packages: None ---------------------------------------------------------------- Workspace configuration appears valid. NOTE: Forcing CMake to run for each package. ---------------------------------------------------------------- [ build ] Found '1' package in 0 .0 seconds. [ build ] Updating package table. Starting >>> catkin_tools_prebuild Finished <<< catkin_tools_prebuild [ 9 .9 seconds ] Starting >>> grove_ultrasonic_ranger Finished <<< grove_ultrasonic_ranger [ 12 .0 seconds ] [ build ] Summary: All 2 packages succeeded! [ build ] Ignored: None. [ build ] Warnings: None. [ build ] Abandoned: None. [ build ] Failed: None. [ build ] Runtime: 21 .9 seconds total. [ build ] Note: Workspace packages have changed, please re-source setup files to use them. As the final note of the build output suggests, we have to source the setup.bash files in the devel space. 1 fjp@ubuntu:~/git/2wd-robot/ros$ source devel/setup.bash To make the ranger node executable we have to modify the ranger.py file: 1 fjp@ubuntu:~/git/2wd-robot/ros/src/grove_ultrasonic_ranger/src$ sudo chmod a+x ranger.py Then we can test the node using rosrun : 1 2 3 4 5 6 7 8 9 10 fjp@ubuntu:~/git/2wd-robot/ros$ sudo su [ sudo ] password for fjp: root@ubuntu:/home/fjp/git/2wd-robot/ros# source devel/setup.bash root@ubuntu:/home/fjp/git/2wd-robot/ros# rosrun grove_ultrasonic_ranger ranger.py Distance : 1 .617 m Distance : 1 .617 m Distance : 1 .617 m Distance : 0 .108 m Distance : 0 .092 m Distance : 0 .099 m This lets the node publish range messages which we can capture in another terminal window using rostopic . First we use rostopic list to find the name of the topic we are interested in: 1 2 3 4 fjp@ubuntu:~/git/2wd-robot/ros$ rostopic list /distance /rosout /rosout_agg We named our topic /distance which we can use with the rostopic echo command to see the published messages: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 fjp@ubuntu:~/git/2wd-robot/ros$ rostopic echo /distance header: seq: 1 stamp: secs: 1576778377 nsecs: 746809959 frame_id: \"ranger_distance\" radiation_type: 0 field_of_view: 0 .261799007654 min_range: 0 .019999999553 max_range: 3 .5 range: 1 .61674261093 --- header: seq: 2 stamp: secs: 1576778378 nsecs: 459048986 frame_id: \"ranger_distance\" radiation_type: 0 field_of_view: 0 .261799007654 min_range: 0 .019999999553 max_range: 3 .5 range: 1 .61261284351 --- header: seq: 3 stamp: secs: 1576778379 nsecs: 172172069 frame_id: \"ranger_distance\" radiation_type: 0 field_of_view: 0 .261799007654 min_range: 0 .019999999553 max_range: 3 .5 range: 1 .61657905579 --- header: seq: 4 stamp: secs: 1576778379 nsecs: 884002923 frame_id: \"ranger_distance\" radiation_type: 0 field_of_view: 0 .261799007654 min_range: 0 .019999999553 max_range: 3 .5 range: 1 .61277639866 --- header: seq: 5 stamp: secs: 1576778380 nsecs: 596549034 frame_id: \"ranger_distance\" radiation_type: 0 field_of_view: 0 .261799007654 min_range: 0 .019999999553 max_range: 3 .5 range: 1 .67693090439 --- Informational Distance Measurements To provide additional information when an obstacle is too close or the robot has no obstacle in front of it we use REP-117 as guideline. To successfully implement this REP, checks for valid measurements should use floating-point standards and follow this form: 1 2 3 4 5 6 7 8 9 10 11 if ( minimum_range <= value && value <= maximum_range ){ // Represents expected pre-REP logic and is the only necessary condition for most applications. // This is a valid measurement. } else if ( ! isfinite ( value ) && value < 0 ){ // Object too close to measure. } else if ( ! isfinite ( value ) && value > 0 ){ // No objects detected in range. } else if ( isnan ( value ) ){ // This is an erroneous, invalid, or missing measurement. } else { // The sensor reported these measurements as valid, but they are discarded per the limits defined by minimum_range and maximum_range. }","title":"Grove ultrasonic ranger"},{"location":"grove_ultrasonic_ranger/#grove-ultrasonic-ranger","text":"To avoid obstacles the Grove ultrasonic ranger from Seeed Studio is used. We will create a ROS package with catkin create pkg PKG_NAME [--catkin-deps [DEP [DEP ...]]] : 1 2 3 4 5 6 fjp@ubuntu:~/git/2wd-robot/ros/src$ catkin create pkg grove_ultrasonic_ranger --catkin-deps rospy roscpp sensor_msgs Creating package \"grove_ultrasonic_ranger\" in \"/home/fjp/git/2wd-robot/ros/src\" ... Created file grove_ultrasonic_ranger/CMakeLists.txt Created file grove_ultrasonic_ranger/package.xml Created folder grove_ultrasonic_ranger/include/grove_ultrasonic_ranger Created folder grove_ultrasonic_ranger/src The package depends on the two ROS client libraries rospy and roscpp . To signalise the current distance to obstacles the sensor_msgs/Range message is used. After connecting the signal pin of the sensor to (physical) GPIO 11 of the Raspberry Pi 4 B and power it with 5V and ground, we can test its functionality with the available python script ultrasonic.py from Seed Studio. The following shows the truncated output of the ultrasonic.py script when moving an obstacle in front of the sensor. We see that the distance value changes as expected. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 fjp@ubuntu:~/git/2wd-robot/ros/src/grove_ultrasonic_ranger/src$ sudo python ultrasonic.py SeeedStudio Grove Ultrasonic get data and print Ultrasonic Measurement Distance : 2 .0 CM Ultrasonic Measurement Distance : 2 .1 CM Ultrasonic Measurement Distance : 2 .0 CM Ultrasonic Measurement Distance : 3 .5 CM Ultrasonic Measurement Distance : 3 .5 CM Ultrasonic Measurement Distance : 2 .4 CM Ultrasonic Measurement Distance : 12 .9 CM ... Ultrasonic Measurement Distance : 45 .8 CM Ultrasonic Measurement Distance : 510 .7 CM Ultrasonic Measurement Distance : 510 .7 CM Ultrasonic Measurement Distance : 45 .7 CM Ultrasonic Measurement Distance : 161 .8 CM Ultrasonic Measurement Distance : 26 .9 CM Ultrasonic Measurement Distance : 18 .3 CM Ultrasonic Measurement Distance : 160 .9 CM Ultrasonic Measurement Distance : 158 .3 CM Ultrasonic Measurement Distance : 159 .4 CM ...","title":"Grove - Ultrasonic Ranger"},{"location":"grove_ultrasonic_ranger/#modified-groveultrasonicranger-library","text":"To use the ultrasonic ranger as a ROS node it is convenient to wrap the sensor functionality in a class. This provides an easy to extend interface for the ultrasonic ranger ( API ) Therefore I copied the core functionality of the ultrasonic.py script from Seeed Studio in a class named GroveUltrasonicRanger . Executing the grove_ultrasonic_ranger.py will result in the following output: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 fjp@ubuntu:~/git/2wd-robot/ros/src/grove_ultrasonic_ranger/src$ sudo python grove_ultrasonic_ranger.py SeeedStudio Grove Ultrasonic get data and print Distance : 0 .051 m Distance : 0 .069 m Distance : 0 .098 m Distance : 0 .131 m Distance : 0 .153 m Distance : 0 .172 m Distance : 0 .207 m Distance : 0 .210 m Distance : 0 .234 m Distance : 0 .256 m GPIO.cleanup () GPIO.cleanup () done","title":"Modified GroveUltrasonicRanger Library"},{"location":"grove_ultrasonic_ranger/#ros-node-for-ultrasonic-ranger","text":"ROS provides the Range Message in the sensor_msgs header . This message type can be used to write a wrapper that will act as a ROS node for the Grove ultrasonic sensor. To design this node we will send out measurements periodically over a topic of type sensor_msgs/Range . The code for this node is in ranger.py . After writing the node we need to build the packages in the workspace with catkin build . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 fjp@ubuntu:~/git/2wd-robot/ros$ catkin build ---------------------------------------------------------------- Profile: default Extending: [ env ] /opt/ros/melodic Workspace: /home/fjp/git/2wd-robot/ros ---------------------------------------------------------------- Build Space: [ exists ] /home/fjp/git/2wd-robot/ros/build Devel Space: [ exists ] /home/fjp/git/2wd-robot/ros/devel Install Space: [ unused ] /home/fjp/git/2wd-robot/ros/install Log Space: [ missing ] /home/fjp/git/2wd-robot/ros/logs Source Space: [ exists ] /home/fjp/git/2wd-robot/ros/src DESTDIR: [ unused ] None ---------------------------------------------------------------- Devel Space Layout: linked Install Space Layout: None ---------------------------------------------------------------- Additional CMake Args: None Additional Make Args: None Additional catkin Make Args: None Internal Make Job Server: True Cache Job Environments: False ---------------------------------------------------------------- Whitelisted Packages: None Blacklisted Packages: None ---------------------------------------------------------------- Workspace configuration appears valid. NOTE: Forcing CMake to run for each package. ---------------------------------------------------------------- [ build ] Found '1' package in 0 .0 seconds. [ build ] Updating package table. Starting >>> catkin_tools_prebuild Finished <<< catkin_tools_prebuild [ 9 .9 seconds ] Starting >>> grove_ultrasonic_ranger Finished <<< grove_ultrasonic_ranger [ 12 .0 seconds ] [ build ] Summary: All 2 packages succeeded! [ build ] Ignored: None. [ build ] Warnings: None. [ build ] Abandoned: None. [ build ] Failed: None. [ build ] Runtime: 21 .9 seconds total. [ build ] Note: Workspace packages have changed, please re-source setup files to use them. As the final note of the build output suggests, we have to source the setup.bash files in the devel space. 1 fjp@ubuntu:~/git/2wd-robot/ros$ source devel/setup.bash To make the ranger node executable we have to modify the ranger.py file: 1 fjp@ubuntu:~/git/2wd-robot/ros/src/grove_ultrasonic_ranger/src$ sudo chmod a+x ranger.py Then we can test the node using rosrun : 1 2 3 4 5 6 7 8 9 10 fjp@ubuntu:~/git/2wd-robot/ros$ sudo su [ sudo ] password for fjp: root@ubuntu:/home/fjp/git/2wd-robot/ros# source devel/setup.bash root@ubuntu:/home/fjp/git/2wd-robot/ros# rosrun grove_ultrasonic_ranger ranger.py Distance : 1 .617 m Distance : 1 .617 m Distance : 1 .617 m Distance : 0 .108 m Distance : 0 .092 m Distance : 0 .099 m This lets the node publish range messages which we can capture in another terminal window using rostopic . First we use rostopic list to find the name of the topic we are interested in: 1 2 3 4 fjp@ubuntu:~/git/2wd-robot/ros$ rostopic list /distance /rosout /rosout_agg We named our topic /distance which we can use with the rostopic echo command to see the published messages: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 fjp@ubuntu:~/git/2wd-robot/ros$ rostopic echo /distance header: seq: 1 stamp: secs: 1576778377 nsecs: 746809959 frame_id: \"ranger_distance\" radiation_type: 0 field_of_view: 0 .261799007654 min_range: 0 .019999999553 max_range: 3 .5 range: 1 .61674261093 --- header: seq: 2 stamp: secs: 1576778378 nsecs: 459048986 frame_id: \"ranger_distance\" radiation_type: 0 field_of_view: 0 .261799007654 min_range: 0 .019999999553 max_range: 3 .5 range: 1 .61261284351 --- header: seq: 3 stamp: secs: 1576778379 nsecs: 172172069 frame_id: \"ranger_distance\" radiation_type: 0 field_of_view: 0 .261799007654 min_range: 0 .019999999553 max_range: 3 .5 range: 1 .61657905579 --- header: seq: 4 stamp: secs: 1576778379 nsecs: 884002923 frame_id: \"ranger_distance\" radiation_type: 0 field_of_view: 0 .261799007654 min_range: 0 .019999999553 max_range: 3 .5 range: 1 .61277639866 --- header: seq: 5 stamp: secs: 1576778380 nsecs: 596549034 frame_id: \"ranger_distance\" radiation_type: 0 field_of_view: 0 .261799007654 min_range: 0 .019999999553 max_range: 3 .5 range: 1 .67693090439 ---","title":"ROS Node for Ultrasonic Ranger"},{"location":"grove_ultrasonic_ranger/#informational-distance-measurements","text":"To provide additional information when an obstacle is too close or the robot has no obstacle in front of it we use REP-117 as guideline. To successfully implement this REP, checks for valid measurements should use floating-point standards and follow this form: 1 2 3 4 5 6 7 8 9 10 11 if ( minimum_range <= value && value <= maximum_range ){ // Represents expected pre-REP logic and is the only necessary condition for most applications. // This is a valid measurement. } else if ( ! isfinite ( value ) && value < 0 ){ // Object too close to measure. } else if ( ! isfinite ( value ) && value > 0 ){ // No objects detected in range. } else if ( isnan ( value ) ){ // This is an erroneous, invalid, or missing measurement. } else { // The sensor reported these measurements as valid, but they are discarded per the limits defined by minimum_range and maximum_range. }","title":"Informational Distance Measurements"},{"location":"hardware-interfaces/","text":"The hardware interfaces provide an interface between the components (sensors and actuators) of the 2WD robot and its processing units, the Raspberry Pi 4 B (or the Nvidia Jetson Nano) and the microcontroller (in this case the Teensy 4.0). USB The Universial Serial Bus (USB) connections are required to connect the Single Board Computer (SBC) with the microcontroller. Using this connection, it is possible to communicate via rosserial . Another USB connector is used for the RPLidar laser scanner. Info See the section USB Devices below to setup the required permissions and allow the communication between this interface. Single Board Computer GPIO Currently, one GPIO pin is used to connect the ultrasonic ranger. The ultrasonic ranger uses just a single GPIO pin for communicating its measured distances. Therefore, we can use one of the GPIO pins such as physical pin 11 . Info In case you are using LM393 speed sensors, instead of the encoders of the DG01D-E, the LM393 use a single digital GPIO pin each. These pins could be directly connected to the Raspberry Pi GPIOs and setup using software interrupts with the RPi.GPIO library. Alternatively they could be also connected to the pins of the microcontroller, e.g. Teensy. For this build the Microcontroller Digital Pins Four digital pins on the Teensy microcontroller are in use for the two quadrature encoders of the DG01D-E. Info See the diffbot_base package for the running software script to read the encoder ticks. Single Board Computer I2C Connection The I2C connections on the Raspberry Pi 4 B are used for multiple components such as the motor driver and the oled display. I2C Pinout on Raspberry Pi 4 B. Using these ports on the Raspberry Pi 4 B, requires that we enable the I2C interface. To do so, we will use the tool i2cdetect which requires that we install a tool on Ubuntu called i2c-tools : 1 2 3 4 5 6 7 fjp@ubuntu:~/git/2wd-robot$ i2cdetect Command 'i2cdetect' not found, but can be installed with: sudo apt install i2c-tools fjp@ubuntu:~/git/2wd-robot$ sudo apt install i2c-tools This i2cdetect tool is a userspace program to scan an I2C bus for devices given a specific i2cbus argument which indicates the number or name of the I2C bus to be scanned, and should correspond to one of the busses listed by i2cdetect -l . See also info i2cdetect for the manual page. To test if the i2c ports are working we use the following commands: 1 2 3 4 $ i2cdetect -y 0 Error: Could not open file '/dev/i2c-0' or '/dev/i2c/0': No such file or directory $ i2cdetect -y 1 Error: Could not open file '/dev/i2c-1' or '/dev/i2c/1': No such file or directory The ports are not setup correctly yet, which is why we need to enable the following two lines in the /boot/firmware/config.txt file: 1 2 dtparam = i2c0 = on dtparam = i2c1 = on After rebooting the Raspberry Pi and entering the command again the following output will appear: 1 2 3 $ i2cdetect -y 0 Error: Could not open file `/dev/i2c-0': Permission denied Run as root? Running as root using sudo will work (please read on, there is a better way): 1 2 3 4 5 6 7 8 9 10 11 $ sudo i2cdetect -y 0 [sudo] password for fjp: 0 1 2 3 4 5 6 7 8 9 a b c d e f 00: -- -- -- -- -- -- -- -- -- -- -- -- -- 10: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 20: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 30: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 40: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 50: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 60: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 70: -- -- -- -- -- -- -- -- As mentioned there is a better way to access the i2c devices without using power user privileges. When issuing the following: 1 2 ls -l /dev/i2c-0 crw-rw---- 1 root i2c 89, 0 Apr 1 2020 /dev/i2c-0 we see that the /dev/i2c-0 device belongs to user root and i2c user group. To get access without sudo we can add other users, requiering access to the i2c group with: 1 2 3 4 sudo adduser fjp i2c Adding user `fjp' to group `i2c' ... Adding user fjp to group i2c Done. After logging out and back in again the access will be granted and following output will come up: 1 2 3 4 5 6 7 8 9 10 $ i2cdetect -y 0 0 1 2 3 4 5 6 7 8 9 a b c d e f 00: -- -- -- -- -- -- -- -- -- -- -- -- -- 10: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 20: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 30: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 40: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 50: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 60: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 70: -- -- -- -- -- -- -- -- It outputs a table with the list of detected devices on the specified bus. In this case there are no connected devices on I2C bus 0. Alternative setup using raspi-config On Raspian Buster, the official Raspberry OS, we could use the `raspi-config` tool: fjp@ubuntu : ~/git/2wd-robot $ sudo raspi-config Raspberry Pi 4 Model B Rev 1.1 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 Raspberry Pi Software Configuration Tool (raspi-config) \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 1 Change User Password Change password for the current user \u2502 \u2502 2 Network Options Configure network settings \u2502 \u2502 3 Boot Options Configure options for start-up \u2502 \u2502 4 Localisation Options Set up language and regional settings to match your location \u2502 \u2502 5 Interfacing Options Configure connections to peripherals \u2502 \u2502 6 Overclock Configure overclocking for your Pi \u2502 \u2502 7 Advanced Options Configure advanced settings \u2502 \u2502 8 Update Update this tool to the latest version \u2502 \u2502 9 About raspi-config Information about this configuration tool \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 <Select> <Finish> \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Select the i2c option: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 Raspberry Pi Software Configuration Tool (raspi-config) \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 P1 Camera Enable/Disable connection to the Raspberry Pi Camera \u2502 \u2502 P2 SSH Enable/Disable remote command line access to your Pi using SSH \u2502 \u2502 P3 VNC Enable/Disable graphical remote access to your Pi using RealVNC \u2502 \u2502 P4 SPI Enable/Disable automatic loading of SPI kernel module \u2502 \u2502 P5 I2C Enable/Disable automatic loading of I2C kernel module \u2502 \u2502 P6 Serial Enable/Disable shell and kernel messages on the serial connection \u2502 \u2502 P7 1-Wire Enable/Disable one-wire interface \u2502 \u2502 P8 Remote GPIO Enable/Disable remote access to GPIO pins \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 <Select> <Back> \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 And enable the interface: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 Would you like the ARM I2C interface to be enabled? \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 <Yes> <No> \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Confirm the activation and restart the RPi: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 The ARM I2C interface is enabled \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 <Ok> \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 USB Devices Similar to accessing i2c devices, a non root user can use usb connections by adding it to the the dialout group: 1 2 3 4 sudo adduser fjp dialout Adding user `fjp' to group `dialout' ... Adding user fjp to group dialout Done.","title":"Hardware Interfaces"},{"location":"hardware-interfaces/#usb","text":"The Universial Serial Bus (USB) connections are required to connect the Single Board Computer (SBC) with the microcontroller. Using this connection, it is possible to communicate via rosserial . Another USB connector is used for the RPLidar laser scanner. Info See the section USB Devices below to setup the required permissions and allow the communication between this interface.","title":"USB"},{"location":"hardware-interfaces/#single-board-computer-gpio","text":"Currently, one GPIO pin is used to connect the ultrasonic ranger. The ultrasonic ranger uses just a single GPIO pin for communicating its measured distances. Therefore, we can use one of the GPIO pins such as physical pin 11 . Info In case you are using LM393 speed sensors, instead of the encoders of the DG01D-E, the LM393 use a single digital GPIO pin each. These pins could be directly connected to the Raspberry Pi GPIOs and setup using software interrupts with the RPi.GPIO library. Alternatively they could be also connected to the pins of the microcontroller, e.g. Teensy. For this build the","title":"Single Board Computer GPIO"},{"location":"hardware-interfaces/#microcontroller-digital-pins","text":"Four digital pins on the Teensy microcontroller are in use for the two quadrature encoders of the DG01D-E. Info See the diffbot_base package for the running software script to read the encoder ticks.","title":"Microcontroller Digital Pins"},{"location":"hardware-interfaces/#single-board-computer-i2c-connection","text":"The I2C connections on the Raspberry Pi 4 B are used for multiple components such as the motor driver and the oled display. I2C Pinout on Raspberry Pi 4 B. Using these ports on the Raspberry Pi 4 B, requires that we enable the I2C interface. To do so, we will use the tool i2cdetect which requires that we install a tool on Ubuntu called i2c-tools : 1 2 3 4 5 6 7 fjp@ubuntu:~/git/2wd-robot$ i2cdetect Command 'i2cdetect' not found, but can be installed with: sudo apt install i2c-tools fjp@ubuntu:~/git/2wd-robot$ sudo apt install i2c-tools This i2cdetect tool is a userspace program to scan an I2C bus for devices given a specific i2cbus argument which indicates the number or name of the I2C bus to be scanned, and should correspond to one of the busses listed by i2cdetect -l . See also info i2cdetect for the manual page. To test if the i2c ports are working we use the following commands: 1 2 3 4 $ i2cdetect -y 0 Error: Could not open file '/dev/i2c-0' or '/dev/i2c/0': No such file or directory $ i2cdetect -y 1 Error: Could not open file '/dev/i2c-1' or '/dev/i2c/1': No such file or directory The ports are not setup correctly yet, which is why we need to enable the following two lines in the /boot/firmware/config.txt file: 1 2 dtparam = i2c0 = on dtparam = i2c1 = on After rebooting the Raspberry Pi and entering the command again the following output will appear: 1 2 3 $ i2cdetect -y 0 Error: Could not open file `/dev/i2c-0': Permission denied Run as root? Running as root using sudo will work (please read on, there is a better way): 1 2 3 4 5 6 7 8 9 10 11 $ sudo i2cdetect -y 0 [sudo] password for fjp: 0 1 2 3 4 5 6 7 8 9 a b c d e f 00: -- -- -- -- -- -- -- -- -- -- -- -- -- 10: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 20: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 30: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 40: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 50: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 60: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 70: -- -- -- -- -- -- -- -- As mentioned there is a better way to access the i2c devices without using power user privileges. When issuing the following: 1 2 ls -l /dev/i2c-0 crw-rw---- 1 root i2c 89, 0 Apr 1 2020 /dev/i2c-0 we see that the /dev/i2c-0 device belongs to user root and i2c user group. To get access without sudo we can add other users, requiering access to the i2c group with: 1 2 3 4 sudo adduser fjp i2c Adding user `fjp' to group `i2c' ... Adding user fjp to group i2c Done. After logging out and back in again the access will be granted and following output will come up: 1 2 3 4 5 6 7 8 9 10 $ i2cdetect -y 0 0 1 2 3 4 5 6 7 8 9 a b c d e f 00: -- -- -- -- -- -- -- -- -- -- -- -- -- 10: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 20: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 30: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 40: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 50: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 60: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 70: -- -- -- -- -- -- -- -- It outputs a table with the list of detected devices on the specified bus. In this case there are no connected devices on I2C bus 0. Alternative setup using raspi-config On Raspian Buster, the official Raspberry OS, we could use the `raspi-config` tool: fjp@ubuntu : ~/git/2wd-robot $ sudo raspi-config Raspberry Pi 4 Model B Rev 1.1 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 Raspberry Pi Software Configuration Tool (raspi-config) \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 1 Change User Password Change password for the current user \u2502 \u2502 2 Network Options Configure network settings \u2502 \u2502 3 Boot Options Configure options for start-up \u2502 \u2502 4 Localisation Options Set up language and regional settings to match your location \u2502 \u2502 5 Interfacing Options Configure connections to peripherals \u2502 \u2502 6 Overclock Configure overclocking for your Pi \u2502 \u2502 7 Advanced Options Configure advanced settings \u2502 \u2502 8 Update Update this tool to the latest version \u2502 \u2502 9 About raspi-config Information about this configuration tool \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 <Select> <Finish> \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Select the i2c option: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 Raspberry Pi Software Configuration Tool (raspi-config) \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 P1 Camera Enable/Disable connection to the Raspberry Pi Camera \u2502 \u2502 P2 SSH Enable/Disable remote command line access to your Pi using SSH \u2502 \u2502 P3 VNC Enable/Disable graphical remote access to your Pi using RealVNC \u2502 \u2502 P4 SPI Enable/Disable automatic loading of SPI kernel module \u2502 \u2502 P5 I2C Enable/Disable automatic loading of I2C kernel module \u2502 \u2502 P6 Serial Enable/Disable shell and kernel messages on the serial connection \u2502 \u2502 P7 1-Wire Enable/Disable one-wire interface \u2502 \u2502 P8 Remote GPIO Enable/Disable remote access to GPIO pins \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 <Select> <Back> \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 And enable the interface: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 Would you like the ARM I2C interface to be enabled? \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 <Yes> <No> \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Confirm the activation and restart the RPi: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 The ARM I2C interface is enabled \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 <Ok> \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518","title":"Single Board Computer I2C Connection"},{"location":"hardware-interfaces/#usb-devices","text":"Similar to accessing i2c devices, a non root user can use usb connections by adding it to the the dialout group: 1 2 3 4 sudo adduser fjp dialout Adding user `fjp' to group `dialout' ... Adding user fjp to group dialout Done.","title":"USB Devices"},{"location":"jetson-nano-setup/","text":"These are the instructions to setup the official Ubuntu 18.04 image from Nvidia on a Jetson Nano. Note In case you don't have a Jetson Nano, it is also possible to create the robot using a Raspberry Pi 4 B. See the related section in the documentation . Installing the JetPack SDK The JetPack SDK is a tool that installs the software tools and operating system for a Jetson Development Kit. Note JetPack SDK includes the latest Jetson Linux Driver Package (L4T) with Linux operating system and CUDA-X accelerated libraries and APIs for Deep Learning, Computer Vision, Accelerated Computing and Multimedia. It also includes samples, documentation, and developer tools for both host computer and developer kit, and supports higher level SDKs such as DeepStream for streaming video analytics and Isaac for robotics. 1 Start by downloading the 6 GB SD card image for the Jetson Nano Developer Kit from the Nvidia Developer website ( direct download link ). Alternatively you can get the Jetpack from the Jetson Download Center . Note In case you want to use the NVIDIA SDK Manager you need to be part of the NVIDIA Developer Program. You can use this manager to configure the Jetpack image before flashing it. Next use a tool to flash the image onto the SD card. One option is to use balenaEtcher that is available on Ubuntu, Mac and Windows. Info Flashing in this context, means the transfer of software data, also refered to as Firmware, from your computer to a device, such as a the sd card in this case. The term \u201cto flash\u201d comes from the Flash storage component of a device where the Firmware is stored. Power Supply There are four ways you can power the Jetson Nano. Provide 2 Amps at 5 Volts to the micro USB connector 4 Amps at 5 Volts to the barrel jack connector. 5 Volts on the GPIO headers, where each of the two 5 Volt pins can handle up to 3 A. The Nano has two of these 5 Volt pins, which means you could consume 6 A total at 5 V by having the two pins connected to a power supply that is capable of delivering 6 A at 5 V, corresponding to 30 Watts. Power Over Ethernet (POE) In its default power consumption state ('mode 0') the Nano itself uses 10 Watts, which is 2 A at 5 V. This, however, is only for the Jetson Nano compute module and doesn't include the peripherals. To provide power to peripherals, for example over the USB connectors, you need to operate the Nano itself in 5 Watts mode ('mode 1' or MAXN). This way there are 5 Watts left for peripherals via the micro USB connector, that provides 2 A at 5 V. The following table 2 shows the power modes for the Jetson Nano predefined by NVIDIA and the associated caps on use of the module's resources. Property MAXN 3 5W Power Budget 10 Watts 5 Watts Mode ID 0 1 Online CPU 4 2 CPU Maximal Frequency (MHz) 1479 918 GPU TPC 1 1 GPU Maximal Frequency (MHz) 921.6 640 Memory Maximal Frequency (MHz) 1600 1600 SOC clocks maximal frequency (MHz) All modes adsp 844.8 csi 750 se 627.2 ape 499.2 nvdec 716.8 tsec 408 host1x 408 nvenc 716.8 tsecb 627.2 isp 793.6 nvjpg 627.2 vi 793.6 display 665.6 pcie 500 vic03 627.2 Note In practice, when running from the micro-USB connector, we should be running in 5V mode to power the rest of the sensors, like the laser scanner. The drawback is that this will slow down the computations on the Jetson Nano. See the section below on how to do that. To develop and test code that requires high power, it is convenient to use the second option. To connect the 4 Amps @ 5 Volts barrel jack connector to the Jetson Nano a Jumper is required. Note Make sure to use a 5 V 4 Amps switching power supply. For example the Mean Well GST25E05-P1J or the AC/DC Desktop Adapter 5 V from Adafruit . DC barrel jack 5.5 mm OD / 2.1 mm ID / 9.5 mm length, center pin positive. Connect the jumper on J48. J48 is located between the Barrel Jack connector and the Camera connector. This jumper tells the Nano to use the Barrel Jack instead of the micro-USB port. Then plug the power supply into the Barrel Jack, and the Nano boots. TODO add image of Jumper location Note There are two variants of the Jetson Nano. The older A02 and the revised B01. Depending on the variant the location of the jumper is slightly different. - For an A02 carrier board (pre-2020) J48 is the solo header next to the camera port. - For a B01 carrier board (2020+, has two camera ports) J48 is a solo header behind the barrel jack and the HDMI port. Prepare Ubuntu After flashing the image to the sd card insert it to the Jetson Nano, hook it up to a monitor via HDMI and power it up by pluggin in the micro USB or even better barrel jack connector (don't forget the jumper on J48). The follow the instructions on the screen to setup Ubuntu 18.04 Bionic Beaver. For this you will need to accept the Nvidia End User License Agreement, set the desired language, keyboard, time zone, login credentials, APP Partition size (choose max possible), and delete unused bootloader that is done automatically with the new QSPI image (MaxSPI) of Jetpack 4.5. This QSPI update will take about two mins. Finally select the NVPModel mode explained above . First we go with the default MAXN 10 Watts mode ('mode 0'). We will changed this later at runtime - when powering the robot over the power bank - using the nvpmodel GUI or nvpmodel command line utility. Refer to the NVIDIA Jetson Linux Developer Guide for further information. Once finished, follow the next steps to install ROS Melodic. Note To proceed with the next steps on installing ROS and other related dependencies you can run a bash script. Just clone this repository: https://github.com/fjp/diffbot.git and run ubuntu-setup.sh . But to learn more, you should follow the instructions on the following pages. Resources JetPack SDK Jetson Nano \u2013 Use More Power! - Jetson Hacks https://developer.nvidia.com/embedded/jetpack \u21a9 Nvidia Jetson Nano: Supported Modes and Power Efficiency \u21a9 The default mode is MAXN (power budget 10 watts, mode ID 0). \u21a9","title":"Jetson Nano Setup"},{"location":"jetson-nano-setup/#installing-the-jetpack-sdk","text":"The JetPack SDK is a tool that installs the software tools and operating system for a Jetson Development Kit. Note JetPack SDK includes the latest Jetson Linux Driver Package (L4T) with Linux operating system and CUDA-X accelerated libraries and APIs for Deep Learning, Computer Vision, Accelerated Computing and Multimedia. It also includes samples, documentation, and developer tools for both host computer and developer kit, and supports higher level SDKs such as DeepStream for streaming video analytics and Isaac for robotics. 1 Start by downloading the 6 GB SD card image for the Jetson Nano Developer Kit from the Nvidia Developer website ( direct download link ). Alternatively you can get the Jetpack from the Jetson Download Center . Note In case you want to use the NVIDIA SDK Manager you need to be part of the NVIDIA Developer Program. You can use this manager to configure the Jetpack image before flashing it. Next use a tool to flash the image onto the SD card. One option is to use balenaEtcher that is available on Ubuntu, Mac and Windows. Info Flashing in this context, means the transfer of software data, also refered to as Firmware, from your computer to a device, such as a the sd card in this case. The term \u201cto flash\u201d comes from the Flash storage component of a device where the Firmware is stored.","title":"Installing the JetPack SDK"},{"location":"jetson-nano-setup/#power-supply","text":"There are four ways you can power the Jetson Nano. Provide 2 Amps at 5 Volts to the micro USB connector 4 Amps at 5 Volts to the barrel jack connector. 5 Volts on the GPIO headers, where each of the two 5 Volt pins can handle up to 3 A. The Nano has two of these 5 Volt pins, which means you could consume 6 A total at 5 V by having the two pins connected to a power supply that is capable of delivering 6 A at 5 V, corresponding to 30 Watts. Power Over Ethernet (POE) In its default power consumption state ('mode 0') the Nano itself uses 10 Watts, which is 2 A at 5 V. This, however, is only for the Jetson Nano compute module and doesn't include the peripherals. To provide power to peripherals, for example over the USB connectors, you need to operate the Nano itself in 5 Watts mode ('mode 1' or MAXN). This way there are 5 Watts left for peripherals via the micro USB connector, that provides 2 A at 5 V. The following table 2 shows the power modes for the Jetson Nano predefined by NVIDIA and the associated caps on use of the module's resources. Property MAXN 3 5W Power Budget 10 Watts 5 Watts Mode ID 0 1 Online CPU 4 2 CPU Maximal Frequency (MHz) 1479 918 GPU TPC 1 1 GPU Maximal Frequency (MHz) 921.6 640 Memory Maximal Frequency (MHz) 1600 1600 SOC clocks maximal frequency (MHz) All modes adsp 844.8 csi 750 se 627.2 ape 499.2 nvdec 716.8 tsec 408 host1x 408 nvenc 716.8 tsecb 627.2 isp 793.6 nvjpg 627.2 vi 793.6 display 665.6 pcie 500 vic03 627.2 Note In practice, when running from the micro-USB connector, we should be running in 5V mode to power the rest of the sensors, like the laser scanner. The drawback is that this will slow down the computations on the Jetson Nano. See the section below on how to do that. To develop and test code that requires high power, it is convenient to use the second option. To connect the 4 Amps @ 5 Volts barrel jack connector to the Jetson Nano a Jumper is required. Note Make sure to use a 5 V 4 Amps switching power supply. For example the Mean Well GST25E05-P1J or the AC/DC Desktop Adapter 5 V from Adafruit . DC barrel jack 5.5 mm OD / 2.1 mm ID / 9.5 mm length, center pin positive. Connect the jumper on J48. J48 is located between the Barrel Jack connector and the Camera connector. This jumper tells the Nano to use the Barrel Jack instead of the micro-USB port. Then plug the power supply into the Barrel Jack, and the Nano boots. TODO add image of Jumper location Note There are two variants of the Jetson Nano. The older A02 and the revised B01. Depending on the variant the location of the jumper is slightly different. - For an A02 carrier board (pre-2020) J48 is the solo header next to the camera port. - For a B01 carrier board (2020+, has two camera ports) J48 is a solo header behind the barrel jack and the HDMI port.","title":"Power Supply"},{"location":"jetson-nano-setup/#prepare-ubuntu","text":"After flashing the image to the sd card insert it to the Jetson Nano, hook it up to a monitor via HDMI and power it up by pluggin in the micro USB or even better barrel jack connector (don't forget the jumper on J48). The follow the instructions on the screen to setup Ubuntu 18.04 Bionic Beaver. For this you will need to accept the Nvidia End User License Agreement, set the desired language, keyboard, time zone, login credentials, APP Partition size (choose max possible), and delete unused bootloader that is done automatically with the new QSPI image (MaxSPI) of Jetpack 4.5. This QSPI update will take about two mins. Finally select the NVPModel mode explained above . First we go with the default MAXN 10 Watts mode ('mode 0'). We will changed this later at runtime - when powering the robot over the power bank - using the nvpmodel GUI or nvpmodel command line utility. Refer to the NVIDIA Jetson Linux Developer Guide for further information. Once finished, follow the next steps to install ROS Melodic. Note To proceed with the next steps on installing ROS and other related dependencies you can run a bash script. Just clone this repository: https://github.com/fjp/diffbot.git and run ubuntu-setup.sh . But to learn more, you should follow the instructions on the following pages.","title":"Prepare Ubuntu"},{"location":"jetson-nano-setup/#resources","text":"JetPack SDK Jetson Nano \u2013 Use More Power! - Jetson Hacks https://developer.nvidia.com/embedded/jetpack \u21a9 Nvidia Jetson Nano: Supported Modes and Power Efficiency \u21a9 The default mode is MAXN (power budget 10 watts, mode ID 0). \u21a9","title":"Resources"},{"location":"laser-range-scanner/","text":"Laser Range Scanner For SLAM (simultaneous localization and mapping) and navigation a laser range scanner is a cruical requirement to use the ROS navigation stack (see also diffbot_navigation ). DiffBot uses the 360 field of view RPLidar A2 M8 with 12 meter range. Mounting When installing the RPLidar on DiffBot make sure it is located high enough to avoid occluding the laser. It is also important to mount the laser in the correct location and that its direction and orientation matches the origin alignment in the robot description. In the figure below we can see, that the side with the communication cable is pointing in the positive x-direction. In case the cable should be mounted in the opposite direction, it is necessary to adapt the origin of the laser link in the robot description. Installation manual for RPLidar A2 (source: [robopeak/rplidar_ros/](https://github.com/robopeak/rplidar_ros/wiki/How-to-use-rplidar).","title":"Laser Range Scanner"},{"location":"laser-range-scanner/#laser-range-scanner","text":"For SLAM (simultaneous localization and mapping) and navigation a laser range scanner is a cruical requirement to use the ROS navigation stack (see also diffbot_navigation ). DiffBot uses the 360 field of view RPLidar A2 M8 with 12 meter range.","title":"Laser Range Scanner"},{"location":"laser-range-scanner/#mounting","text":"When installing the RPLidar on DiffBot make sure it is located high enough to avoid occluding the laser. It is also important to mount the laser in the correct location and that its direction and orientation matches the origin alignment in the robot description. In the figure below we can see, that the side with the communication cable is pointing in the positive x-direction. In case the cable should be mounted in the opposite direction, it is necessary to adapt the origin of the laser link in the robot description. Installation manual for RPLidar A2 (source: [robopeak/rplidar_ros/](https://github.com/robopeak/rplidar_ros/wiki/How-to-use-rplidar).","title":"Mounting"},{"location":"legal-notice/","text":"Legal Notice Provider Franz Pucher Carinagasse 8 6800 Feldkirch Austria Contact Options E-Mail Address: ros@fjp.at Phone: +4369917251989 Social Media Accounts and other Online Profiles Liability and Intellectual Property Rights Information Liability Disclaimer : While the content of this website has been put together with great care and reflects our current knowledge, it is provided for information purposes without being legally binding, unless the disclosure of this information is required by law (e.g. the legal information), the privacy policy, terms and conditions or mandatory instructions for consumers) . We reserve the right to modify or delete the content, whether in full or in part, provided this does not affect our existing contractual obligations. All website content is subject to change and non-binding. Link Disclaimer : We do not accept any responsibility for or endorse the content of external websites we link to, whether directly or indirectly. The providers of the linked websites are solely responsible for all content presented on their websites and in particular, any damage resulting from the use the information offered on their websites. Copyrights and Trademarks : All contents presented on this website, such as texts, photographs, graphics, brands and trademarks are protected by the respective intellectual property rights (copyrights, trademark rights). The use, reproduction, etc. are subject to our rights or the rights of the respective authors or rights owners. Information on legal infringements : Please notify us if you notice any rights violations on our website. Once notified, we will promptly remove any illegal content or links.","title":"Legal notice"},{"location":"lm393_speed_sensor/","text":"LM393 Speed Sensor - Odometry Note that this sensor is deprecated in the current version of DiffBot because it isn't a quadrature encoder. Instead, the DG01D-E motor includes a quadrature encoder that can measure ticks (can be converted to speed) and the direction the motor is turning (clock-wise or anti-clock-wise). The LM393 speed sensor could be used in combination with an additional information about the current driving direction, for example coming from the software. Howerver, this won't be as accurate as using a quadrature encoder that provides this information. To measure how far the robot has driven, we use the LM393 speed sensor from Joy-IT as odometry sensor. First, we will create a ROS package with catkin create pkg PKG_NAME [--catkin-deps [DEP [DEP ...]]] : 1 2 3 4 5 6 7 fjp@ubuntu:~/git/2wd-robot/ros/src$ catkin create pkg lm393_speed_sensor --catkin-deps rospy roscpp nav_msgs Creating package \"lm393_speed_sensor\" in \"/home/fjp/git/2wd-robot/ros/src\" ... Created file lm393_speed_sensor/CMakeLists.txt Created file lm393_speed_sensor/package.xml Created folder lm393_speed_sensor/include/lm393_speed_sensor Created folder lm393_speed_sensor/src Successfully created package files in /home/fjp/git/2wd-robot/ros/src/lm393_speed_sensor. The package depends on the two ROS client libraries rospy and roscpp . The current implementation uses python and the RPi.GPIO library for interrupts. To achieve more percise results, C++ should be used instead. To signalise the current pose of the robot in the odometry frame, the nav_msgs/Range message is used. Connection To get the speed sensors working, we connect the signal pins to (physical) GPIO 15 and (physical) GPIO 16 of the Raspberry Pi 4 B and power them with 3.3V. The ground pins are connected to ground of the Pi. LM393 Speed Sensor Library To use the LM393 speed sensor as a ROS node the sensor functionality is wraped in a class. This provides an easy to extend interface for the speed sensor ( API ) The code consists of a class LM393SpeedSensor which has two interrupt service routines (ISR) methods. Using the RPi.GPIO interrupt capabilities, these ISR methods are used as callback functions when the sensor measures a falling edge. This is the case when the rotary disk spins and the optocoupler measures a high to low signal due to the spinning disk. The sensor API is implemented in the lm393_speed_sensor.py python module. Executing the module will result in the following output when the motors are spinning freely with full speed and using some force to slow them down. We see that the rotational speed \\(\\omega\\) , measured in RPM ( revolutions per minute ) changes. 1 2 fjp@ubuntu:~/git/2wd-robot/ros/src/lm393_speed_sensor/src$ sudo python lm393_speed_sensor.py TODO The rotational speed \\(\\omega\\) values will be converted to tangential velocity values using the radius \\(r\\) of the wheels. \\[ v = \\omega \\cdot r = 2 \\pi n \\cdot r \\] ROS Node for LM393 Speed Sensor ROS provides the Odometry Message in the nav_msgs header . This message type can be used to write a wrapper that will act as a ROS node for the LM393 speed sensor. To design this node we will send out measurements periodically over a topic of type nav_msgs/Odometry . The code for this node is in speed_sensor.py . After writing the node we need to build the packages in the workspace with catkin build . 1 2 fjp@ubuntu:~/git/2wd-robot/ros$ catkin build TODO As the final note of the build output suggests, we have to source the setup.bash files in the devel space. 1 fjp@ubuntu:~/git/2wd-robot/ros$ source devel/setup.bash To make the speed_sensor node executable we have to modify the speed_sensor.py file: 1 fjp@ubuntu:~/git/2wd-robot/ros/src/grove_ultrasonic_ranger/src$ sudo chmod a+x speed_sensor.py Then we can test the node using rosrun : 1 2 3 4 fjp@ubuntu:~/git/2wd-robot/ros$ sudo su [ sudo ] password for fjp: root@ubuntu:/home/fjp/git/2wd-robot/ros# source devel/setup.bash root@ubuntu:/home/fjp/git/2wd-robot/ros# rosrun lm393_speed_sensor speed_sensor.py This lets the node publish range messages which we can capture in another terminal window using rostopic . First we use rostopic list to find the name of the topic we are interested in: 1 2 fjp@ubuntu:~/git/2wd-robot/ros$ rostopic list TODO We named our topic /odom which we can use with the rostopic echo command to see the published messages:","title":"Lm393 speed sensor"},{"location":"lm393_speed_sensor/#lm393-speed-sensor-odometry","text":"Note that this sensor is deprecated in the current version of DiffBot because it isn't a quadrature encoder. Instead, the DG01D-E motor includes a quadrature encoder that can measure ticks (can be converted to speed) and the direction the motor is turning (clock-wise or anti-clock-wise). The LM393 speed sensor could be used in combination with an additional information about the current driving direction, for example coming from the software. Howerver, this won't be as accurate as using a quadrature encoder that provides this information. To measure how far the robot has driven, we use the LM393 speed sensor from Joy-IT as odometry sensor. First, we will create a ROS package with catkin create pkg PKG_NAME [--catkin-deps [DEP [DEP ...]]] : 1 2 3 4 5 6 7 fjp@ubuntu:~/git/2wd-robot/ros/src$ catkin create pkg lm393_speed_sensor --catkin-deps rospy roscpp nav_msgs Creating package \"lm393_speed_sensor\" in \"/home/fjp/git/2wd-robot/ros/src\" ... Created file lm393_speed_sensor/CMakeLists.txt Created file lm393_speed_sensor/package.xml Created folder lm393_speed_sensor/include/lm393_speed_sensor Created folder lm393_speed_sensor/src Successfully created package files in /home/fjp/git/2wd-robot/ros/src/lm393_speed_sensor. The package depends on the two ROS client libraries rospy and roscpp . The current implementation uses python and the RPi.GPIO library for interrupts. To achieve more percise results, C++ should be used instead. To signalise the current pose of the robot in the odometry frame, the nav_msgs/Range message is used.","title":"LM393 Speed Sensor - Odometry"},{"location":"lm393_speed_sensor/#connection","text":"To get the speed sensors working, we connect the signal pins to (physical) GPIO 15 and (physical) GPIO 16 of the Raspberry Pi 4 B and power them with 3.3V. The ground pins are connected to ground of the Pi.","title":"Connection"},{"location":"lm393_speed_sensor/#lm393-speed-sensor-library","text":"To use the LM393 speed sensor as a ROS node the sensor functionality is wraped in a class. This provides an easy to extend interface for the speed sensor ( API ) The code consists of a class LM393SpeedSensor which has two interrupt service routines (ISR) methods. Using the RPi.GPIO interrupt capabilities, these ISR methods are used as callback functions when the sensor measures a falling edge. This is the case when the rotary disk spins and the optocoupler measures a high to low signal due to the spinning disk. The sensor API is implemented in the lm393_speed_sensor.py python module. Executing the module will result in the following output when the motors are spinning freely with full speed and using some force to slow them down. We see that the rotational speed \\(\\omega\\) , measured in RPM ( revolutions per minute ) changes. 1 2 fjp@ubuntu:~/git/2wd-robot/ros/src/lm393_speed_sensor/src$ sudo python lm393_speed_sensor.py TODO The rotational speed \\(\\omega\\) values will be converted to tangential velocity values using the radius \\(r\\) of the wheels. \\[ v = \\omega \\cdot r = 2 \\pi n \\cdot r \\]","title":"LM393 Speed Sensor Library"},{"location":"lm393_speed_sensor/#ros-node-for-lm393-speed-sensor","text":"ROS provides the Odometry Message in the nav_msgs header . This message type can be used to write a wrapper that will act as a ROS node for the LM393 speed sensor. To design this node we will send out measurements periodically over a topic of type nav_msgs/Odometry . The code for this node is in speed_sensor.py . After writing the node we need to build the packages in the workspace with catkin build . 1 2 fjp@ubuntu:~/git/2wd-robot/ros$ catkin build TODO As the final note of the build output suggests, we have to source the setup.bash files in the devel space. 1 fjp@ubuntu:~/git/2wd-robot/ros$ source devel/setup.bash To make the speed_sensor node executable we have to modify the speed_sensor.py file: 1 fjp@ubuntu:~/git/2wd-robot/ros/src/grove_ultrasonic_ranger/src$ sudo chmod a+x speed_sensor.py Then we can test the node using rosrun : 1 2 3 4 fjp@ubuntu:~/git/2wd-robot/ros$ sudo su [ sudo ] password for fjp: root@ubuntu:/home/fjp/git/2wd-robot/ros# source devel/setup.bash root@ubuntu:/home/fjp/git/2wd-robot/ros# rosrun lm393_speed_sensor speed_sensor.py This lets the node publish range messages which we can capture in another terminal window using rostopic . First we use rostopic list to find the name of the topic we are interested in: 1 2 fjp@ubuntu:~/git/2wd-robot/ros$ rostopic list TODO We named our topic /odom which we can use with the rostopic echo command to see the published messages:","title":"ROS Node for LM393 Speed Sensor"},{"location":"oak/","text":"OpenCV AI Kit The OpenCV AI Kit (OAK) is an open source hardware and software project for spatial AI and is a result of a very successful Kickstarter Campaign . The hardware can basically be separated into two devices: OAK\u2014D is a spatial AI powerhouse, capable of simultaneously running advanced neural networks while providing depth from two stereo cameras and color information from a single 4K camera in the center. The OAK\u2014D hardware comes with a 1 meter USB-C cable and a 5V power supply. OAK\u20141 is the tiny-but-mighty 4K camera capable of running the same advanced neural networks as OAK\u2014D, but in an even more minuscule form factor for projects where space and power are at a premium. Each OAK\u20141 kit includes the OAK\u20141 module with aluminum enclosure, 1 meter USB 3 Type-A to Type-C cable, and getting started guide. Note There are also options for onboard Wifi and Power over Ethernet (POE). !!! quote What is spacial AI and 3D Object Localization? First, it is necessary to define what 'Object Detection' is: It is the technical term for finding the bounding box of an object of interest, in pixel space (i.e. pixel coordinates), in an image. 1 2 3D Object Localization (or 3D Object Detection), is all about finding such objects in physical space, instead of pixel space. This is useful when trying to real-time measure or interact with the physical world. This part of the documentation explains the following Installing software for OAK on your host (Linux, macOS, Windows, Raspberry Pi) Interacting with OAK and how to run the demo programs Deploy a custom model with OpenVINO Toolkit Training your own Model with SuperAnnotate Train your own Model on a GPU, either locally or in the Cloud, using e.g. Tensorflow Keras. Project using OAK Setup of OAK-1 and OAK-D The documentation of OAK can be found at docs.luxonis.com . To install the software for both devices on your different host platforms please follow the instructions in the Python API . TODO install command for different os Test installation instructions Interaction with OAK Demo Program Overview Examples Object Detection with Yolo-v3 ... Python API: DepthAI Deplyinig Neural Network Models to OAK SuperAnnotate Training Custom Models","title":"OpenCV AI Kit"},{"location":"oak/#opencv-ai-kit","text":"The OpenCV AI Kit (OAK) is an open source hardware and software project for spatial AI and is a result of a very successful Kickstarter Campaign . The hardware can basically be separated into two devices: OAK\u2014D is a spatial AI powerhouse, capable of simultaneously running advanced neural networks while providing depth from two stereo cameras and color information from a single 4K camera in the center. The OAK\u2014D hardware comes with a 1 meter USB-C cable and a 5V power supply. OAK\u20141 is the tiny-but-mighty 4K camera capable of running the same advanced neural networks as OAK\u2014D, but in an even more minuscule form factor for projects where space and power are at a premium. Each OAK\u20141 kit includes the OAK\u20141 module with aluminum enclosure, 1 meter USB 3 Type-A to Type-C cable, and getting started guide. Note There are also options for onboard Wifi and Power over Ethernet (POE). !!! quote What is spacial AI and 3D Object Localization? First, it is necessary to define what 'Object Detection' is: It is the technical term for finding the bounding box of an object of interest, in pixel space (i.e. pixel coordinates), in an image. 1 2 3D Object Localization (or 3D Object Detection), is all about finding such objects in physical space, instead of pixel space. This is useful when trying to real-time measure or interact with the physical world. This part of the documentation explains the following Installing software for OAK on your host (Linux, macOS, Windows, Raspberry Pi) Interacting with OAK and how to run the demo programs Deploy a custom model with OpenVINO Toolkit Training your own Model with SuperAnnotate Train your own Model on a GPU, either locally or in the Cloud, using e.g. Tensorflow Keras. Project using OAK","title":"OpenCV AI Kit"},{"location":"oak/#setup-of-oak-1-and-oak-d","text":"The documentation of OAK can be found at docs.luxonis.com . To install the software for both devices on your different host platforms please follow the instructions in the Python API . TODO install command for different os Test installation instructions","title":"Setup of OAK-1 and OAK-D"},{"location":"oak/#interaction-with-oak","text":"Demo Program Overview Examples Object Detection with Yolo-v3 ...","title":"Interaction with OAK"},{"location":"oak/#python-api-depthai","text":"","title":"Python API: DepthAI"},{"location":"oak/#deplyinig-neural-network-models-to-oak","text":"","title":"Deplyinig Neural Network Models to OAK"},{"location":"oak/#superannotate","text":"","title":"SuperAnnotate"},{"location":"oak/#training-custom-models","text":"","title":"Training Custom Models"},{"location":"packages/","text":"Diffbot ROS Packages The following describes the easiest way to make use of diffbot's ROS packages inside the ros-mobile-robots/diffbot repository. The following steps will be performed on both, the workstation/development PC and the single board computer (SBC). Git: clone diffbot repository After setting up ROS on your workstation PC and the SBC (either Raspberry Pi 4B or Jetson Nano ), create a ros workspace in your users home folder and clone the diffbot repository : 1 2 mkdir -p ros_ws/src git clone https://github.com/ros-mobile-robots/diffbot.git Obtain (system) Dependencies The diffbot repository relies on two sorts of dependencies: Source (non binary) dependencies from other (git) repositories. System dependencies available in the (ROS) Ubuntu package repositories. Also refered to as pre built binaries. Source Dependencies Let's first obtain source dependencies from other repositories. To do this the recommended tool to use is vcstool (see also https://github.com/dirk-thomas/vcstool for additional documentation and examples.). Note vcstool replaces wstool . Inside the cloned diffbot repository, make use of the import command and the diffbot.repos file containing the required source repositories: 1 vcs import < diffbot.repos This will clone all repositories which are stored in the diffbot.repos that get passed in via stdin in YAML format. Note The file diffbot.repos contains relative paths and will clone the listed repositories in the parent folder from where the vcs import command is called. When it is called from inside the diffbot repository, which should be located in the src folder of a catkin workspace, then the other repositories are also cloned in the src folder. For the SBC not all dependencies in diffbot.repos are needed. Instead the diffbot_robot.repos is here to clone the rplidar_ros repository. 1 vcs import < diffbot_robot.repos Now that additional packages are inside the catkin workspace it is time to install the system dependencies. System Dependencies All the needed ROS system dependencies which are required by diffbot's packages can be installed using rosdep command, which was installed during the ROS setup. To install all system dependencies use the following command: 1 rosdep install --from-paths src --ignore-src -r -y Info On the following packages pages it is explained that the dependencies of a ROS package are defined inside its package.xml . After the installation of all dependencies finished (which can take a while), it is time to build the catkin workspace. Inside the workspace use catkin-tools to build the packages inside the src folder. Note The first time you run the following command, make sure to execute it inside your catkin workspace and not the src directory. 1 catkin build Now source the catkin workspace either using the created alias or the full command for the bash shell: 1 source devel/setup.bash Examples Now you are ready to follow the examples listed in the readme. Info TODO extend documentation with examples","title":"Packages"},{"location":"packages/#diffbot-ros-packages","text":"The following describes the easiest way to make use of diffbot's ROS packages inside the ros-mobile-robots/diffbot repository. The following steps will be performed on both, the workstation/development PC and the single board computer (SBC).","title":"Diffbot ROS Packages"},{"location":"packages/#git-clone-diffbot-repository","text":"After setting up ROS on your workstation PC and the SBC (either Raspberry Pi 4B or Jetson Nano ), create a ros workspace in your users home folder and clone the diffbot repository : 1 2 mkdir -p ros_ws/src git clone https://github.com/ros-mobile-robots/diffbot.git","title":"Git: clone diffbot repository"},{"location":"packages/#obtain-system-dependencies","text":"The diffbot repository relies on two sorts of dependencies: Source (non binary) dependencies from other (git) repositories. System dependencies available in the (ROS) Ubuntu package repositories. Also refered to as pre built binaries.","title":"Obtain (system) Dependencies"},{"location":"packages/#source-dependencies","text":"Let's first obtain source dependencies from other repositories. To do this the recommended tool to use is vcstool (see also https://github.com/dirk-thomas/vcstool for additional documentation and examples.). Note vcstool replaces wstool . Inside the cloned diffbot repository, make use of the import command and the diffbot.repos file containing the required source repositories: 1 vcs import < diffbot.repos This will clone all repositories which are stored in the diffbot.repos that get passed in via stdin in YAML format. Note The file diffbot.repos contains relative paths and will clone the listed repositories in the parent folder from where the vcs import command is called. When it is called from inside the diffbot repository, which should be located in the src folder of a catkin workspace, then the other repositories are also cloned in the src folder. For the SBC not all dependencies in diffbot.repos are needed. Instead the diffbot_robot.repos is here to clone the rplidar_ros repository. 1 vcs import < diffbot_robot.repos Now that additional packages are inside the catkin workspace it is time to install the system dependencies.","title":"Source Dependencies"},{"location":"packages/#system-dependencies","text":"All the needed ROS system dependencies which are required by diffbot's packages can be installed using rosdep command, which was installed during the ROS setup. To install all system dependencies use the following command: 1 rosdep install --from-paths src --ignore-src -r -y Info On the following packages pages it is explained that the dependencies of a ROS package are defined inside its package.xml . After the installation of all dependencies finished (which can take a while), it is time to build the catkin workspace. Inside the workspace use catkin-tools to build the packages inside the src folder. Note The first time you run the following command, make sure to execute it inside your catkin workspace and not the src directory. 1 catkin build Now source the catkin workspace either using the created alias or the full command for the bash shell: 1 source devel/setup.bash","title":"System Dependencies"},{"location":"packages/#examples","text":"Now you are ready to follow the examples listed in the readme. Info TODO extend documentation with examples","title":"Examples"},{"location":"power-supply/","text":"Power Supply DiffBot uses two different power sources. One USB type C power bank providing 5V to the Raspberry Pi 4B. The other source comes from battery holder, mounted at the bottom of the base plate which can hold up to four AA sized batteries. The standard AA battery has a charge of 2700 mAh and a nominal Voltage of 1.5 V. This means that with four batteries in series the total Voltage supplied to the motor driver would be 6 V. The voltage drop on the motor driver is roughly 2.12 V which means that each motor would operate on 3.88 V. According to the datasheet the DG01D-E motor requires a voltage between 3-9 V, has a gearbox ratio of 1:48 and a speed of 90RPM at 4.5V. Therefore 3.88 V is at the lower operating range of this motor, hence a higher input source to the motor driver is requried to provide higher output voltage to the motors. The maximum allowed input Voltage to the motor driver is 15 V, hence a power source between 6 V to 15 V should be used, in the range of 10 V to 15 V to drive with different speeds. In order to reuse the battery holder on the bottom plate a battery with the same AA form factor but higher voltage is what we are looking for. A helpful overview of different batteris with their exact naming can be found in the List of battery sizes on Wikipedia. Here we can see that the 14500 recharchable Lithium-ion battery provides 3.7 V with a capacity of approximately 900 mAh. This means that four batteries of these provide 14.8 V which is right below the maximum 15 V that the motor driver can handle. Although the capacity is lower compared to non recharchable AA batteries (2700 mAh), these type of batteries can be reused multiple times when charged with a suitable battery charchger such as the Nitecore UMS4 or the XTAR VC4 . DiffBot uses four 14500 batteries from Trustfire with 3.7 V and 900 mAh. Note that four standard AA bateries with 1.5 V each is perfectly fine. Only the motor driver has to provide 100% of its input voltage to avoid stalling the motors. With higher Voltage rated batteries more different speeds will be possible.","title":"Power Supply"},{"location":"power-supply/#power-supply","text":"DiffBot uses two different power sources. One USB type C power bank providing 5V to the Raspberry Pi 4B. The other source comes from battery holder, mounted at the bottom of the base plate which can hold up to four AA sized batteries. The standard AA battery has a charge of 2700 mAh and a nominal Voltage of 1.5 V. This means that with four batteries in series the total Voltage supplied to the motor driver would be 6 V. The voltage drop on the motor driver is roughly 2.12 V which means that each motor would operate on 3.88 V. According to the datasheet the DG01D-E motor requires a voltage between 3-9 V, has a gearbox ratio of 1:48 and a speed of 90RPM at 4.5V. Therefore 3.88 V is at the lower operating range of this motor, hence a higher input source to the motor driver is requried to provide higher output voltage to the motors. The maximum allowed input Voltage to the motor driver is 15 V, hence a power source between 6 V to 15 V should be used, in the range of 10 V to 15 V to drive with different speeds. In order to reuse the battery holder on the bottom plate a battery with the same AA form factor but higher voltage is what we are looking for. A helpful overview of different batteris with their exact naming can be found in the List of battery sizes on Wikipedia. Here we can see that the 14500 recharchable Lithium-ion battery provides 3.7 V with a capacity of approximately 900 mAh. This means that four batteries of these provide 14.8 V which is right below the maximum 15 V that the motor driver can handle. Although the capacity is lower compared to non recharchable AA batteries (2700 mAh), these type of batteries can be reused multiple times when charged with a suitable battery charchger such as the Nitecore UMS4 or the XTAR VC4 . DiffBot uses four 14500 batteries from Trustfire with 3.7 V and 900 mAh. Note that four standard AA bateries with 1.5 V each is perfectly fine. Only the motor driver has to provide 100% of its input voltage to avoid stalling the motors. With higher Voltage rated batteries more different speeds will be possible.","title":"Power Supply"},{"location":"privacy-policy/","text":"Privacy Policy Preamble With the following privacy policy we would like to inform you which types of your personal data (hereinafter also abbreviated as \"data\") we process for which purposes and in which scope. The privacy statement applies to all processing of personal data carried out by us, both in the context of providing our services and in particular on our websites, in mobile applications and within external online presences, such as our social media profiles (hereinafter collectively referred to as \"online services\"). The terms used are not gender-specific. Last Update: 5. July 2021 Table of contents Preamble Controller Overview of processing operations Legal Bases for the Processing Security Precautions Transmission of Personal Data Data Processing in Third Countries Erasure of data Use of Cookies Business services Payment Procedure Provision of online services and web hosting Newsletter and Electronic Communications Web Analysis, Monitoring and Optimization Online Marketing Affiliate-Programms und Affiliate-Links Profiles in Social Networks (Social Media) Plugins and embedded functions and content Changes and Updates to the Privacy Policy Rights of Data Subjects Terminology and Definitions Controller Franz Pucher Carinagasse 8 6800, Feldkirch Austria E-mail address: ros@fjp.at . Legal Notice: impressum url. Overview of processing operations The following table summarises the types of data processed, the purposes for which they are processed and the concerned data subjects. Categories of Processed Data Inventory data (e.g. names, addresses). Content data (e.g. text input, photographs, videos). Contact data (e.g. e-mail, telephone numbers). Meta/communication data (e.g. device information, IP addresses). Usage data (e.g. websites visited, interest in content, access times). Contract data (e.g. contract object, duration, customer category). Payment Data (e.g. bank details, invoices, payment history). Categories of Data Subjects Business and contractual partners. Prospective customers. Communication partner (Recipients of e-mails, letters, etc.). Customers. Users (e.g. website visitors, users of online services). Students/ Participants. Purposes of Processing Affiliate Tracking. Provision of our online services and usability. Conversion tracking (Measurement of the effectiveness of marketing activities). Office and organisational procedures. Direct marketing (e.g. by e-mail or postal). Feedback (e.g. collecting feedback via online form). Marketing. Contact requests and communication. Profiles with user-related information (Creating user profiles). Web Analytics (e.g. access statistics, recognition of returning visitors). Security measures. Provision of contractual services and customer support. Managing and responding to inquiries. Legal Bases for the Processing In the following, you will find an overview of the legal basis of the GDPR on which we base the processing of personal data. Please note that in addition to the provisions of the GDPR, national data protection provisions of your or our country of residence or domicile may apply. If, in addition, more specific legal bases are applicable in individual cases, we will inform you of these in the data protection declaration. Consent (Article 6 (1) (a) GDPR) - The data subject has given consent to the processing of his or her personal data for one or more specific purposes. Performance of a contract and prior requests (Article 6 (1) (b) GDPR) - Performance of a contract to which the data subject is party or in order to take steps at the request of the data subject prior to entering into a contract. Compliance with a legal obligation (Article 6 (1) (c) GDPR) - Processing is necessary for compliance with a legal obligation to which the controller is subject. Legitimate Interests (Article 6 (1) (f) GDPR) - Processing is necessary for the purposes of the legitimate interests pursued by the controller or by a third party, except where such interests are overridden by the interests or fundamental rights and freedoms of the data subject which require protection of personal data. National data protection regulations in Austria : In addition to the data protection regulations of the General Data Protection Regulation, national regulations apply to data protection in Austria. This includes in particular the Federal Act on the Protection of Individuals with regard to the Processing of Personal Data (Data Protection Act - DSG). In particular, the Data Protection Act contains special provisions on the right of access, rectification or cancellation, processing of special categories of personal data, processing for other purposes and transmission and automated decision making in individual cases. Security Precautions We take appropriate technical and organisational measures in accordance with the legal requirements, taking into account the state of the art, the costs of implementation and the nature, scope, context and purposes of processing as well as the risk of varying likelihood and severity for the rights and freedoms of natural persons, in order to ensure a level of security appropriate to the risk. The measures include, in particular, safeguarding the confidentiality, integrity and availability of data by controlling physical and electronic access to the data as well as access to, input, transmission, securing and separation of the data. In addition, we have established procedures to ensure that data subjects' rights are respected, that data is erased, and that we are prepared to respond to data threats rapidly. Furthermore, we take the protection of personal data into account as early as the development or selection of hardware, software and service providers, in accordance with the principle of privacy by design and privacy by default. Masking of the IP address : If IP addresses are processed by us or by the service providers and technologies used and the processing of a complete IP address is not necessary, the IP address is shortened (also referred to as \"IP masking\"). In this process, the last two digits or the last part of the IP address after a full stop are removed or replaced by wildcards. The masking of the IP address is intended to prevent the identification of a person by means of their IP address or to make such identification significantly more difficult. SSL encryption (https) : In order to protect your data transmitted via our online services in the best possible way, we use SSL encryption. You can recognize such encrypted connections by the prefix https:// in the address bar of your browser. Transmission of Personal Data In the context of our processing of personal data, it may happen that the data is transferred to other places, companies or persons or that it is disclosed to them. Recipients of this data may include, for example, service providers commissioned with IT tasks or providers of services and content that are embedded in a website. In such a case, the legal requirements will be respected and in particular corresponding contracts or agreements, which serve the protection of your data, will be concluded with the recipients of your data. Data Processing in Third Countries If we process data in a third country (i.e. outside the European Union (EU), the European Economic Area (EEA)) or the processing takes place in the context of the use of third party services or disclosure or transfer of data to other persons, bodies or companies, this will only take place in accordance with the legal requirements. Subject to express consent or transfer required by contract or law, we process or have processed the data only in third countries with a recognised level of data protection, on the basis of special guarantees, such as a contractual obligation through so-called standard protection clauses of the EU Commission or if certifications or binding internal data protection regulations justify the processing (Article 44 to 49 GDPR, information page of the EU Commission: https://ec.europa.eu/info/law/law-topic/data-protection/international-dimension-data-protection_en ). Erasure of data The data processed by us will be erased in accordance with the statutory provisions as soon as their processing is revoked or other permissions no longer apply (e.g. if the purpose of processing this data no longer applies or they are not required for the purpose). If the data is not deleted because they are required for other and legally permissible purposes, their processing is limited to these purposes. This means that the data will be restricted and not processed for other purposes. This applies, for example, to data that must be stored for commercial or tax reasons or for which storage is necessary to assert, exercise or defend legal claims or to protect the rights of another natural or legal person. In the context of our information on data processing, we may provide users with further information on the deletion and retention of data that is specific to the respective processing operation. Use of Cookies Cookies are text files that contain data from visited websites or domains and are stored by a browser on the user's computer. A cookie is primarily used to store information about a user during or after his visit within an online service. The information stored can include, for example, the language settings on a website, the login status, a shopping basket or the location where a video was viewed. The term \"cookies\" also includes other technologies that fulfil the same functions as cookies (e.g. if user information is stored using pseudonymous online identifiers, also referred to as \"user IDs\"). The following types and functions of cookies are distinguished: Temporary cookies (also: session cookies): Temporary cookies are deleted at the latest after a user has left an online service and closed his browser. Permanent cookies: Permanent cookies remain stored even after closing the browser. For example, the login status can be saved or preferred content can be displayed directly when the user visits a website again. The interests of users who are used for range measurement or marketing purposes can also be stored in such a cookie. First-Party-Cookies : First-Party-Cookies are set by ourselves. Third party cookies : Third party cookies are mainly used by advertisers (so-called third parties) to process user information. Necessary (also: essential) cookies: Cookies can be necessary for the operation of a website (e.g. to save logins or other user inputs or for security reasons). Statistics, marketing and personalisation cookies : Cookies are also generally used to measure a website's reach and when a user's interests or behaviour (e.g. viewing certain content, using functions, etc.) are stored on individual websites in a user profile. Such profiles are used, for example, to display content to users that corresponds to their potential interests. This procedure is also referred to as \"tracking\", i.e. tracking the potential interests of users. If we use cookies or \"tracking\" technologies, we will inform you separately in our privacy policy or in the context of obtaining consent. Information on legal basis: The legal basis on which we process your personal data with the help of cookies depends on whether we ask you for your consent. If this applies and you consent to the use of cookies, the legal basis for processing your data is your declared consent. Otherwise, the data processed with the help of cookies will be processed on the basis of our legitimate interests (e.g. in a business operation of our online service and its improvement) or, if the use of cookies is necessary to fulfill our contractual obligations. Retention period: Unless we provide you with explicit information on the retention period of permanent cookies (e.g. within the scope of a so-called cookie opt-in), please assume that the retention period can be as long as two years. General information on Withdrawal of consent and objection (Opt-Out): Respective of whether processing is based on consent or legal permission, you have the option at any time to object to the processing of your data using cookie technologies or to revoke consent (collectively referred to as \"opt-out\"). You can initially explain your objection using the settings of your browser, e.g. by deactivating the use of cookies (which may also restrict the functionality of our online services). An objection to the use of cookies for online marketing purposes can be raised for a large number of services, especially in the case of tracking, via the websites https://www.aboutads.info/choices/ and https://www.youronlinechoices.com. In addition, you can receive further information on objections in the context of the information on the used service providers and cookies. Processing Cookie Data on the Basis of Consent : We use a cookie management solution in which users' consent to the use of cookies, or the procedures and providers mentioned in the cookie management solution, can be obtained, managed and revoked by the users. The declaration of consent is stored so that it does not have to be retrieved again and the consent can be proven in accordance with the legal obligation. Storage can take place server-sided and/or in a cookie (so-called opt-out cookie or with the aid of comparable technologies) in order to be able to assign the consent to a user or and/or his/her device.Subject to individual details of the providers of cookie management services, the following information applies: The duration of the storage of the consent can be up to two years. In this case, a pseudonymous user identifier is formed and stored with the date/time of consent, information on the scope of the consent (e.g. which categories of cookies and/or service providers) as well as the browser, system and used end device. Processed data types: Usage data (e.g. websites visited, interest in content, access times), Meta/communication data (e.g. device information, IP addresses). Data subjects: Users (e.g. website visitors, users of online services). Legal Basis: Consent (Article 6 (1) (a) GDPR), Legitimate Interests (Article 6 (1) (f) GDPR). Business services We process data of our contractual and business partners, e.g. customers and interested parties (collectively referred to as \"contractual partners\") within the context of contractual and comparable legal relationships as well as associated actions and communication with the contractual partners or pre-contractually, e.g. to answer inquiries. We process this data in order to fulfil our contractual obligations, safeguard our rights and for the purposes of the administrative tasks associated with this data and the business-related organisation. We will only pass on the data of the contractual partners within the scope of the applicable law to third parties insofar as this is necessary for the aforementioned purposes or for the fulfilment of legal obligations or with the consent of data subjects concerned (e.g. telecommunications, transport and other auxiliary services as well as subcontractors, banks, tax and legal advisors, payment service providers or tax authorities). The contractual partners will be informed about further processing, e.g. for marketing purposes, as part of this privacy policy. Which data are necessary for the aforementioned purposes, we inform the contracting partners before or in the context of the data collection, e.g. in online forms by special marking (e.g. colors), and/or symbols (e.g. asterisks or the like), or personally. We delete the data after expiry of statutory warranty and comparable obligations, i.e. in principle after expiry of 4 years, unless the data is stored in a customer account or must be kept for legal reasons of archiving (e.g., as a rule 10 years for tax purposes). In the case of data disclosed to us by the contractual partner within the context of an assignment, we delete the data in accordance with the specifications of the assignment, in general after the end of the assignment. If we use third-party providers or platforms to provide our services, the terms and conditions and privacy policies of the respective third-party providers or platforms shall apply in the relationship between the users and the providers. Online Shop and E-Commerce : We process the data of our customers in order to enable them to select, purchase or order the selected products, goods and related services, as well as their payment and delivery, or performance of other services. If necessary for the execution of an order, we use service providers, in particular postal, freight and shipping companies, in order to carry out the delivery or execution to our customers. For the processing of payment transactions we use the services of banks and payment service providers. The required details are identified as such in the course of the ordering or comparable purchasing process and include the details required for delivery, or other way of making the product aviable and invoicing as well as contact information in order to be able to hold any consultation. Education and Training Services : We process the data of the participants of our education and training programmes (uniformly referred to as \" students\") in order to provide them with our educational and training services. The data processed, the type, scope and purpose of the processing and the necessity of its processing are determined by the underlying contractual and educational relationship. The processing also includes the performance evaluation and evaluation of our services and the teachers and instructors. As part of our activities, we may also process special categories of data, in particular information on the health of persons undergoing training or further training and data revealing ethnic origin, political opinions, religious or philosophical convictions. To this end, we obtain, if necessary, the express consent of the students to be trained and further educated and process the special categories of data otherwise only if it is necessary for the provision of training services, for purposes of health care, social protection or protection of vital interests of the students to be trained and further educated. Insofar as it is necessary for the fulfilment of our contract, for the protection of vital interests or by law, or with the consent of the trainees, we disclose or transfer the data of the students to third parties or agents, e.g. public authorities or in the field of IT, office or comparable services, in compliance with the requirements of professional law. Consulting : We process the data of our clients, clients as well as interested parties and other clients or contractual partners (uniformly referred to as \"clients\") in order to provide them with our consulting services. The data processed, the type, scope and purpose of the processing and the necessity of its processing are determined by the underlying contractual and client relationship. Insofar as it is necessary for the fulfilment of our contract, for the protection of vital interests or by law, or with the consent of the client, we disclose or transfer the client's data to third parties or agents, such as authorities, courts, subcontractors or in the field of IT, office or comparable services, taking into account the professional requirements. Processed data types: Inventory data (e.g. names, addresses), Payment Data (e.g. bank details, invoices, payment history), Contact data (e.g. e-mail, telephone numbers), Contract data (e.g. contract object, duration, customer category), Usage data (e.g. websites visited, interest in content, access times), Meta/communication data (e.g. device information, IP addresses). Data subjects: Prospective customers, Business and contractual partners, Customers, Students/ Participants. Purposes of Processing: Provision of contractual services and customer support, Contact requests and communication, Office and organisational procedures, Managing and responding to inquiries, Security measures. Legal Basis: Performance of a contract and prior requests (Article 6 (1) (b) GDPR), Compliance with a legal obligation (Article 6 (1) (c) GDPR), Legitimate Interests (Article 6 (1) (f) GDPR). Payment Procedure Within the framework of contractual and other legal relationships, due to legal obligations or otherwise on the basis of our legitimate interests, we offer data subjects efficient and secure payment options and use other service providers for this purpose in addition to banks and credit institutions (collectively referred to as \"payment service providers\"). The data processed by the payment service providers includes inventory data, such as the name and address, bank data, such as account numbers or credit card numbers, passwords, TANs and checksums, as well as the contract, total and recipient-related information. The information is required to carry out the transactions. However, the data entered is only processed by the payment service providers and stored with them. I.e. we do not receive any account or credit card related information, but only information with confirmation or negative information of the payment. Under certain circumstances, the data may be transmitted by the payment service providers to credit agencies. The purpose of this transmission is to check identity and creditworthiness. Please refer to the terms and conditions and data protection information of the payment service providers. The terms and conditions and data protection information of the respective payment service providers apply to the payment transactions and can be accessed within the respective websites or transaction applications. We also refer to these for further information and the assertion of revocation, information and other data subject rights. Processed data types: Inventory data (e.g. names, addresses), Payment Data (e.g. bank details, invoices, payment history), Contract data (e.g. contract object, duration, customer category), Usage data (e.g. websites visited, interest in content, access times), Meta/communication data (e.g. device information, IP addresses). Data subjects: Customers, Prospective customers. Purposes of Processing: Provision of contractual services and customer support. Legal Basis: Performance of a contract and prior requests (Article 6 (1) (b) GDPR), Legitimate Interests (Article 6 (1) (f) GDPR). Services and service providers being used: Amazon Payments: Payment service provider; Service provider: Amazon Payments Europe S.C.A. 38 avenue J.F. Kennedy, L-1855 Luxembourg; Website: https://pay.amazon.com ; Privacy Policy: https://pay.amazon.com/us/help/201212490 . PayPal: Payment service provider (e.g. PayPal, PayPal Plus, Braintree, Braintree); Service provider: PayPal (Europe) S.\u00e0 r.l. et Cie, S.C.A., 22-24 Boulevard Royal, L-2449 Luxembourg; Website: https://www.paypal.com ; Privacy Policy: https://www.paypal.com/de/webapps/mpp/ua/privacy-full . Stripe: Payment service provider; Service provider: Stripe, Inc., 510 Townsend Street, San Francisco, CA 94103, USA; Website: https://stripe.com/de ; Privacy Policy: https://stripe.com/en-de/privacy . Provision of online services and web hosting In order to provide our online services securely and efficiently, we use the services of one or more web hosting providers from whose servers (or servers they manage) the online services can be accessed. For these purposes, we may use infrastructure and platform services, computing capacity, storage space and database services, as well as security and technical maintenance services. The data processed within the framework of the provision of the hosting services may include all information relating to the users of our online services that is collected in the course of use and communication. This regularly includes the IP address, which is necessary to be able to deliver the contents of online services to browsers, and all entries made within our online services or from websites. Collection of Access Data and Log Files : We, ourselves or our web hosting provider, collect data on the basis of each access to the server (so-called server log files). Server log files may include the address and name of the web pages and files accessed, the date and time of access, data volumes transferred, notification of successful access, browser type and version, the user's operating system, referrer URL (the previously visited page) and, as a general rule, IP addresses and the requesting provider. The server log files can be used for security purposes, e.g. to avoid overloading the servers (especially in the case of abusive attacks, so-called DDoS attacks) and to ensure the stability and optimal load balancing of the servers . Processed data types: Content data (e.g. text input, photographs, videos), Usage data (e.g. websites visited, interest in content, access times), Meta/communication data (e.g. device information, IP addresses). Data subjects: Users (e.g. website visitors, users of online services). Purposes of Processing: Provision of our online services and usability. Legal Basis: Legitimate Interests (Article 6 (1) (f) GDPR). Services and service providers being used: JSDelivr: Content Delivery Network (CDN) that helps deliver media and files quickly and efficiently, especially under heavy load. Service provider: ProspectOne, Kr\u00f3lewska 65A/1, 30-081, Krak\u00f3w, Poland; Website: https://www.jsdelivr.com ; Privacy Policy: https://www.jsdelivr.com/terms/privacy-policy-jsdelivr-net . Newsletter and Electronic Communications We send newsletters, e-mails and other electronic communications (hereinafter referred to as \"newsletters\") only with the consent of the recipient or a legal permission. Insofar as the contents of the newsletter are specifically described within the framework of registration, they are decisive for the consent of the user. Otherwise, our newsletters contain information about our services and us. In order to subscribe to our newsletters, it is generally sufficient to enter your e-mail address. We may, however, ask you to provide a name for the purpose of contacting you personally in the newsletter or to provide further information if this is required for the purposes of the newsletter. Double opt-in procedure: The registration to our newsletter takes place in general in a so-called Double-Opt-In procedure. This means that you will receive an e-mail after registration asking you to confirm your registration. This confirmation is necessary so that no one can register with external e-mail addresses. The registrations for the newsletter are logged in order to be able to prove the registration process according to the legal requirements. This includes storing the login and confirmation times as well as the IP address. Likewise the changes of your data stored with the dispatch service provider are logged. Deletion and restriction of processing: We may store the unsubscribed email addresses for up to three years based on our legitimate interests before deleting them to provide evidence of prior consent. The processing of these data is limited to the purpose of a possible defense against claims. An individual deletion request is possible at any time, provided that the former existence of a consent is confirmed at the same time. In the case of an obligation to permanently observe an objection, we reserve the right to store the e-mail address solely for this purpose in a blocklist. Information on legal bases: The sending of the newsletter is based on the consent of the recipients or, if consent is not required, on the basis of our legitimate interests in direct marketing. Insofar as we engage a service provider for sending e-mails, this is done on the basis of our legitimate interests. The registration procedure is recorded on the basis of our legitimate interests for the purpose of demonstrating that it has been conducted in accordance with the law. Contents: Information about us, our services, promotions and offers. Measurement of opening rates and click rates : The newsletters contain a so-called \"web-beacon\", i.e. a pixel-sized file, which is retrieved from our server when the newsletter is opened or, if we use a mailing service provider, from its server. Within the scope of this retrieval, technical information such as information about the browser and your system, as well as your IP address and time of retrieval are first collected. This information is used for the technical improvement of our newsletter on the basis of technical data or target groups and their reading behaviour on the basis of their retrieval points (which can be determined with the help of the IP address) or access times. This analysis also includes determining whether newsletters are opened, when they are opened and which links are clicked. This information is assigned to the individual newsletter recipients and stored in their profiles until the profiles are deleted. The evaluations serve us much more to recognize the reading habits of our users and to adapt our content to them or to send different content according to the interests of our users. The measurement of opening rates and click rates as well as the storage of the measurement results in the profiles of the users and their further processing are based on the consent of the users. A separate objection to the performance measurement is unfortunately not possible, in this case the entire newsletter subscription must be cancelled or objected to. In this case, the stored profile information will be deleted. Processed data types: Inventory data (e.g. names, addresses), Contact data (e.g. e-mail, telephone numbers), Meta/communication data (e.g. device information, IP addresses), Usage data (e.g. websites visited, interest in content, access times). Data subjects: Communication partner (Recipients of e-mails, letters, etc.), Users (e.g. website visitors, users of online services). Purposes of Processing: Direct marketing (e.g. by e-mail or postal), Web Analytics (e.g. access statistics, recognition of returning visitors), Conversion tracking (Measurement of the effectiveness of marketing activities), Profiles with user-related information (Creating user profiles). Legal Basis: Consent (Article 6 (1) (a) GDPR), Legitimate Interests (Article 6 (1) (f) GDPR). Opt-Out: You can cancel the receipt of our newsletter at any time, i.e. revoke your consent or object to further receipt. You will find a link to cancel the newsletter either at the end of each newsletter or you can otherwise use one of the contact options listed above, preferably e-mail. Services and service providers being used: Google Analytics: Measuring the success of email campaigns and building user profiles with a storage period of up to two years; Service provider: Google Ireland Limited, Gordon House, Barrow Street, Dublin 4, Ireland, parent company: Google LLC, 1600 Amphitheatre Parkway, Mountain View, CA 94043, USA; Website: https://marketingplatform.google.com/intl/en/about/analytics/ ; Privacy Policy: https://policies.google.com/privacy ; Opt-Out: Opt-Out-Plugin: https://tools.google.com/dlpage/gaoptout?hl=en , Settings for the Display of Advertisements: https://adssettings.google.com/authenticated . HubSpot: Email marketing platform; Service provider: HubSpot, Inc., 25 First St., 2nd floor, Cambridge, Massachusetts 02141, USA; Website: https://www.hubspot.com ; Privacy Policy: https://legal.hubspot.com/privacy-policy . Web Analysis, Monitoring and Optimization Web analysis is used to evaluate the visitor traffic on our website and may include the behaviour, interests or demographic information of users, such as age or gender, as pseudonymous values. With the help of web analysis we can e.g. recognize, at which time our online services or their functions or contents are most frequently used or requested for repeatedly, as well as which areas require optimization. In addition to web analysis, we can also use test procedures, e.g. to test and optimize different versions of our online services or their components. For these purposes, so-called user profiles can be created and stored in a file (so-called \"cookie\") or similar procedures in which the relevant user information for the aforementioned analyses is stored. This information may include, for example, content viewed, web pages visited and elements and technical data used there, such as the browser used, computer system used and information on times of use. If users have consented to the collection of their location data, these may also be processed, depending on the provider. The IP addresses of the users are also stored. However, we use any existing IP masking procedure (i.e. pseudonymisation by shortening the IP address) to protect the user. In general, within the framework of web analysis, A/B testing and optimisation, no user data (such as e-mail addresses or names) is stored, but pseudonyms. This means that we, as well as the providers of the software used, do not know the actual identity of the users, but only the information stored in their profiles for the purposes of the respective processes. Information on legal basis: If we ask the users for their consent to the use of third party providers, the legal basis of the processing is consent. Furthermore, the processing can be a component of our (pre)contractual services, provided that the use of the third party was agreed within this context. Otherwise, user data will be processed on the basis of our legitimate interests (i.e. interest in efficient, economic and recipient friendly services). In this context, we would also like to refer you to the information on the use of cookies in this privacy policy. Processed data types: Usage data (e.g. websites visited, interest in content, access times), Meta/communication data (e.g. device information, IP addresses). Data subjects: Users (e.g. website visitors, users of online services). Purposes of Processing: Web Analytics (e.g. access statistics, recognition of returning visitors), Profiles with user-related information (Creating user profiles). Security measures: IP Masking (Pseudonymization of the IP address). Legal Basis: Consent (Article 6 (1) (a) GDPR), Legitimate Interests (Article 6 (1) (f) GDPR). Services and service providers being used: Google Analytics: Web analytics; Service provider: Google Ireland Limited, Gordon House, Barrow Street, Dublin 4, Ireland, parent company: Google LLC, 1600 Amphitheatre Parkway, Mountain View, CA 94043, USA; Website: https://marketingplatform.google.com/intl/en/about/analytics/ ; Privacy Policy: https://policies.google.com/privacy . Online Marketing We process personal data for the purposes of online marketing, which may include in particular the marketing of advertising space or the display of advertising and other content (collectively referred to as \"Content\") based on the potential interests of users and the measurement of their effectiveness. For these purposes, so-called user profiles are created and stored in a file (so-called \"cookie\") or similar procedure in which the relevant user information for the display of the aforementioned content is stored. This information may include, for example, content viewed, websites visited, online networks used, communication partners and technical information such as the browser used, computer system used and information on usage times. If users have consented to the collection of their sideline data, these can also be processed. The IP addresses of the users are also stored. However, we use provided IP masking procedures (i.e. pseudonymisation by shortening the IP address) to ensure the protection of the user's by using a pseudonym. In general, within the framework of the online marketing process, no clear user data (such as e-mail addresses or names) is secured, but pseudonyms. This means that we, as well as the providers of online marketing procedures, do not know the actual identity of the users, but only the information stored in their profiles. The information in the profiles is usually stored in the cookies or similar memorizing procedures. These cookies can later, generally also on other websites that use the same online marketing technology, be read and analyzed for purposes of content display, as well as supplemented with other data and stored on the server of the online marketing technology provider. Exceptionally, clear data can be assigned to the profiles. This is the case, for example, if the users are members of a social network whose online marketing technology we use and the network links the profiles of the users in the aforementioned data. Please note that users may enter into additional agreements with the social network providers or other service providers, e.g. by consenting as part of a registration process. As a matter of principle, we only gain access to summarised information about the performance of our advertisements. However, within the framework of so-called conversion measurement, we can check which of our online marketing processes have led to a so-called conversion, i.e. to the conclusion of a contract with us. The conversion measurement is used alone for the performance analysis of our marketing activities. Unless otherwise stated, we kindly ask you to consider that cookies used will be stored for a period of two years. Information on legal basis: If we ask users for their consent (e.g. in the context of a so-called \"cookie banner consent\"), the legal basis for processing data for online marketing purposes is this consent. Otherwise, user data will be processed on the basis of our legitimate interests (i.e. interest in the analysis, optimisation and economic operation of our online services. In this context, we would also like to refer you to the information on the use of cookies in this privacy policy. Processed data types: Usage data (e.g. websites visited, interest in content, access times), Meta/communication data (e.g. device information, IP addresses). Data subjects: Users (e.g. website visitors, users of online services). Purposes of Processing: Marketing, Profiles with user-related information (Creating user profiles). Security measures: IP Masking (Pseudonymization of the IP address). Legal Basis: Consent (Article 6 (1) (a) GDPR), Legitimate Interests (Article 6 (1) (f) GDPR). Opt-Out: We refer to the privacy policies of the respective service providers and the possibilities for objection (so-called \"opt-out\"). If no explicit opt-out option has been specified, it is possible to deactivate cookies in the settings of your browser. However, this may restrict the functions of our online offer. We therefore recommend the following additional opt-out options, which are offered collectively for each area: a) Europe: https://www.youronlinechoices.eu . b) Canada: https://www.youradchoices.ca/choices . c) USA: https://www.aboutads.info/choices . d) Cross-regional: https://optout.aboutads.info . Services and service providers being used: Google Analytics: Online marketing and web analytics; Service provider: Google Ireland Limited, Gordon House, Barrow Street, Dublin 4, Ireland, parent company: Google LLC, 1600 Amphitheatre Parkway, Mountain View, CA 94043, USA; Website: https://marketingplatform.google.com/intl/en/about/analytics/ ; Privacy Policy: https://policies.google.com/privacy ; Opt-Out: Opt-Out-Plugin: https://tools.google.com/dlpage/gaoptout?hl=en , Settings for the Display of Advertisements: https://adssettings.google.com/authenticated . Google Adsense with personalized ads: We use the Google Adsense service with personalized ads, which helps us to display ads within our online services and we receive a remuneration for their display or other use. ; Service provider: Google Ireland Limited, Gordon House, Barrow Street, Dublin 4, Ireland, parent company: Google LLC, 1600 Amphitheatre Parkway, Mountain View, CA 94043, USA; Website: https://marketingplatform.google.com ; Privacy Policy: https://policies.google.com/privacy . Amazon: Marketing of advertising media and advertising spaces; Service provider: Amazon Europe Core S.\u00e0.r.l., Amazon EU S.\u00e0.r.l., Amazon Services Europe S.\u00e0.r.l. and Amazon Media EU S.\u00e0.r.l., all four located at 38, avenue John F. Kennedy, L-1855 Luxembourg, and Amazon Instant Video Germany GmbH, Domagkstr. 28, 80807 Munich (together \"Amazon Europe\"), parent company: Amazon.com, Inc., 2021 Seventh Ave, Seattle, Washington 98121, USA; Website: https://www.amazon.com ; Privacy Policy: https://www.amazon.com/gp/help/customer/display.html?nodeId=201909010 . HubSpot: Marketing software for lead generation, marketing automation and analysis of marketing activities; Service provider: HubSpot, Inc., 25 First St., 2nd floor, Cambridge, Massachusetts 02141, USA; Website: https://www.hubspot.de ; Privacy Policy: https://legal.hubspot.com/privacy-policy . Affiliate-Programms und Affiliate-Links Within our online services, we include so-called affiliate links or other references (which for example may include search forms, widgets or discount codes) to the offers and services of third parties (collectively referred to as \"affiliate links\"). When users follow affiliate links or subsequently take advantage of offers, we may receive commission or other benefits (collectively referred to as \"commission\") from these third parties. In order to be able to track whether the users have followed the offers of an affiliate link used by us, it is necessary for the respective third party to know that the users have followed an affiliate link used within our online services. The assignment of affiliate links to the respective business transactions or other actions (e.g., purchases) serves the sole purpose of commission settlement and is removed as soon as it is no longer required for this purpose. For the purposes of the aforementioned affiliate link assignment, the affiliate links may be supplemented by certain values that may be a component of the link or otherwise stored, for example, in a cookie. The values may include in particular the source website (referrer), time, an online identifier of the operator of the website on which the affiliate link was located, an online identifier of the respective offer, the type of link used, the type of offer and an online identifier of the user. Information on legal basis: If we ask the users for their consent to the use of third party providers, the legal basis of the processing is consent. Furthermore, the processing can be a component of our (pre)contractual services, provided that the use of the third party was agreed within this context. Otherwise, user data will be processed on the basis of our legitimate interests (i.e. interest in efficient, economic and recipient friendly services). In this context, we would also like to refer you to the information on the use of cookies in this privacy policy. Processed data types: Contract data (e.g. contract object, duration, customer category), Usage data (e.g. websites visited, interest in content, access times), Meta/communication data (e.g. device information, IP addresses). Data subjects: Users (e.g. website visitors, users of online services). Purposes of Processing: Affiliate Tracking. Legal Basis: Consent (Article 6 (1) (a) GDPR), Performance of a contract and prior requests (Article 6 (1) (b) GDPR), Legitimate Interests (Article 6 (1) (f) GDPR). Services and service providers being used: Amazon Affiliate Program: Amazon Affiliate Program - Amazon and the Amazon logo are trademarks of Amazon.com, Inc. or one of its affiliates. Service provider: Amazon Europe Core S.\u00e0.r.l., Amazon EU S.\u00e0.r.l., Amazon Services Europe S.\u00e0.r.l. and Amazon Media EU S.\u00e0.r.l., all four located at 38, avenue John F. Kennedy, L-1855 Luxembourg, and Amazon Instant Video Germany GmbH, Domagkstr. 28, 80807 Munich (together \"Amazon Europe\"), parent company: Amazon.com, Inc., 2021 Seventh Ave, Seattle, Washington 98121, USA.; Website: https://www.amazon.com ; Privacy Policy: https://www.amazon.com/gp/help/customer/display.html?nodeId=201909010 . Profiles in Social Networks (Social Media) We maintain online presences within social networks and process user data in this context in order to communicate with the users active there or to offer information about us. We would like to point out that user data may be processed outside the European Union. This may entail risks for users, e.g. by making it more difficult to enforce users' rights. In addition, user data is usually processed within social networks for market research and advertising purposes. For example, user profiles can be created on the basis of user behaviour and the associated interests of users. The user profiles can then be used, for example, to place advertisements within and outside the networks which are presumed to correspond to the interests of the users. For these purposes, cookies are usually stored on the user's computer, in which the user's usage behaviour and interests are stored. Furthermore, data can be stored in the user profiles independently of the devices used by the users (especially if the users are members of the respective networs or will become members later on). For a detailed description of the respective processing operations and the opt-out options, please refer to the respective data protection declarations and information provided by the providers of the respective networks. Also in the case of requests for information and the exercise of rights of data subjects, we point out that these can be most effectively pursued with the providers. Only the providers have access to the data of the users and can directly take appropriate measures and provide information. If you still need help, please do not hesitate to contact us. Facebook : We are jointly responsible (so called \"joint controller\") with Facebook Ireland Ltd. for the collection (but not the further processing) of data of visitors to our Facebook page. This data includes information about the types of content users view or interact with, or the actions they take (see \"Things that you and others do and provide\" in the Facebook Data Policy: https://www.facebook.com/policy ), and information about the devices used by users (e.g., IP addresses, operating system, browser type, language settings, cookie information; see \"Device Information\" in the Facebook Data Policy: https://www.facebook.com/policy ). As explained in the Facebook Data Policy under \"How we use this information?\" Facebook also collects and uses information to provide analytics services, known as \"page insights,\" to site operators to help them understand how people interact with their pages and with content associated with them. We have concluded a special agreement with Facebook (\"Information about Page-Insights\", https://www.facebook.com/legal/terms/page_controller_addendum ), which regulates in particular the security measures that Facebook must observe and in which Facebook has agreed to fulfill the rights of the persons concerned (i.e. users can send information access or deletion requests directly to Facebook). The rights of users (in particular to access to information, erasure, objection and complaint to the competent supervisory authority) are not restricted by the agreements with Facebook. Further information can be found in the \"Information about Page Insights\" ( https://www.facebook.com/legal/terms/information_about_page_insights_data ). Processed data types: Contact data (e.g. e-mail, telephone numbers), Content data (e.g. text input, photographs, videos), Usage data (e.g. websites visited, interest in content, access times), Meta/communication data (e.g. device information, IP addresses). Data subjects: Users (e.g. website visitors, users of online services). Purposes of Processing: Contact requests and communication, Feedback (e.g. collecting feedback via online form), Marketing. Legal Basis: Legitimate Interests (Article 6 (1) (f) GDPR). Services and service providers being used: Instagram: Social network; Service provider: Instagram Inc., 1601 Willow Road, Menlo Park, CA, 94025, USA, , Mutterunternehmen: Facebook, 1 Hacker Way, Menlo Park, CA 94025, USA; Website: https://www.instagram.com ; Privacy Policy: https://instagram.com/about/legal/privacy . Facebook: Social network; Service provider: Facebook Ireland Ltd., 4 Grand Canal Square, Grand Canal Harbour, Dublin 2, Irland, parent company: Facebook, 1 Hacker Way, Menlo Park, CA 94025, USA; Website: https://www.facebook.com ; Privacy Policy: https://www.facebook.com/about/privacy ; Opt-Out: Settings for advertisements: https://www.facebook.com/adpreferences/ad_settings (login at Facebook is required). LinkedIn: Social network; Service provider: LinkedIn Ireland Unlimited Company, Wilton Place, Dublin 2, Ireland; Website: https://www.linkedin.com ; Privacy Policy: https://www.linkedin.com/legal/privacy-policy ; Opt-Out: https://www.linkedin.com/psettings/guest-controls/retargeting-opt-out . Twitter: Social network; Service provider: Twitter International Company, One Cumberland Place, Fenian Street, Dublin 2 D02 AX07, Ireland, parent company: Twitter Inc., 1355 Market Street, Suite 900, San Francisco, CA 94103, USA; Privacy Policy: https://twitter.com/de/privacy , (Settings) https://twitter.com/personalization . Vimeo: Social network and video platform; Service provider: Vimeo Inc., Attention: Legal Department, 555 West 18th Street New York, New York 10011, USA; Website: https://vimeo.com ; Privacy Policy: https://vimeo.com/privacy . YouTube: Social network and video platform; Service provider: Google Ireland Limited, Gordon House, Barrow Street, Dublin 4, Ireland, parent company: Google LLC, 1600 Amphitheatre Parkway, Mountain View, CA 94043, USA; Privacy Policy: https://policies.google.com/privacy ; Opt-Out: https://adssettings.google.com/authenticated . Plugins and embedded functions and content Within our online services, we integrate functional and content elements that are obtained from the servers of their respective providers (hereinafter referred to as \"third-party providers\"). These may, for example, be graphics, videos or city maps (hereinafter uniformly referred to as \"Content\"). The integration always presupposes that the third-party providers of this content process the IP address of the user, since they could not send the content to their browser without the IP address. The IP address is therefore required for the presentation of these contents or functions. We strive to use only those contents, whose respective offerers use the IP address only for the distribution of the contents. Third parties may also use so-called pixel tags (invisible graphics, also known as \"web beacons\") for statistical or marketing purposes. The \"pixel tags\" can be used to evaluate information such as visitor traffic on the pages of this website. The pseudonymous information may also be stored in cookies on the user's device and may include technical information about the browser and operating system, referring websites, visit times and other information about the use of our website, as well as may be linked to such information from other sources. Information on legal basis: If we ask users for their consent (e.g. in the context of a so-called \"cookie banner consent\"), the legal basis for processing is this consent. Otherwise, user data will be processed on the basis of our legitimate interests (i.e. interest in the analysis, optimisation and economic operation of our online services. We refer you to the note on the use of cookies in this privacy policy. Processed data types: Usage data (e.g. websites visited, interest in content, access times), Meta/communication data (e.g. device information, IP addresses), Inventory data (e.g. names, addresses), Contact data (e.g. e-mail, telephone numbers), Content data (e.g. text input, photographs, videos). Data subjects: Users (e.g. website visitors, users of online services). Purposes of Processing: Provision of our online services and usability, Provision of contractual services and customer support. Legal Basis: Legitimate Interests (Article 6 (1) (f) GDPR), Consent (Article 6 (1) (a) GDPR), Performance of a contract and prior requests (Article 6 (1) (b) GDPR). Services and service providers being used: Font Awesome: Display of fonts and symbols; Service provider: Fonticons, Inc. ,6 Porter Road Apartment 3R, Cambridge, MA 02140, USA; Website: https://fontawesome.com/ ; Privacy Policy: https://fontawesome.com/privacy . Google Fonts: We integrate the fonts (\"Google Fonts\") of the provider Google, whereby the data of the users are used solely for purposes of the representation of the fonts in the browser of the users. The integration takes place on the basis of our legitimate interests in a technically secure, maintenance-free and efficient use of fonts, their uniform presentation and consideration of possible licensing restrictions for their integration. Service provider: Google Ireland Limited, Gordon House, Barrow Street, Dublin 4, Ireland, parent company: Google LLC, 1600 Amphitheatre Parkway, Mountain View, CA 94043, USA; Website: https://fonts.google.com/ ; Privacy Policy: https://policies.google.com/privacy . YouTube videos: Video contents; Service provider: Google Ireland Limited, Gordon House, Barrow Street, Dublin 4, Ireland, , parent company: Google LLC, 1600 Amphitheatre Parkway, Mountain View, CA 94043, USA; Website: https://www.youtube.com ; Privacy Policy: https://policies.google.com/privacy ; Opt-Out: Opt-Out-Plugin: https://tools.google.com/dlpage/gaoptout?hl=en , Settings for the Display of Advertisements: https://adssettings.google.com/authenticated . YouTube-Videos: Video content; YouTube is integrated via the domain https://www.youtube-nocookie.com in the so-called \" enhanced data protection mode\", whereby no cookies on user activities are collected in order to personalise the video playback. Nevertheless, information on the user's interaction with the video (e.g. remembering the last playback point) may be stored. Service provider: Google Ireland Limited, Gordon House, Barrow Street, Dublin 4, Ireland, , parent company: Google LLC, 1600 Amphitheatre Parkway, Mountain View, CA 94043, USA; Website: https://www.youtube.com ; Privacy Policy: https://policies.google.com/privacy . Changes and Updates to the Privacy Policy We kindly ask you to inform yourself regularly about the contents of our data protection declaration. We will adjust the privacy policy as changes in our data processing practices make this necessary. We will inform you as soon as the changes require your cooperation (e.g. consent) or other individual notification. If we provide addresses and contact information of companies and organizations in this privacy policy, we ask you to note that addresses may change over time and to verify the information before contacting us. Rights of Data Subjects As data subject, you are entitled to various rights under the GDPR, which arise in particular from Articles 15 to 21 of the GDPR: Right to Object: You have the right, on grounds arising from your particular situation, to object at any time to the processing of your personal data which is based on letter (e) or (f) of Article 6(1) GDPR , including profiling based on those provisions. Where personal data are processed for direct marketing purposes, you have the right to object at any time to the processing of the personal data concerning you for the purpose of such marketing, which includes profiling to the extent that it is related to such direct marketing. Right of withdrawal for consents: You have the right to revoke consents at any time. Right of access: You have the right to request confirmation as to whether the data in question will be processed and to be informed of this data and to receive further information and a copy of the data in accordance with the provisions of the law. Right to rectification: You have the right, in accordance with the law, to request the completion of the data concerning you or the rectification of the incorrect data concerning you. Right to Erasure and Right to Restriction of Processing: In accordance with the statutory provisions, you have the right to demand that the relevant data be erased immediately or, alternatively, to demand that the processing of the data be restricted in accordance with the statutory provisions. Right to data portability: You have the right to receive data concerning you which you have provided to us in a structured, common and machine-readable format in accordance with the legal requirements, or to request its transmission to another controller. Complaint to the supervisory authority: In accordance with the law and without prejudice to any other administrative or judicial remedy, you also have the right to lodge a complaint with a data protection supervisory authority, in particular a supervisory authority in the Member State where you habitually reside, the supervisory authority of your place of work or the place of the alleged infringement, if you consider that the processing of personal data concerning you infringes the GDPR. Terminology and Definitions This section provides an overview of the terms used in this privacy policy. Many of the terms are drawn from the law and defined mainly in Article 4 GDPR. The legal definitions are binding. The following explanations, on the other hand, are intended above all for the purpose of comprehension. The terms are sorted alphabetically. Affiliate Tracking: Affiliate tracking logs links that the linking websites use to refer users to websites with products or other offers. The owners of the respective linked websites can receive a commission if users follow these so-called \"affiliate links\" and subsequently take advantage of the offers (e.g. buy goods or use services). To this end, it is necessary for providers to be able to track whether users who are interested in certain offers subsequently follow the affiliate links. It is therefore necessary for the functionality of affiliate links that they are supplemented by certain values that become part of the link or are otherwise stored, e.g. in a cookie. The values include in particular the source website (referrer), time, an online identification of the owner of the website on which the affiliate link was located, an online identification of the respective offer, an online identifier of the user, as well as tracking specific values such as advertising media ID, partner ID and categorizations Controller: \"Controller\" means the natural or legal person, public authority, agency or other body which, alone or jointly with others, determines the purposes and means of the processing of personal data. Conversion tracking: Conversion tracking is a method used to evaluate the effectiveness of marketing measures. For this purpose, a cookie is usually stored on the devices of the users within the websites on which the marketing measures take place and then called up again on the target website (e.g. we can thus trace whether the advertisements placed by us on other websites were successful). IP Masking: IP masking is a method by which the last octet, i.e. the last two numbers of an IP address, are deleted so that the IP address alone can no longer be used to uniquely identify a person. IP masking is therefore a means of pseudonymising processing methods, particularly in online marketing. Personal Data: \"personal data\" means any information relating to an identified or identifiable natural person (\"data subject\"); an identifiable natural person is one who can be identified, directly or indirectly, in particular by reference to an identifier such as a name, an identification number, location data, an online identifier or to one or more factors specific to the physical, physiological, genetic, mental, economic, cultural or social identity of that natural person. Processing: The term \"processing\" covers a wide range and practically every handling of data, be it collection, evaluation, storage, transmission or erasure. Profiles with user-related information: The processing of \"profiles with user-related information\", or \"profiles\" for short, includes any kind of automated processing of personal data that consists of using these personal data to analyse, evaluate or predict certain personal aspects relating to a natural person (depending on the type of profiling, this may include different information concerning demographics, behaviour and interests, such as interaction with websites and their content, etc.) (e.g. interests in certain content or products, click behaviour on a website or location). Cookies and web beacons are often used for profiling purposes. Web Analytics: Web Analytics serves the evaluation of visitor traffic of online services and can determine their behavior or interests in certain information, such as content of websites. With the help of web analytics, website owners, for example, can recognize at what time visitors visit their website and what content they are interested in. This allows them, for example, to optimize the content of the website to better meet the needs of their visitors. For purposes of web analytics, pseudonymous cookies and web beacons are frequently used in order to recognise returning visitors and thus obtain more precise analyses of the use of an online service.","title":"Privacy policy"},{"location":"robot-description/","text":"DiffBot Robot Description The description of the 2WD robot will be created in its own package named diffbot_description . The description uses URDF and xacro . For this we create a package with catkin create pkg PKG_NAME [--catkin-deps [DEP [DEP ...]]] : 1 2 3 4 5 fjp@ubuntu:~/git/diffbot/ros/src$ catkin create pkg diffbot_description Creating package \"diffbot_description\" in \"/home/fjp/git/diffbot/ros/src\"... Created file diffbot_description/CMakeLists.txt Created file diffbot_description/package.xml Successfully created package files in /home/fjp/git/diffbot/ros/src/diffbot_description. Because this package contains only descriptions and launch files it doesn't require any dependencies. According to ROS conventions we create the following folders where the individual files realted to the robot description will be placed: 1 fjp@ubuntu:~/git/diffbot/ros/src/robot_description$ mkdir urdf meshes launch The urdf folder will be used to keep the urdf and xacro files. The meshes folder keeps the meshes that are included in the urdf file, and the launch folder keeps the ROS launch files. Robot Model To model the two wheeled differential drive robot we follow REP-120 . It states to use a base_link and a base_footprint . The resulting description files can be found in the diffbot_description package. Required Tools To check a urdf file we can make use of the tools check_urdf and urdf_to_graphiz in the liburdfdom-tools debian package. Install it with the following command: 1 sudo apt install liburdfdom-tools First we need to convert the robot description of DiffBot, which is present as xacro file, to a urdf file by issuing the following command: 1 fjp@ubuntu:~/git/diffbot/ros$ rosrun xacro xacro ` rospack find diffbot_description ` /urdf/diffbot.urdf.xacro -o /tmp/diffbot.urdf After we've created the urdf from the xacro file we can check the urdf files for errors with: 1 2 3 4 5 6 7 8 fjp@ubuntu:/tmp$ check_urdf diffbot.urdf robot name is: diffbot ---------- Successfully Parsed XML --------------- root Link: base_footprint has 1 child(ren) child(1): base_link child(1): caster_link child(2): front_left_wheel child(3): front_right_wheel It is also helpful to output a graphviz diagram of the robot model: 1 2 3 4 fjp@ubuntu:/tmp$ urdf_to_graphiz diffbot.urdf Created file diffbot.gv Created file diffbot.pdf fjp@ubuntu:/tmp$ evince diffbot.pdf Graphviz diagram of DiffBot URDF robot description. To visualize the 3D model in RViz we first need to install the joint-state-publisher-gui which was separated from non-gui joint-state-publisher . There exists a debian package which can be installed with the following command: 1 fjp@ubuntu:~/git/diffbot/ros$ sudo apt install ros-noetic-joint-state-publisher-gui After installing the required dependency, the view_diffbot.launch launch file can be executed using roslaunch command: 1 fjp@ubuntu:~/git/diffbot/ros$ roslaunch diffbot_description view_diffbot.launch According to the launch file's configuration, this will show the robot in RViz together with the joint-state-publisher-gui to set the joint values: DiffBot displayed in RViz. With the robot descripton loaded on the ROS parameter server, it's possible to use the TF Tree rqt plugin to display the transformation tree (see image above). In the next section, Gazebo Simulation , the robot model is prepared for simulation inside of Gazebo . URDF in Gazebo http://gazebosim.org/tutorials?tut=ros_urdf&cat=connect_ros#Tutorial:UsingaURDFinGazebo References http://gazebosim.org/tutorials?tut=ros_urdf&cat=connect_ros#Tutorial:UsingaURDFinGazebo https://answers.ros.org/question/30539/choosing-the-right-coefficients-for-gazebo-simulation/ https://answers.ros.org/question/231880/how-to-improve-amcl-pose-estimate/","title":"Robot Description"},{"location":"robot-description/#diffbot-robot-description","text":"The description of the 2WD robot will be created in its own package named diffbot_description . The description uses URDF and xacro . For this we create a package with catkin create pkg PKG_NAME [--catkin-deps [DEP [DEP ...]]] : 1 2 3 4 5 fjp@ubuntu:~/git/diffbot/ros/src$ catkin create pkg diffbot_description Creating package \"diffbot_description\" in \"/home/fjp/git/diffbot/ros/src\"... Created file diffbot_description/CMakeLists.txt Created file diffbot_description/package.xml Successfully created package files in /home/fjp/git/diffbot/ros/src/diffbot_description. Because this package contains only descriptions and launch files it doesn't require any dependencies. According to ROS conventions we create the following folders where the individual files realted to the robot description will be placed: 1 fjp@ubuntu:~/git/diffbot/ros/src/robot_description$ mkdir urdf meshes launch The urdf folder will be used to keep the urdf and xacro files. The meshes folder keeps the meshes that are included in the urdf file, and the launch folder keeps the ROS launch files.","title":"DiffBot Robot Description"},{"location":"robot-description/#robot-model","text":"To model the two wheeled differential drive robot we follow REP-120 . It states to use a base_link and a base_footprint . The resulting description files can be found in the diffbot_description package.","title":"Robot Model"},{"location":"robot-description/#required-tools","text":"To check a urdf file we can make use of the tools check_urdf and urdf_to_graphiz in the liburdfdom-tools debian package. Install it with the following command: 1 sudo apt install liburdfdom-tools First we need to convert the robot description of DiffBot, which is present as xacro file, to a urdf file by issuing the following command: 1 fjp@ubuntu:~/git/diffbot/ros$ rosrun xacro xacro ` rospack find diffbot_description ` /urdf/diffbot.urdf.xacro -o /tmp/diffbot.urdf After we've created the urdf from the xacro file we can check the urdf files for errors with: 1 2 3 4 5 6 7 8 fjp@ubuntu:/tmp$ check_urdf diffbot.urdf robot name is: diffbot ---------- Successfully Parsed XML --------------- root Link: base_footprint has 1 child(ren) child(1): base_link child(1): caster_link child(2): front_left_wheel child(3): front_right_wheel It is also helpful to output a graphviz diagram of the robot model: 1 2 3 4 fjp@ubuntu:/tmp$ urdf_to_graphiz diffbot.urdf Created file diffbot.gv Created file diffbot.pdf fjp@ubuntu:/tmp$ evince diffbot.pdf Graphviz diagram of DiffBot URDF robot description. To visualize the 3D model in RViz we first need to install the joint-state-publisher-gui which was separated from non-gui joint-state-publisher . There exists a debian package which can be installed with the following command: 1 fjp@ubuntu:~/git/diffbot/ros$ sudo apt install ros-noetic-joint-state-publisher-gui After installing the required dependency, the view_diffbot.launch launch file can be executed using roslaunch command: 1 fjp@ubuntu:~/git/diffbot/ros$ roslaunch diffbot_description view_diffbot.launch According to the launch file's configuration, this will show the robot in RViz together with the joint-state-publisher-gui to set the joint values: DiffBot displayed in RViz. With the robot descripton loaded on the ROS parameter server, it's possible to use the TF Tree rqt plugin to display the transformation tree (see image above). In the next section, Gazebo Simulation , the robot model is prepared for simulation inside of Gazebo .","title":"Required Tools"},{"location":"robot-description/#urdf-in-gazebo","text":"http://gazebosim.org/tutorials?tut=ros_urdf&cat=connect_ros#Tutorial:UsingaURDFinGazebo","title":"URDF in Gazebo"},{"location":"robot-description/#references","text":"http://gazebosim.org/tutorials?tut=ros_urdf&cat=connect_ros#Tutorial:UsingaURDFinGazebo https://answers.ros.org/question/30539/choosing-the-right-coefficients-for-gazebo-simulation/ https://answers.ros.org/question/231880/how-to-improve-amcl-pose-estimate/","title":"References"},{"location":"ros-network-setup/","text":"ROS Network Setup ROS is a distributed computing environment. This allows running compute expensive tasks such as visualization or path planning for navigation on machines with more performance and sending goals to robots operating on less performant hardware like DiffBot with its Raspberry Pi 4 B. For detailed instructions see ROS Network Setup and the ROS Environment Variables . The setup between the work machine that handles compute heavy tasks and DiffBot is as follows: TODO image On DiffBot we configure the ROS_MASTER_URI to be the IP address of the work machine. 1 export ROS_MASTER_URI=http://192.168.0.9:11311/ To test run the master on the work machine using the roscore command. Then run the the listener from the roscpp_tutorials package in another terminal: 1 fjp@workmachine:~/git/diffbot/ros$ rosrun roscpp_tutorials listener Then switch to a terminal on DiffBot's Raspberry Pi and run the talker from roscpp_tutorials : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 fjp@diffbot:~/git/diffbot/ros$ rosrun roscpp_tutorials talker [ INFO] [1602018325.633133449]: hello world 0 [ INFO] [1602018325.733137152]: hello world 1 [ INFO] [1602018325.833112540]: hello world 2 [ INFO] [1602018325.933114483]: hello world 3 [ INFO] [1602018326.033114093]: hello world 4 [ INFO] [1602018326.133112684]: hello world 5 [ INFO] [1602018326.233112183]: hello world 6 [ INFO] [1602018326.333113126]: hello world 7 [ INFO] [1602018326.433113680]: hello world 8 [ INFO] [1602018326.533113031]: hello world 9 [ INFO] [1602018326.633110140]: hello world 10 [ INFO] [1602018326.733108954]: hello world 11 [ INFO] [1602018326.833113267]: hello world 12 [ INFO] [1602018326.933164505]: hello world 13 [ INFO] [1602018327.033119135]: hello world 14 [ INFO] [1602018327.133113559]: hello world 15 [ INFO] [1602018327.233111003]: hello world 16 [ INFO] [1602018327.333110705]: hello world 17 [ INFO] [1602018327.433126425]: hello world 18 [ INFO] [1602018327.533111498]: hello world 19 [ INFO] [1602018327.633107978]: hello world 20 [ INFO] [1602018327.733110736]: hello world 21 [ INFO] [1602018327.833107605]: hello world 22 [ INFO] [1602018327.933111659]: hello world 23 [ INFO] [1602018328.033108065]: hello world 24 [ INFO] [1602018328.133110379]: hello world 25 [ INFO] [1602018328.233150191]: hello world 26 [ INFO] [1602018328.333135986]: hello world 27 [ INFO] [1602018328.433153558]: hello world 28 [ INFO] [1602018328.533154557]: hello world 29 [ INFO] [1602018328.633151667]: hello world 30 [ INFO] [1602018328.733128777]: hello world 31 [ INFO] [1602018328.833170108]: hello world 32 [ INFO] [1602018328.933172402]: hello world 33 Looking back at the terminal on the work machine you should see the output: 1 2 3 4 5 6 7 8 fjp.github.io git:(master) rosrun roscpp_tutorials listener [ INFO] [1602018328.330070016]: I heard: [hello world 27] [ INFO] [1602018328.430244670]: I heard: [hello world 28] [ INFO] [1602018328.530173113]: I heard: [hello world 29] [ INFO] [1602018328.630251690]: I heard: [hello world 30] [ INFO] [1602018328.730334064]: I heard: [hello world 31] [ INFO] [1602018328.830346566]: I heard: [hello world 32] [ INFO] [1602018328.930009032]: I heard: [hello world 33] Note that it can take some time receiving the messages from DiffBot on the work machine, which we can see from the time stamps in the outputs above.","title":"ROS Network Setup"},{"location":"ros-network-setup/#ros-network-setup","text":"ROS is a distributed computing environment. This allows running compute expensive tasks such as visualization or path planning for navigation on machines with more performance and sending goals to robots operating on less performant hardware like DiffBot with its Raspberry Pi 4 B. For detailed instructions see ROS Network Setup and the ROS Environment Variables . The setup between the work machine that handles compute heavy tasks and DiffBot is as follows: TODO image On DiffBot we configure the ROS_MASTER_URI to be the IP address of the work machine. 1 export ROS_MASTER_URI=http://192.168.0.9:11311/ To test run the master on the work machine using the roscore command. Then run the the listener from the roscpp_tutorials package in another terminal: 1 fjp@workmachine:~/git/diffbot/ros$ rosrun roscpp_tutorials listener Then switch to a terminal on DiffBot's Raspberry Pi and run the talker from roscpp_tutorials : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 fjp@diffbot:~/git/diffbot/ros$ rosrun roscpp_tutorials talker [ INFO] [1602018325.633133449]: hello world 0 [ INFO] [1602018325.733137152]: hello world 1 [ INFO] [1602018325.833112540]: hello world 2 [ INFO] [1602018325.933114483]: hello world 3 [ INFO] [1602018326.033114093]: hello world 4 [ INFO] [1602018326.133112684]: hello world 5 [ INFO] [1602018326.233112183]: hello world 6 [ INFO] [1602018326.333113126]: hello world 7 [ INFO] [1602018326.433113680]: hello world 8 [ INFO] [1602018326.533113031]: hello world 9 [ INFO] [1602018326.633110140]: hello world 10 [ INFO] [1602018326.733108954]: hello world 11 [ INFO] [1602018326.833113267]: hello world 12 [ INFO] [1602018326.933164505]: hello world 13 [ INFO] [1602018327.033119135]: hello world 14 [ INFO] [1602018327.133113559]: hello world 15 [ INFO] [1602018327.233111003]: hello world 16 [ INFO] [1602018327.333110705]: hello world 17 [ INFO] [1602018327.433126425]: hello world 18 [ INFO] [1602018327.533111498]: hello world 19 [ INFO] [1602018327.633107978]: hello world 20 [ INFO] [1602018327.733110736]: hello world 21 [ INFO] [1602018327.833107605]: hello world 22 [ INFO] [1602018327.933111659]: hello world 23 [ INFO] [1602018328.033108065]: hello world 24 [ INFO] [1602018328.133110379]: hello world 25 [ INFO] [1602018328.233150191]: hello world 26 [ INFO] [1602018328.333135986]: hello world 27 [ INFO] [1602018328.433153558]: hello world 28 [ INFO] [1602018328.533154557]: hello world 29 [ INFO] [1602018328.633151667]: hello world 30 [ INFO] [1602018328.733128777]: hello world 31 [ INFO] [1602018328.833170108]: hello world 32 [ INFO] [1602018328.933172402]: hello world 33 Looking back at the terminal on the work machine you should see the output: 1 2 3 4 5 6 7 8 fjp.github.io git:(master) rosrun roscpp_tutorials listener [ INFO] [1602018328.330070016]: I heard: [hello world 27] [ INFO] [1602018328.430244670]: I heard: [hello world 28] [ INFO] [1602018328.530173113]: I heard: [hello world 29] [ INFO] [1602018328.630251690]: I heard: [hello world 30] [ INFO] [1602018328.730334064]: I heard: [hello world 31] [ INFO] [1602018328.830346566]: I heard: [hello world 32] [ INFO] [1602018328.930009032]: I heard: [hello world 33] Note that it can take some time receiving the messages from DiffBot on the work machine, which we can see from the time stamps in the outputs above.","title":"ROS Network Setup"},{"location":"ros-setup/","text":"ROS Installation The robot setup is supposed to run on Ubuntu Mate 20.04 Focal Fossa. ROS Noetic is intended to run with this Ubuntu version. To install ROS follow the installation instructions . Info In the 1.4 Installation step](http://wiki.ros.org/noetic/Installation/Ubuntu#Installation-1) you have to choose how much of ROS you want to install. For the development pc you can go with the sudo apt install ros-noetic-desktop-full command. For the robot install the ros-noetic-robot Ubuntu package. Other system dependencies will be installed with the rosdep command, explained in the following section. Another program that is required to run ROS nodes written with the rospy client library is python-is-python3 . Install it with: 1 sudo apt install python-is-python3 Dependencies After having git cloned one or more ROS packages, such as diffbot , it is necessary to install system dependencies of the packages in the catkin workspace. For this, ROS provides the rosdep tool. To install all system dependencies of the packages in your catkin workspace make use of the following command ( source ): 1 rosdep install --from-paths src --ignore-src -r -y This will go through each package's package.xml file and install the listed dependencies that are currently not installed on your system. Build Tool: catkin_tools To work with ROS we will use catkin_tools instead of catkin_make . catkin_tools provide commands such as catkin build which we will use instead of catkin_make because the catkin_tools are more actively developed than catkin_make ref . Note It is recommended to use catkin_tools instead of the default catkin when building ROS workspaces. catkin_tools provides a number of benefits over regular catkin_make and will be used in the documentation. All packages can be built using catkin_make however: use catkin_make in place of catkin build where appropriate. Bug The current way to install catkin-tools in the documentation from the Ubuntu package repository doesn't work. Follow the steps below instead for now. Success As of now the correct way to install catkin-tools is to use the following command: 1 sudo apt-get install python3-osrf-pycommon python3-catkin-tools For your reference, you can read more about it in this open issue . After sucessfully installing catkin_tools we can create and initialize a workspace (called ros for this project) with the commands listed in the build_tools documentation : Note Note that we already source d the setup.bash while following the ROS installation instructions . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 fjp@ubuntu:~/git/2wd-robot/ros$ mkdir -p ~/git/2wd-robot/ros/src # Make a new workspace and source space fjp@ubuntu:~/git/2wd-robot/ros$ cd ~/git/2wd-robot/ros # Navigate to the workspace root fjp@ubuntu:~/git/2wd-robot/ros$ catkin init # Initialize it with a hidden marker file Initializing catkin workspace in ` /home/fjp/git/2wd-robot/ros ` . ---------------------------------------------------------------- Profile: default Extending: [ env ] /opt/ros/melodic Workspace: /home/fjp/git/2wd-robot/ros ---------------------------------------------------------------- Build Space: [ missing ] /home/fjp/git/2wd-robot/ros/build Devel Space: [ missing ] /home/fjp/git/2wd-robot/ros/devel Install Space: [ unused ] /home/fjp/git/2wd-robot/ros/install Log Space: [ missing ] /home/fjp/git/2wd-robot/ros/logs Source Space: [ exists ] /home/fjp/git/2wd-robot/ros/src DESTDIR: [ unused ] None ---------------------------------------------------------------- Devel Space Layout: linked Install Space Layout: None ---------------------------------------------------------------- Additional CMake Args: None Additional Make Args: None Additional catkin Make Args: None Internal Make Job Server: True Cache Job Environments: False ---------------------------------------------------------------- Whitelisted Packages: None Blacklisted Packages: None ---------------------------------------------------------------- Workspace configuration appears valid. ---------------------------------------------------------------- Command Overview of catkin_tools To create packages , which will be covered in the next posts in more depth, we will use catkin create pkg PKG_NAME . Building the workspace is done with catkin build . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 fjp@ubuntu:~/git/2wd-robot/ros$ catkin build ---------------------------------------------------------------- Profile: default Extending: [ env ] /opt/ros/melodic Workspace: /home/fjp/git/2wd-robot/ros ---------------------------------------------------------------- Build Space: [ exists ] /home/fjp/git/2wd-robot/ros/build Devel Space: [ exists ] /home/fjp/git/2wd-robot/ros/devel Install Space: [ unused ] /home/fjp/git/2wd-robot/ros/install Log Space: [ missing ] /home/fjp/git/2wd-robot/ros/logs Source Space: [ exists ] /home/fjp/git/2wd-robot/ros/src DESTDIR: [ unused ] None ---------------------------------------------------------------- Devel Space Layout: linked Install Space Layout: None ---------------------------------------------------------------- Additional CMake Args: None Additional Make Args: None Additional catkin Make Args: None Internal Make Job Server: True Cache Job Environments: False ---------------------------------------------------------------- Whitelisted Packages: None Blacklisted Packages: None ---------------------------------------------------------------- Workspace configuration appears valid. NOTE: Forcing CMake to run for each package. ---------------------------------------------------------------- [ build ] No packages were found in the source space '/home/fjp/git/2wd-robot/ros/src' [ build ] No packages to be built. [ build ] Package table is up to date. Starting >>> catkin_tools_prebuild Finished <<< catkin_tools_prebuild [ 10 .0 seconds ] [ build ] Summary: All 1 packages succeeded! [ build ] Ignored: None. [ build ] Warnings: None. [ build ] Abandoned: None. [ build ] Failed: None. [ build ] Runtime: 10 .1 seconds total. Environment Setup Finally the newly built packages have to be loaded in the environment using source . 1 fjp@ubuntu:~/git/2wd-robot/ros$ source ~/git/2wd-robot/ros/devel/setup.bash # Load the workspace's environment Tip To avoid tediously typing the above source command, it is convenient to create an alias in your ~/.bashrc or ~/.zshrc similar to the following: 1 alias s='source devel/setup.bash' or using the absolute path 1 alias sa='source ~/git/2wd-robot/ros/devel/setup.bash' It is recommended to use the correct setup script for the shell you use ( bash , zsh , etc.). In case you are unsure, you can check with the echo $SHELL command which will most likely output /bin/bash . Info Instead of source it is possible to use the . command instead. Don't confuse it though with the current directory, which is also represented as . . Resources Although the catkin tutorial uses catkin_make it provides a helpful guide to create a workspace","title":"ROS Setup"},{"location":"ros-setup/#ros-installation","text":"The robot setup is supposed to run on Ubuntu Mate 20.04 Focal Fossa. ROS Noetic is intended to run with this Ubuntu version. To install ROS follow the installation instructions . Info In the 1.4 Installation step](http://wiki.ros.org/noetic/Installation/Ubuntu#Installation-1) you have to choose how much of ROS you want to install. For the development pc you can go with the sudo apt install ros-noetic-desktop-full command. For the robot install the ros-noetic-robot Ubuntu package. Other system dependencies will be installed with the rosdep command, explained in the following section. Another program that is required to run ROS nodes written with the rospy client library is python-is-python3 . Install it with: 1 sudo apt install python-is-python3","title":"ROS Installation"},{"location":"ros-setup/#dependencies","text":"After having git cloned one or more ROS packages, such as diffbot , it is necessary to install system dependencies of the packages in the catkin workspace. For this, ROS provides the rosdep tool. To install all system dependencies of the packages in your catkin workspace make use of the following command ( source ): 1 rosdep install --from-paths src --ignore-src -r -y This will go through each package's package.xml file and install the listed dependencies that are currently not installed on your system.","title":"Dependencies"},{"location":"ros-setup/#build-tool-catkin_tools","text":"To work with ROS we will use catkin_tools instead of catkin_make . catkin_tools provide commands such as catkin build which we will use instead of catkin_make because the catkin_tools are more actively developed than catkin_make ref . Note It is recommended to use catkin_tools instead of the default catkin when building ROS workspaces. catkin_tools provides a number of benefits over regular catkin_make and will be used in the documentation. All packages can be built using catkin_make however: use catkin_make in place of catkin build where appropriate. Bug The current way to install catkin-tools in the documentation from the Ubuntu package repository doesn't work. Follow the steps below instead for now. Success As of now the correct way to install catkin-tools is to use the following command: 1 sudo apt-get install python3-osrf-pycommon python3-catkin-tools For your reference, you can read more about it in this open issue . After sucessfully installing catkin_tools we can create and initialize a workspace (called ros for this project) with the commands listed in the build_tools documentation : Note Note that we already source d the setup.bash while following the ROS installation instructions . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 fjp@ubuntu:~/git/2wd-robot/ros$ mkdir -p ~/git/2wd-robot/ros/src # Make a new workspace and source space fjp@ubuntu:~/git/2wd-robot/ros$ cd ~/git/2wd-robot/ros # Navigate to the workspace root fjp@ubuntu:~/git/2wd-robot/ros$ catkin init # Initialize it with a hidden marker file Initializing catkin workspace in ` /home/fjp/git/2wd-robot/ros ` . ---------------------------------------------------------------- Profile: default Extending: [ env ] /opt/ros/melodic Workspace: /home/fjp/git/2wd-robot/ros ---------------------------------------------------------------- Build Space: [ missing ] /home/fjp/git/2wd-robot/ros/build Devel Space: [ missing ] /home/fjp/git/2wd-robot/ros/devel Install Space: [ unused ] /home/fjp/git/2wd-robot/ros/install Log Space: [ missing ] /home/fjp/git/2wd-robot/ros/logs Source Space: [ exists ] /home/fjp/git/2wd-robot/ros/src DESTDIR: [ unused ] None ---------------------------------------------------------------- Devel Space Layout: linked Install Space Layout: None ---------------------------------------------------------------- Additional CMake Args: None Additional Make Args: None Additional catkin Make Args: None Internal Make Job Server: True Cache Job Environments: False ---------------------------------------------------------------- Whitelisted Packages: None Blacklisted Packages: None ---------------------------------------------------------------- Workspace configuration appears valid. ----------------------------------------------------------------","title":"Build Tool: catkin_tools"},{"location":"ros-setup/#command-overview-of-catkin_tools","text":"To create packages , which will be covered in the next posts in more depth, we will use catkin create pkg PKG_NAME . Building the workspace is done with catkin build . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 fjp@ubuntu:~/git/2wd-robot/ros$ catkin build ---------------------------------------------------------------- Profile: default Extending: [ env ] /opt/ros/melodic Workspace: /home/fjp/git/2wd-robot/ros ---------------------------------------------------------------- Build Space: [ exists ] /home/fjp/git/2wd-robot/ros/build Devel Space: [ exists ] /home/fjp/git/2wd-robot/ros/devel Install Space: [ unused ] /home/fjp/git/2wd-robot/ros/install Log Space: [ missing ] /home/fjp/git/2wd-robot/ros/logs Source Space: [ exists ] /home/fjp/git/2wd-robot/ros/src DESTDIR: [ unused ] None ---------------------------------------------------------------- Devel Space Layout: linked Install Space Layout: None ---------------------------------------------------------------- Additional CMake Args: None Additional Make Args: None Additional catkin Make Args: None Internal Make Job Server: True Cache Job Environments: False ---------------------------------------------------------------- Whitelisted Packages: None Blacklisted Packages: None ---------------------------------------------------------------- Workspace configuration appears valid. NOTE: Forcing CMake to run for each package. ---------------------------------------------------------------- [ build ] No packages were found in the source space '/home/fjp/git/2wd-robot/ros/src' [ build ] No packages to be built. [ build ] Package table is up to date. Starting >>> catkin_tools_prebuild Finished <<< catkin_tools_prebuild [ 10 .0 seconds ] [ build ] Summary: All 1 packages succeeded! [ build ] Ignored: None. [ build ] Warnings: None. [ build ] Abandoned: None. [ build ] Failed: None. [ build ] Runtime: 10 .1 seconds total.","title":"Command Overview of catkin_tools"},{"location":"ros-setup/#environment-setup","text":"Finally the newly built packages have to be loaded in the environment using source . 1 fjp@ubuntu:~/git/2wd-robot/ros$ source ~/git/2wd-robot/ros/devel/setup.bash # Load the workspace's environment Tip To avoid tediously typing the above source command, it is convenient to create an alias in your ~/.bashrc or ~/.zshrc similar to the following: 1 alias s='source devel/setup.bash' or using the absolute path 1 alias sa='source ~/git/2wd-robot/ros/devel/setup.bash' It is recommended to use the correct setup script for the shell you use ( bash , zsh , etc.). In case you are unsure, you can check with the echo $SHELL command which will most likely output /bin/bash . Info Instead of source it is possible to use the . command instead. Don't confuse it though with the current directory, which is also represented as . .","title":"Environment Setup"},{"location":"ros-setup/#resources","text":"Although the catkin tutorial uses catkin_make it provides a helpful guide to create a workspace","title":"Resources"},{"location":"rpi-setup/","text":"These are the instructions to setup a custom Ubuntu 20.04 Focal Fossa on Raspberry Pi 4 B. Note In case you don't have a Raspberry Pi 4 B, it is also possible to create the robot using a Jetson Nano from Nvidia. See the related section in the documentation . Obtain Ubuntu 20.04 Mate Image for Raspberry Pi To install the long term supported (LTS) Ubuntu 20.04 on the Raspberry Pi 4B we make use of arm64 version of Ubuntu Mate . Download the latest release of the image and flash it to an empty sd card. To do this follow the instructions on the Raspberry Pi documentation or balenaEtcher . Another way is to use the Raspberry Pi Imager explained here . Info Flashing in this context, means the transfer of software data, also refered to as Firmware, from your computer to a device, such as a the sd card in this case. The term \u201cto flash\u201d comes from the Flash storage component of a device where the Firmware is stored. Wifi Issues So far there are no known issues using WiFi with Ubuntu Mate 20.04 arm64 on the Raspberry Pi 4B. Possible issues with other images If you are not in the US it is possible that you encounter connection problems when connected to a 5Ghz Wifi network. If you are in a different country than the US you need to update your regulatory country. 5Ghz needs this to know the right bands to use. This can be changed by editing the value of `REGDOMAIN` in the file `/etc/default/crda` ([Central Regulatory Domain Agent](https://wireless.wiki.kernel.org/en/developers/regulatory/crda)) to the code for your country [ref](https://github.com/TheRemote/Ubuntu-Server-raspi4-unofficial/issues/98). There might be some wifi issues where the connection is lost after some time. This might be a firmware issue reported here . Although the wifi power save management is turned off by default, make sure to also do this for external wifi usb dongles by editing the following file 1 sudo vim /etc/NetworkManager/conf.d/default-wifi-powersave-on.conf It contains: 1 2 [connection] wifi.powersave = 3 Set this to 2 to turn power management off. Possible values for the wifi.powersave field are: 1 2 3 4 NM_SETTING_WIRELESS_POWERSAVE_DEFAULT (0): use the default value NM_SETTING_WIRELESS_POWERSAVE_IGNORE (1): don't touch existing setting NM_SETTING_WIRELESS_POWERSAVE_DISABLE (2): disable powersave NM_SETTING_WIRELESS_POWERSAVE_ENABLE (3): enable powersave (Informal source on GitHub for these values.) You can check if it is turned off with iwconfig : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 wlan0 IEEE 802.11 ESSID:\"wifiname\" Mode:Managed Frequency:5.18 GHz Access Point: E0:28:6D:48:33:5D Bit Rate=433.3 Mb/s Tx-Power=31 dBm Retry short limit:7 RTS thr:off Fragment thr:off Power Management:off Link Quality=70/70 Signal level=-27 dBm Rx invalid nwid:0 Rx invalid crypt:0 Rx invalid frag:0 Tx excessive retries:1 Invalid misc:0 Missed beacon:0 lo no wireless extensions. wlan1 IEEE 802.11AC ESSID:\"wifiname\" Nickname:\"WIFI@RTL8821CU\" Mode:Managed Frequency:5.18 GHz Access Point: E0:28:6D:48:33:5D Bit Rate:434 Mb/s Sensitivity:0/0 Retry:off RTS thr:off Fragment thr:off Power Management:off Link Quality=64/100 Signal level=-26 dBm Noise level=0 dBm Rx invalid nwid:0 Rx invalid crypt:0 Rx invalid frag:0 Tx excessive retries:0 Invalid misc:0 Missed beacon:0 eth0 no wireless extensions. This is the output when there is one external usb WiFi dongle connected to the Raspberry Pi 4 B and no ethernet cable. Instructions to install Wifi drivers In case you use a Realtek USB Wifi dongle it might not be directly supported by the linux kernel. To install the correct driver, you first have to figure out the driver id. When plugging in the USB Wifi dongle and running dmesg afterwards shoud output something similar to: 1 2 3 4 5 [ 1430.931258] usb 1-1.3: new high-speed USB device number 13 using xhci_hcd [ 1431.031913] usb 1-1.3: New USB device found, idVendor=0bda, idProduct=c811, bcdDevice= 2.00 [ 1431.031925] usb 1-1.3: New USB device strings: Mfr=1, Product=2, SerialNumber=3 [ 1431.031932] usb 1-1.3: Product: 802.11ac NIC [ 1431.031939] usb 1-1.3: Manufacturer: Realtek With the idVendor=0bda and idProduct=c811 you can search for the correct driver. In this case it is the 1 RTL8821cu USB 0bda:c811 Realtek WEP WPA WPA2 For it the instructions in the https://github.com/brektrou/rtl8821CU repository can be used to install the driver. Warning These instructions are specifically for the Realtek (id c811) USB wifi dongle. In case you use another USB dongle it might work directly or there exists a ubuntu package. In general try to use Google for instructions on how to install. The following is only an example on what might be needed to get the driver working. After cloning this package, the Makefile needs to be adapted to work for the Raspberry Pi 4B: 1 2 3 4 ###################### Platform Related ####################### CONFIG_PLATFORM_I386_PC = n CONFIG_PLATFORM_ARM_RPI = n CONFIG_PLATFORM_ARM64_RPI = y Then follow the instructions from the README.md: 1 2 3 # For AArch64 sudo cp /lib/modules/$(uname -r)/build/arch/arm64/Makefile /lib/modules/$(uname -r)/build/arch/arm64/Makefile.$(date +%Y%m%d%H%M) sudo sed -i 's/-mgeneral-regs-only//' /lib/modules/$(uname -r)/build/arch/arm64/Makefile and finally make and install the driver: 1 2 make sudo make After a reboot the USB Wifi dongle should be detected and two wifi adapters should show up - the internal wifi module on the RPi and the USB dongle. Sources Disable power management in Stretch How to turn off Wireless power management permanently RPi4: WiFi client crashes often (brcmf_fw_crashed: Firmware has halted or crashed) WLAN Karten Realtek (german) Prepare Ubuntu After flashing the image to the sd card insert it to the Pi, hook it up to a monitor via HDMI and power it up by pluggin in the USB-C connector. Then you should follow the installation instructions on the screen. Once finished, follow the next steps to install ROS Noetic. Note To proceed with the next steps on installing ROS and other related dependencies you can run a bash script. Just clone this repository: https://github.com/fjp/diffbot.git and run ubuntu-setup.sh . But to learn more, you should follow the instructions on the following pages.","title":"Raspberry Pi Setup"},{"location":"rpi-setup/#obtain-ubuntu-2004-mate-image-for-raspberry-pi","text":"To install the long term supported (LTS) Ubuntu 20.04 on the Raspberry Pi 4B we make use of arm64 version of Ubuntu Mate . Download the latest release of the image and flash it to an empty sd card. To do this follow the instructions on the Raspberry Pi documentation or balenaEtcher . Another way is to use the Raspberry Pi Imager explained here . Info Flashing in this context, means the transfer of software data, also refered to as Firmware, from your computer to a device, such as a the sd card in this case. The term \u201cto flash\u201d comes from the Flash storage component of a device where the Firmware is stored.","title":"Obtain Ubuntu 20.04 Mate Image for Raspberry Pi"},{"location":"rpi-setup/#wifi-issues","text":"So far there are no known issues using WiFi with Ubuntu Mate 20.04 arm64 on the Raspberry Pi 4B. Possible issues with other images If you are not in the US it is possible that you encounter connection problems when connected to a 5Ghz Wifi network. If you are in a different country than the US you need to update your regulatory country. 5Ghz needs this to know the right bands to use. This can be changed by editing the value of `REGDOMAIN` in the file `/etc/default/crda` ([Central Regulatory Domain Agent](https://wireless.wiki.kernel.org/en/developers/regulatory/crda)) to the code for your country [ref](https://github.com/TheRemote/Ubuntu-Server-raspi4-unofficial/issues/98). There might be some wifi issues where the connection is lost after some time. This might be a firmware issue reported here . Although the wifi power save management is turned off by default, make sure to also do this for external wifi usb dongles by editing the following file 1 sudo vim /etc/NetworkManager/conf.d/default-wifi-powersave-on.conf It contains: 1 2 [connection] wifi.powersave = 3 Set this to 2 to turn power management off. Possible values for the wifi.powersave field are: 1 2 3 4 NM_SETTING_WIRELESS_POWERSAVE_DEFAULT (0): use the default value NM_SETTING_WIRELESS_POWERSAVE_IGNORE (1): don't touch existing setting NM_SETTING_WIRELESS_POWERSAVE_DISABLE (2): disable powersave NM_SETTING_WIRELESS_POWERSAVE_ENABLE (3): enable powersave (Informal source on GitHub for these values.) You can check if it is turned off with iwconfig : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 wlan0 IEEE 802.11 ESSID:\"wifiname\" Mode:Managed Frequency:5.18 GHz Access Point: E0:28:6D:48:33:5D Bit Rate=433.3 Mb/s Tx-Power=31 dBm Retry short limit:7 RTS thr:off Fragment thr:off Power Management:off Link Quality=70/70 Signal level=-27 dBm Rx invalid nwid:0 Rx invalid crypt:0 Rx invalid frag:0 Tx excessive retries:1 Invalid misc:0 Missed beacon:0 lo no wireless extensions. wlan1 IEEE 802.11AC ESSID:\"wifiname\" Nickname:\"WIFI@RTL8821CU\" Mode:Managed Frequency:5.18 GHz Access Point: E0:28:6D:48:33:5D Bit Rate:434 Mb/s Sensitivity:0/0 Retry:off RTS thr:off Fragment thr:off Power Management:off Link Quality=64/100 Signal level=-26 dBm Noise level=0 dBm Rx invalid nwid:0 Rx invalid crypt:0 Rx invalid frag:0 Tx excessive retries:0 Invalid misc:0 Missed beacon:0 eth0 no wireless extensions. This is the output when there is one external usb WiFi dongle connected to the Raspberry Pi 4 B and no ethernet cable.","title":"Wifi Issues"},{"location":"rpi-setup/#instructions-to-install-wifi-drivers","text":"In case you use a Realtek USB Wifi dongle it might not be directly supported by the linux kernel. To install the correct driver, you first have to figure out the driver id. When plugging in the USB Wifi dongle and running dmesg afterwards shoud output something similar to: 1 2 3 4 5 [ 1430.931258] usb 1-1.3: new high-speed USB device number 13 using xhci_hcd [ 1431.031913] usb 1-1.3: New USB device found, idVendor=0bda, idProduct=c811, bcdDevice= 2.00 [ 1431.031925] usb 1-1.3: New USB device strings: Mfr=1, Product=2, SerialNumber=3 [ 1431.031932] usb 1-1.3: Product: 802.11ac NIC [ 1431.031939] usb 1-1.3: Manufacturer: Realtek With the idVendor=0bda and idProduct=c811 you can search for the correct driver. In this case it is the 1 RTL8821cu USB 0bda:c811 Realtek WEP WPA WPA2 For it the instructions in the https://github.com/brektrou/rtl8821CU repository can be used to install the driver. Warning These instructions are specifically for the Realtek (id c811) USB wifi dongle. In case you use another USB dongle it might work directly or there exists a ubuntu package. In general try to use Google for instructions on how to install. The following is only an example on what might be needed to get the driver working. After cloning this package, the Makefile needs to be adapted to work for the Raspberry Pi 4B: 1 2 3 4 ###################### Platform Related ####################### CONFIG_PLATFORM_I386_PC = n CONFIG_PLATFORM_ARM_RPI = n CONFIG_PLATFORM_ARM64_RPI = y Then follow the instructions from the README.md: 1 2 3 # For AArch64 sudo cp /lib/modules/$(uname -r)/build/arch/arm64/Makefile /lib/modules/$(uname -r)/build/arch/arm64/Makefile.$(date +%Y%m%d%H%M) sudo sed -i 's/-mgeneral-regs-only//' /lib/modules/$(uname -r)/build/arch/arm64/Makefile and finally make and install the driver: 1 2 make sudo make After a reboot the USB Wifi dongle should be detected and two wifi adapters should show up - the internal wifi module on the RPi and the USB dongle. Sources Disable power management in Stretch How to turn off Wireless power management permanently RPi4: WiFi client crashes often (brcmf_fw_crashed: Firmware has halted or crashed) WLAN Karten Realtek (german)","title":"Instructions to install Wifi drivers"},{"location":"rpi-setup/#prepare-ubuntu","text":"After flashing the image to the sd card insert it to the Pi, hook it up to a monitor via HDMI and power it up by pluggin in the USB-C connector. Then you should follow the installation instructions on the screen. Once finished, follow the next steps to install ROS Noetic. Note To proceed with the next steps on installing ROS and other related dependencies you can run a bash script. Just clone this repository: https://github.com/fjp/diffbot.git and run ubuntu-setup.sh . But to learn more, you should follow the instructions on the following pages.","title":"Prepare Ubuntu"},{"location":"teensy-mcu/","text":"Teensy Setup The Teensy 3.2 microcontroller (MCU) is used to get the ticks from the encoders attached to the motors and send this information (counts) as a message over the /diffbot/ticks_left and /diffbot/ticks_right ropics. For this rosserial is running on the Teensy MCU which allows it to create a node on the Teensy that can communicate with the ROS Master running on the Raspberry Pi. To setup rosserial on the work PC and the Raspberry Pi the following package has to be installed: 1 sudo apt install ros-noetic-rosserial To program the Teensy board with the work PC the Arduino IDE with the Teensyduino add-on can be used. Other options are to use PlatformIO plugin for VSCode. How to install the Arduino IDE and Teensyduino is listed in the instructions on the Teensy website . Here the instructions to setup Teensyduino in Linux are listed: Download the Linux udev rules (link at the top of this page) and copy the file to /etc/udev/rules.d. sudo cp 49-teensy.rules /etc/udev/rules.d/ Download and extract one of Arduino's Linux packages. Note: Arduino from Linux distro packages is not supported. Download the corresponding Teensyduino installer. Run the installer by adding execute permission and then execute it. chmod 755 TeensyduinoInstall.linux64 ./TeensyduinoInstall.linux64 The first step can be used on the work PC and the Raspberry Pi to enable the USB communication with the Teensy board. Step two of these instructions are only necessary on the work PC to actually program the Teensy board. Note Make sure to download the Arduino IDE from the website and don't install it from the Ubuntu repositories. The following video shows installation process, more instructions to setup the Arduino IDE can be found in the ROS wiki . To check if the connection to the Teensy board works use these commands on the Raspberry Pi: 1 2 3 4 5 $ lsusb Bus 002 Device 001: ID 1d6b:0003 Linux Foundation 3.0 root hub Bus 001 Device 003: ID 16c0:0483 Van Ooijen Technische Informatica Teensyduino Serial Bus 001 Device 002: ID 2109:3431 VIA Labs, Inc. Hub Bus 001 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub And to see on which serial port it is connected use: 1 2 $ ls /dev | grep ttyACM0 ttyACM0 If the output is empty it might be the case that the board is connected to another port like ttyUSB0 . Encoder Program When installing Teensyduino new example programs are provided. One of them is to test Encoders. The code for the motor encoders uses it as basis together with a pubsub example from rosserial: TODO link to code encoders.ino After the program is flashed to the Teensy board it can be tested with the following procedure: Start a ROS master by executing roscore in a new terminal. Create a rosserial node using rosserial_python package: 1 2 3 4 5 6 7 8 9 $ rosrun rosserial_python serial_node.py _port: = /dev/ttyACM0 _baud: = 115200 [INFO] [1602784903.659869]: ROS Serial Python Node [INFO] [1602784903.692366]: Connecting to /dev/ttyACM0 at 115200 baud [INFO] [1602784905.809722]: Requesting topics... [INFO] [1602784905.824418]: Note: publish buffer size is 512 bytes [INFO] [1602784905.829712]: Setup publisher on /diffbot/ticks_left [std_msgs/Int32] [INFO] [1602784905.839914]: Setup publisher on /diffbot/ticks_right [std_msgs/Int32] [INFO] [1602784905.856772]: Note: subscribe buffer size is 512 bytes [INFO] [1602784905.861749]: Setup subscriber on /reset [std_msgs/Empty] In case of the following error, probably the wrong program is flashed to the Teensy board: 1 [ERROR] [1602782376.724880]: Unable to sync with device; possible link problem or link software version mismatch such as hydro rosserial_python with groovy Arduino Note Note that the rosserial node needs to be stopped to flash new sketches to the Teensy board. Each DG01D-E motor has two signal pins for its built-in encoder. For these, the Teensy pins 5, 6 are used for the left encoder and 7, 8 are used for the right one, see also the Teensy pinout . Teensy 4.0 Pins. TODO add connection schematic With one motor encoder connected to pins 5, 6, echo the /encoder_ticks topic: 1 rostopic echo /encoder_ticks Rotating a wheel attached to the motor shaft 360 degree (one full turn) will increase the first value of the encoders array: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 --- header: seq: 190323 stamp: secs: 0 nsecs: 0 frame_id: '' encoders: [0, 0] --- header: seq: 190324 stamp: secs: 0 nsecs: 0 frame_id: '' encoders: [230, 0] --- header: seq: 190325 stamp: secs: 0 nsecs: 0 frame_id: '' encoders: [350, 0] --- header: seq: 190326 stamp: secs: 0 nsecs: 0 frame_id: '' encoders: [480, 0] --- header: seq: 190327 stamp: secs: 0 nsecs: 0 frame_id: '' encoders: [540, 0] The found value 540 for a full turn of the wheel is important for the hardware interface.","title":"Teensy MCU"},{"location":"teensy-mcu/#teensy-setup","text":"The Teensy 3.2 microcontroller (MCU) is used to get the ticks from the encoders attached to the motors and send this information (counts) as a message over the /diffbot/ticks_left and /diffbot/ticks_right ropics. For this rosserial is running on the Teensy MCU which allows it to create a node on the Teensy that can communicate with the ROS Master running on the Raspberry Pi. To setup rosserial on the work PC and the Raspberry Pi the following package has to be installed: 1 sudo apt install ros-noetic-rosserial To program the Teensy board with the work PC the Arduino IDE with the Teensyduino add-on can be used. Other options are to use PlatformIO plugin for VSCode. How to install the Arduino IDE and Teensyduino is listed in the instructions on the Teensy website . Here the instructions to setup Teensyduino in Linux are listed: Download the Linux udev rules (link at the top of this page) and copy the file to /etc/udev/rules.d. sudo cp 49-teensy.rules /etc/udev/rules.d/ Download and extract one of Arduino's Linux packages. Note: Arduino from Linux distro packages is not supported. Download the corresponding Teensyduino installer. Run the installer by adding execute permission and then execute it. chmod 755 TeensyduinoInstall.linux64 ./TeensyduinoInstall.linux64 The first step can be used on the work PC and the Raspberry Pi to enable the USB communication with the Teensy board. Step two of these instructions are only necessary on the work PC to actually program the Teensy board. Note Make sure to download the Arduino IDE from the website and don't install it from the Ubuntu repositories. The following video shows installation process, more instructions to setup the Arduino IDE can be found in the ROS wiki . To check if the connection to the Teensy board works use these commands on the Raspberry Pi: 1 2 3 4 5 $ lsusb Bus 002 Device 001: ID 1d6b:0003 Linux Foundation 3.0 root hub Bus 001 Device 003: ID 16c0:0483 Van Ooijen Technische Informatica Teensyduino Serial Bus 001 Device 002: ID 2109:3431 VIA Labs, Inc. Hub Bus 001 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub And to see on which serial port it is connected use: 1 2 $ ls /dev | grep ttyACM0 ttyACM0 If the output is empty it might be the case that the board is connected to another port like ttyUSB0 .","title":"Teensy Setup"},{"location":"teensy-mcu/#encoder-program","text":"When installing Teensyduino new example programs are provided. One of them is to test Encoders. The code for the motor encoders uses it as basis together with a pubsub example from rosserial: TODO link to code encoders.ino After the program is flashed to the Teensy board it can be tested with the following procedure: Start a ROS master by executing roscore in a new terminal. Create a rosserial node using rosserial_python package: 1 2 3 4 5 6 7 8 9 $ rosrun rosserial_python serial_node.py _port: = /dev/ttyACM0 _baud: = 115200 [INFO] [1602784903.659869]: ROS Serial Python Node [INFO] [1602784903.692366]: Connecting to /dev/ttyACM0 at 115200 baud [INFO] [1602784905.809722]: Requesting topics... [INFO] [1602784905.824418]: Note: publish buffer size is 512 bytes [INFO] [1602784905.829712]: Setup publisher on /diffbot/ticks_left [std_msgs/Int32] [INFO] [1602784905.839914]: Setup publisher on /diffbot/ticks_right [std_msgs/Int32] [INFO] [1602784905.856772]: Note: subscribe buffer size is 512 bytes [INFO] [1602784905.861749]: Setup subscriber on /reset [std_msgs/Empty] In case of the following error, probably the wrong program is flashed to the Teensy board: 1 [ERROR] [1602782376.724880]: Unable to sync with device; possible link problem or link software version mismatch such as hydro rosserial_python with groovy Arduino Note Note that the rosserial node needs to be stopped to flash new sketches to the Teensy board. Each DG01D-E motor has two signal pins for its built-in encoder. For these, the Teensy pins 5, 6 are used for the left encoder and 7, 8 are used for the right one, see also the Teensy pinout . Teensy 4.0 Pins. TODO add connection schematic With one motor encoder connected to pins 5, 6, echo the /encoder_ticks topic: 1 rostopic echo /encoder_ticks Rotating a wheel attached to the motor shaft 360 degree (one full turn) will increase the first value of the encoders array: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 --- header: seq: 190323 stamp: secs: 0 nsecs: 0 frame_id: '' encoders: [0, 0] --- header: seq: 190324 stamp: secs: 0 nsecs: 0 frame_id: '' encoders: [230, 0] --- header: seq: 190325 stamp: secs: 0 nsecs: 0 frame_id: '' encoders: [350, 0] --- header: seq: 190326 stamp: secs: 0 nsecs: 0 frame_id: '' encoders: [480, 0] --- header: seq: 190327 stamp: secs: 0 nsecs: 0 frame_id: '' encoders: [540, 0] The found value 540 for a full turn of the wheel is important for the hardware interface.","title":"Encoder Program"},{"location":"packages/raspicam_node/","text":"Raspberry Pi Camera The diffbot robot uses the UbiquityRobotics/raspicam_node to interface the Raspberry Pi Camera v2 . To publish the camera image on the /raw/image/compressed topic run the following on the robot: 1 roslaunch raspicam_node camerav2_410x308_30fps.launch Running rostopic list should show the topics of raspicam node:: 1 2 3 4 /raspicam_node/camera_info /raspicam_node/image/compressed /raspicam_node/parameter_descriptions /raspicam_node/parameter_updates To view the compressed image run the following command on your development PC: 1 rosrun image_view image_view image:=/raspicam_node/image _image_transport:=compressed This will open up a window where you can see the camera image. If it is dark, check if you removed the lense protection ;-)","title":"Rasperry Pi Camera"},{"location":"packages/raspicam_node/#raspberry-pi-camera","text":"The diffbot robot uses the UbiquityRobotics/raspicam_node to interface the Raspberry Pi Camera v2 . To publish the camera image on the /raw/image/compressed topic run the following on the robot: 1 roslaunch raspicam_node camerav2_410x308_30fps.launch Running rostopic list should show the topics of raspicam node:: 1 2 3 4 /raspicam_node/camera_info /raspicam_node/image/compressed /raspicam_node/parameter_descriptions /raspicam_node/parameter_updates To view the compressed image run the following command on your development PC: 1 rosrun image_view image_view image:=/raspicam_node/image _image_transport:=compressed This will open up a window where you can see the camera image. If it is dark, check if you removed the lense protection ;-)","title":"Raspberry Pi Camera"},{"location":"packages/remo_description/","text":"Remo Description ROS URDF description package of REMO robot (Research Education Mobile/Modular robot) a highly modifiable and extendable autonomous mobile robot based on Nvidia's Jetbot . This ROS package is found in the remo_description repository contains the stl files to 3D print Remo robot. Usage This is a ROS package which should be cloned in a catkin workspace. To use remo_description inside a Gazebo simulation or on a real 3D printed Remo robot, you can directly make use of the ROS packages in the ros-mobile-robots/diffbot repository. Most of the launch files you find in the diffbot repository accept a model argument. Just append model:=remo to the end of a roslaunch command to make use of this remo_description package. Camera Types The remo.urdf.xacro accepts a camera_type xacro arg which lets you choose between the following different camera types Raspicam v2 with IMX219 OAK-1 OAK-D Single Board Computer Types Another xacro argument is the sbc_type wher you can select between jetson and rpi . Jetson Nano Raspberry Pi 4 B :handshake: Acknowledgment Louis Morandy-Rapin\u00e9 for his great work on REMO robot and designing it in Fusion 360 . References Nvidia Jetbot","title":"Remo Description"},{"location":"packages/remo_description/#remo-description","text":"ROS URDF description package of REMO robot (Research Education Mobile/Modular robot) a highly modifiable and extendable autonomous mobile robot based on Nvidia's Jetbot . This ROS package is found in the remo_description repository contains the stl files to 3D print Remo robot.","title":"Remo Description"},{"location":"packages/remo_description/#usage","text":"This is a ROS package which should be cloned in a catkin workspace. To use remo_description inside a Gazebo simulation or on a real 3D printed Remo robot, you can directly make use of the ROS packages in the ros-mobile-robots/diffbot repository. Most of the launch files you find in the diffbot repository accept a model argument. Just append model:=remo to the end of a roslaunch command to make use of this remo_description package.","title":"Usage"},{"location":"packages/remo_description/#camera-types","text":"The remo.urdf.xacro accepts a camera_type xacro arg which lets you choose between the following different camera types Raspicam v2 with IMX219 OAK-1 OAK-D","title":"Camera Types"},{"location":"packages/remo_description/#single-board-computer-types","text":"Another xacro argument is the sbc_type wher you can select between jetson and rpi . Jetson Nano Raspberry Pi 4 B","title":"Single Board Computer Types"},{"location":"packages/remo_description/#handshake-acknowledgment","text":"Louis Morandy-Rapin\u00e9 for his great work on REMO robot and designing it in Fusion 360 .","title":":handshake: Acknowledgment"},{"location":"packages/remo_description/#references","text":"Nvidia Jetbot","title":"References"},{"location":"theory/","text":"Theory behind Robotics The following pages will provide an overview of the following important topics of robotics: For more details be sure to check out the Self Driving cars with Duckietown edX MOOC . Also don't forget to read through the resources listed on the main page of this documentation and consult your robotics engineering text books of choice. Modeling and Control Introduction to control systems Representations and models PID control Robot Vision Introduction to projective geometry Camera modeling and calibration Image processing Object Detection Convolutional neural networks One and two stage object detection State Estimation and Localization Bayes filtering framework Parameterized methods (Kalman filter) Sampling-based methods (Particle and histogram filter) Planning I Planning formalization Graphs Planning II Probabilistic roadmaps Sampling-based planning Learning by Reinforcement Markov decision processes Value functions Policy gradients Domain randomization Learning by Imitation Behaviour cloning Online imitation learning Safety and uncertainty Resources Self Driving cars with Duckietown edX MOOC","title":"Theory behind Robotics"},{"location":"theory/#theory-behind-robotics","text":"The following pages will provide an overview of the following important topics of robotics: For more details be sure to check out the Self Driving cars with Duckietown edX MOOC . Also don't forget to read through the resources listed on the main page of this documentation and consult your robotics engineering text books of choice. Modeling and Control Introduction to control systems Representations and models PID control Robot Vision Introduction to projective geometry Camera modeling and calibration Image processing Object Detection Convolutional neural networks One and two stage object detection State Estimation and Localization Bayes filtering framework Parameterized methods (Kalman filter) Sampling-based methods (Particle and histogram filter) Planning I Planning formalization Graphs Planning II Probabilistic roadmaps Sampling-based planning Learning by Reinforcement Markov decision processes Value functions Policy gradients Domain randomization Learning by Imitation Behaviour cloning Online imitation learning Safety and uncertainty","title":"Theory behind Robotics"},{"location":"theory/#resources","text":"Self Driving cars with Duckietown edX MOOC","title":"Resources"},{"location":"theory/actuation/","text":"Actuators Motor Encoder Gearbox Gearboxes can be used to fine-tune the performance characteristics of a motion axis. They're most commonly considered torque multipliers but actually serve several other functions, including speed matching, inertia reduction, and resolution increase ref .","title":"Actuation"},{"location":"theory/actuation/#actuators","text":"","title":"Actuators"},{"location":"theory/actuation/#motor","text":"","title":"Motor"},{"location":"theory/actuation/#encoder","text":"","title":"Encoder"},{"location":"theory/actuation/#gearbox","text":"Gearboxes can be used to fine-tune the performance characteristics of a motion axis. They're most commonly considered torque multipliers but actually serve several other functions, including speed matching, inertia reduction, and resolution increase ref .","title":"Gearbox"},{"location":"theory/motion-and-odometry/","text":"Robotic Motion and Odometry The following section describes the theory of robotic motion and odometry, which is part of the book Elements of Robotics . The section focuses on a detailed look on the quadrature encoders that are attached to the robot wheels. For DiffBot the encoders are part of the motors DG01D-E . This section reviews the basic concepts of distance, time, velocity and acceleration. The physics of motion can be described using calculus, but a computer cannot deal with continuous functions; instead, discrete approximations must be used. Odometry, the fundamental algorithm for computing robotic motion. An approximation of the location of a robot can be obtained by repeatedly computing the distance moved and the change direction from the velocity of the wheels in a short period of time. Unfortunately, odometry is subject to serious errors. It is important to understand that errors in direction are much more significant than errors in distance. See the following video explaining odometry: In the simplest implementation, the speed of the wheels of a robot is assumed to be proportional to the power applied by the motors. To improve the accuracy of odometry wheel encoders can be used, which measure the actual number of revolutions of the wheels. The following video from Sparkfun gives an overview of Encoders Distance, Velocity and Time In general, if a robot moves at a constant applied to the motors it causes the wheels to rotate, which in turn causes the robot velocity \\(v\\) for a period of time \\(t\\) , the distance \\(s\\) it moves is \\(s = v \\cdot t\\) . When power is to move at some velocity. However, we cannot specify that a certain power causes a certain velocity No two electrical or mechanical components are ever precisely identical. The environment affects the velocity of a robot because of different friction of the surface External forces can affect the velocity of a robot. It needs more power to sustain a specific velocity when moving uphill and less power when moving downhill, because the force of gravity decreases and increases the velocity. Note Velocity is speed in a direction. A robot can be moving 10cm/s forwards or backwards; in both cases, the speed is the same but the velocity is different. Acceleration as Change in Velocity To get a true picture of the motion of a robot, we need to divide its motion into small segments \\(s_1,s_2,\\dots\\) and measure the distance and time for each segment individually. Then, we can compute the velocities for each segment. In symbols, if we denote the length of the segment si by \\(\\Delta s_i = x_{i+1} \u2212 x_i\\) and the time it takes the robot to cross segment si by \\(\\Delta t_i = t_{i+1} \u2212 t_i\\) , then \\(v_i\\) , the velocity in segment \\(s_i\\) is given by: \\[ v_i = \\frac{\\Delta s_i}{\\Delta t_i} \\] Acceleration is defined as the change in velocity over a period of time \\[a_i = \\frac{\\Delta v_i}{\\Delta t_i}\\] References Kinematics equations for Differential Drive and Articulated Steering","title":"Motion and Odometry"},{"location":"theory/motion-and-odometry/#robotic-motion-and-odometry","text":"The following section describes the theory of robotic motion and odometry, which is part of the book Elements of Robotics . The section focuses on a detailed look on the quadrature encoders that are attached to the robot wheels. For DiffBot the encoders are part of the motors DG01D-E . This section reviews the basic concepts of distance, time, velocity and acceleration. The physics of motion can be described using calculus, but a computer cannot deal with continuous functions; instead, discrete approximations must be used. Odometry, the fundamental algorithm for computing robotic motion. An approximation of the location of a robot can be obtained by repeatedly computing the distance moved and the change direction from the velocity of the wheels in a short period of time. Unfortunately, odometry is subject to serious errors. It is important to understand that errors in direction are much more significant than errors in distance. See the following video explaining odometry: In the simplest implementation, the speed of the wheels of a robot is assumed to be proportional to the power applied by the motors. To improve the accuracy of odometry wheel encoders can be used, which measure the actual number of revolutions of the wheels. The following video from Sparkfun gives an overview of Encoders","title":"Robotic Motion and Odometry"},{"location":"theory/motion-and-odometry/#distance-velocity-and-time","text":"In general, if a robot moves at a constant applied to the motors it causes the wheels to rotate, which in turn causes the robot velocity \\(v\\) for a period of time \\(t\\) , the distance \\(s\\) it moves is \\(s = v \\cdot t\\) . When power is to move at some velocity. However, we cannot specify that a certain power causes a certain velocity No two electrical or mechanical components are ever precisely identical. The environment affects the velocity of a robot because of different friction of the surface External forces can affect the velocity of a robot. It needs more power to sustain a specific velocity when moving uphill and less power when moving downhill, because the force of gravity decreases and increases the velocity. Note Velocity is speed in a direction. A robot can be moving 10cm/s forwards or backwards; in both cases, the speed is the same but the velocity is different.","title":"Distance, Velocity and Time"},{"location":"theory/motion-and-odometry/#acceleration-as-change-in-velocity","text":"To get a true picture of the motion of a robot, we need to divide its motion into small segments \\(s_1,s_2,\\dots\\) and measure the distance and time for each segment individually. Then, we can compute the velocities for each segment. In symbols, if we denote the length of the segment si by \\(\\Delta s_i = x_{i+1} \u2212 x_i\\) and the time it takes the robot to cross segment si by \\(\\Delta t_i = t_{i+1} \u2212 t_i\\) , then \\(v_i\\) , the velocity in segment \\(s_i\\) is given by: \\[ v_i = \\frac{\\Delta s_i}{\\Delta t_i} \\] Acceleration is defined as the change in velocity over a period of time \\[a_i = \\frac{\\Delta v_i}{\\Delta t_i}\\]","title":"Acceleration as Change in Velocity"},{"location":"theory/motion-and-odometry/#references","text":"Kinematics equations for Differential Drive and Articulated Steering","title":"References"},{"location":"theory/architecture/docker/","text":"Docker Docker Containers A container includes an application and its dependencies. It's goal is to easily ship (deploy) and handle applications. Like real world shipping containers, Docker containers wrap up an application in a filesystem containing everything the application needs to run: source code runtime libraries system tools configruation files The result is that a containerzied application will run identically on any host. And there are no incompatibilieties of any kind. Why Containerization In the traditional operating systems like Linux Ubuntu a package manager install apps. These apps run on shared runtime libraries. Therfore, applications are coupled, because they share the same dependencies. This can can lead to compatibility issues , when, for example, two or more applications requires a different version of the same shared library. When using containerization, each application running in a container comes with its own needed set of libraries. This way, each application container is isolated and can be updated independently. Difference between Containerization and Virtual Machines Containers are different to virtual machines. In a virtual machine environment there is a host system that runs a hypervisor (e.g. VMWare). The hypervisor divides the physical hardware resources among the virtual machines. Each virtual machine runs its own operating system. The disadvantage of this is that there is a considerable overhead and processes cannot communicate over different virtual machines. On the other hand, containerization has minimal overhead and it allow us to deploy multiple applications so that they can communicate with each other.","title":"Docker"},{"location":"theory/architecture/docker/#docker","text":"","title":"Docker"},{"location":"theory/architecture/docker/#docker-containers","text":"A container includes an application and its dependencies. It's goal is to easily ship (deploy) and handle applications. Like real world shipping containers, Docker containers wrap up an application in a filesystem containing everything the application needs to run: source code runtime libraries system tools configruation files The result is that a containerzied application will run identically on any host. And there are no incompatibilieties of any kind.","title":"Docker Containers"},{"location":"theory/architecture/docker/#why-containerization","text":"In the traditional operating systems like Linux Ubuntu a package manager install apps. These apps run on shared runtime libraries. Therfore, applications are coupled, because they share the same dependencies. This can can lead to compatibility issues , when, for example, two or more applications requires a different version of the same shared library. When using containerization, each application running in a container comes with its own needed set of libraries. This way, each application container is isolated and can be updated independently.","title":"Why Containerization"},{"location":"theory/architecture/docker/#difference-between-containerization-and-virtual-machines","text":"Containers are different to virtual machines. In a virtual machine environment there is a host system that runs a hypervisor (e.g. VMWare). The hypervisor divides the physical hardware resources among the virtual machines. Each virtual machine runs its own operating system. The disadvantage of this is that there is a considerable overhead and processes cannot communicate over different virtual machines. On the other hand, containerization has minimal overhead and it allow us to deploy multiple applications so that they can communicate with each other.","title":"Difference between Containerization and Virtual Machines"},{"location":"theory/modeling-control/control-systems-introduction/","text":"Introduction to Control Systems Quote Control systems is the science of making machines behave the way we want them to behave as opposed to how they would naturally behave. Systems have input and output signals and a behavior that evolves over time. Systems can be broken down into components, which are something that we think we understand. In physical systems, inputs are generated by actuators. For example, DC motors or LEDs. Outputs are measured by sensors, for example, cameras or wheel encoders. These sensors produce observations. graph RL A[Actuators] -- Inputs --> P[Process and Environment]; DIR(\"<img src='https://iconscout.com/ms-icon-310x310.png' width='30' />\") P -->|Outputs| S[Sensors]; S -- Observations --> id1[\"<img src='' />\"]; style id1 fill:#ffff,stroke-width:0px In the case of a mobile robot, the system is the real physical robot and its environment around it. This system receives inputs from the robot's actuators, which results in an observable output by the robot's sensors. For example a change in the camera viewport of the environment or a change in encoder ticks because of rotating wheels. Controlling a system means to design a logical component that will output, at every instant in time, the commands for the plant actuators so that the output of our system follow a given plan. Need for Control Systems The main objectives of the controller are the following: Stability Performance Robustness Stability Mathematically, stability can be formalized as Bounded Input Bounded Output . The output of a system will be bounded for every input to the system that is bounded. TODO Add stability image In other words, if finite energy is provided to the system, then finite energy should exit the system. TODO Add stability I/O image According to the definition of stability provided (BIBO stability), a system goes unstable when an output of the system diverges (always continues growing in time), when provided with a finite energy input. When defining the output as \"distance from the center of the lane\", driving outside of the road would be an example of instability. \\begin{tikzpicture} % Draw axes \\draw[help lines, color=gray!30, dashed] (-0.1,-0.1) grid (3.0,2.0); \\draw [<->,thick] (0,2) node (a) [above] {$y$} |- (3,0) node (b) [right] {$x$}; % Draw two intersecting lines \\draw[color=red] (0,0) -- (0,1.5) coordinate (b_1) -- (2.5,1.5) coordinate (b_2); \\end{tikzpicture} \\begin{tikzpicture}[node distance=2.5cm,auto] \\node [draw, fill=blue!20, minimum size=2em, pin={[pin edge={to-,thin,black}]above:$v_0$}] (a) {$\\frac{1}{s}$}; \\node (b) [left of=a,node distance=2cm, coordinate] {a}; \\node [draw, fill=blue!20, minimum size=2em, pin={[pin edge={to-,thin,black}]above:$p_0$}] (c) [right of=a] {$\\frac{1}{s}$}; \\node [coordinate] (end) [right of=c, node distance=2cm]{}; \\path[->] (b) edge node {$a$} (a); \\path[->] (a) edge node {$v$} (c); \\draw[->] (c) edge node {$p$} (end); \\end{tikzpicture} \\begin{tikzpicture} % Draw axes \\draw[help lines, color=gray!30, dashed] (-0.1,-0.1) grid (3.0,2.0); \\draw [<->,thick] (0,2) node (a) [above] {$y$} |- (3,0) node (b) [right] {$x$}; % Draw two intersecting lines \\draw[color=red] (0,0) -- (0,1.5) coordinate (b_1) -- (2.5,1.5) coordinate (b_2); \\end{tikzpicture} \\begin{tikzpicture}[node distance=2.5cm,auto] \\node [draw, fill=blue!20, minimum size=2em, pin={[pin edge={to-,thin,black}]above:$v_0$}] (a) {$\\frac{1}{s}$}; \\node (b) [left of=a,node distance=2cm, coordinate] {a}; \\node [draw, fill=blue!20, minimum size=2em, pin={[pin edge={to-,thin,black}]above:$p_0$}] (c) [right of=a] {$\\frac{1}{s}$}; \\node [coordinate] (end) [right of=c, node distance=2cm]{}; \\path[->] (b) edge node {$a$} (a); \\path[->] (a) edge node {$v$} (c); \\draw[->] (c) edge node {$p$} (end); \\end{tikzpicture} Performance Beside being stable a system needs also to perform well. Performance can be measured in several ways: Time before reaching the target state: How quickly does the system converge to the plan Tracking error: how closely a reference signal is followed or how precisely does the system converge. Maximum acceptable error at any point in time Disturbance rejection: the ability to compensate for external stimuli. How well can the system recover from unexpected external stimuli, such as the noise in the measurements or disturbances, like a sudden gust of wind or hitting a bump on the road. Noise attenuation: the ability to minimize the effect of high-frequency signals added to the measurements or inputs. These are all good examples of performance criteria for controller design. TODO Add performance image with direct path to target and max acceptable error Robustness Robustness, is the ability of the controller to provide stability and performance even in the presence of uncertainty in the mathematical model of the plant. TODO Add model uncertainty image Model uncertainty can mean that the wheels of the robot are slightly of different sizes or the disposition of mass is different than what's assumed. Also the parameters parameters of the system might just change over time because of wear and tear. A robust controller would handle these cases well despite the uncertainty. Robustness and performance are trade offs and striking the right balance is a challenge that really depends on the application. Model uncertainty is defined as a \"bounded variation\" of the parameters describing the controlled system's model. Choosing completely different sensors (e.g., a camera instead of the planned pressure sensor for altitude control of a hot air balloon) might induce a completely different structure in the plant's model, which wouldn't not therefore be a \"bounded\" variation. \"Bounded variation\" in the system behavior of a mobile robot could count as: Wear and tear of components over time. Slight imperfections in the assembly. Controller Structure TODO Add stability, performance, robustness images With the mentioned objectives in mind, stability, performance, and robustness, it is important to think about the structure of the controller. Open Loop Control Close Loop Control The simplest approach is for the user to send a predetermined sequence of commands directly to the actuators, which is called open loop control. TODO Add open loop image Open Loop Pros it is stable it is convenient works when the model of the system is good Open Loop Cons everything must be planned in advance If our understanding of the platform response is good, we will obtain the desired outcome. If it is not good or if something unexpected happens during the execution, then the end result will diverge from the plan. In open loop control, information flows only in one direction, from the controller to the plant. To enable the controller to take into account the actual execution, it is possible to close the loop by feeding the sensor measurements back to the controller, creating a feedback loop. TODO Add closed loop image Closed Feedback Loop Pros it adapts to circumstances Closed Feedback Loop Cons it is less convenient it can destabalize a stable system Feedback control is very powerful because it allows the whole system to adapt to circumstances as they are unfolding and apply corrections on the fly. The measurements themselves, though, generally need to be processed before being fed back to the controller. This is required because the raw measurements might include a lot of redundant data straight out of the sensors. How to structure this agent is up to the designer, to us. TODO Add closed loop empty agent image We could use different approaches, for example, a deep neural network trained from real data or a simulation to translate the images from the camera directly into commands to the wheels to keep the robot following a desired plan. Traditionally, agents are designed with three main components, perception, planning, and control: to see, to plan, and to act. TODO Add closed loop designed agent image Perception Perception graph LR id1[\"<img src='' />\"] -- observations --> P[perception] subgraph agent/controller P --> |estimate/belief|PL[planner] --> C[controller] P --> |estimate/belief|C end C -- commands --> id2[\"<img src='' />\"] style id1 fill:#ffff,stroke-width:0px style id2 fill:#ffff,stroke-width:0px TODO Add closed loop empty agent image: perception highlighted The perception block is responsible for transforming the data from the sensors into actionable information for the robot. This passage is sometimes called filtering or estimation. These estimates of relevant quantities, for example, the position or the orientation of the robot's reference frame with respect to the world and the other objects in it, represent what is the robot's belief of his current state. This belief might be more or less corresponding to the truth depending on the quality of the measurements and the perception solution used. Planner TODO Add closed loop empty agent image: planner highlighted The planner instead provides a reference path for the robot to follow. For example, between two navigation points in a more general task of reaching a goal position while avoiding obstacles on the road. The planner receives the state estimate as input so it can adjust the nominal plan on the fly. Controller TODO Add closed loop empty agent image: controller highlighted The plan and the state estimate are finally fed into the actual controller, which uses them to compute a decision applying a certain logic. For a mobile robot, the decision of the controller will be a sequence of commands which will be sent to the motors, finally closing the loop. Summary Systems are input/output relations we can formalize Control is about making the system's output follow a given plan The control objectives are stability, performance and robustness The traditional feedback control architecture includes perception planning and control steps Designing a controller allows to have systems behave in a desired way, rather than following their natural dynamics. Although we can measure quantities of interest and drive the system through actuators, not all systems are controllable. Some systems simply can't be controlled. Systems have their natural dynamics, fundamentally dictated by the laws of physics. More often than not, just \"letting the system go\" will not meet the user's requirements. Control systems leverage our abilities to measure quantities of interest and to actuate (or influence the physical world through devices), to drive the system where we want, and how we want. In practice a controller consists of lines of code or one or more mechanical devices. A controller is typically a logic , that outputs decisions . These decisions are translated to the real world through actuators. It is actually possible to create control logics with analog devices too. Stability is the first design objective for most controllers because unstable systems are potentially unsafe. An unstable system might lead to overshooting driving behaviour, not following a reference path and even driving outside of the road without heading back. There are very few instances in which sending a system unstable might be desirable (exceptions being, e.g., acrobatic flight or selected military/destructive applications). An unstable system will behave unpredictably, potentially with catastrophic results. References Self Driving Cars with Duckietown","title":"Introduction to Control Systems"},{"location":"theory/modeling-control/control-systems-introduction/#introduction-to-control-systems","text":"Quote Control systems is the science of making machines behave the way we want them to behave as opposed to how they would naturally behave. Systems have input and output signals and a behavior that evolves over time. Systems can be broken down into components, which are something that we think we understand. In physical systems, inputs are generated by actuators. For example, DC motors or LEDs. Outputs are measured by sensors, for example, cameras or wheel encoders. These sensors produce observations. graph RL A[Actuators] -- Inputs --> P[Process and Environment]; DIR(\"<img src='https://iconscout.com/ms-icon-310x310.png' width='30' />\") P -->|Outputs| S[Sensors]; S -- Observations --> id1[\"<img src='' />\"]; style id1 fill:#ffff,stroke-width:0px In the case of a mobile robot, the system is the real physical robot and its environment around it. This system receives inputs from the robot's actuators, which results in an observable output by the robot's sensors. For example a change in the camera viewport of the environment or a change in encoder ticks because of rotating wheels. Controlling a system means to design a logical component that will output, at every instant in time, the commands for the plant actuators so that the output of our system follow a given plan.","title":"Introduction to Control Systems"},{"location":"theory/modeling-control/control-systems-introduction/#need-for-control-systems","text":"The main objectives of the controller are the following: Stability Performance Robustness","title":"Need for Control Systems"},{"location":"theory/modeling-control/control-systems-introduction/#stability","text":"Mathematically, stability can be formalized as Bounded Input Bounded Output . The output of a system will be bounded for every input to the system that is bounded. TODO Add stability image In other words, if finite energy is provided to the system, then finite energy should exit the system. TODO Add stability I/O image According to the definition of stability provided (BIBO stability), a system goes unstable when an output of the system diverges (always continues growing in time), when provided with a finite energy input. When defining the output as \"distance from the center of the lane\", driving outside of the road would be an example of instability. \\begin{tikzpicture} % Draw axes \\draw[help lines, color=gray!30, dashed] (-0.1,-0.1) grid (3.0,2.0); \\draw [<->,thick] (0,2) node (a) [above] {$y$} |- (3,0) node (b) [right] {$x$}; % Draw two intersecting lines \\draw[color=red] (0,0) -- (0,1.5) coordinate (b_1) -- (2.5,1.5) coordinate (b_2); \\end{tikzpicture} \\begin{tikzpicture}[node distance=2.5cm,auto] \\node [draw, fill=blue!20, minimum size=2em, pin={[pin edge={to-,thin,black}]above:$v_0$}] (a) {$\\frac{1}{s}$}; \\node (b) [left of=a,node distance=2cm, coordinate] {a}; \\node [draw, fill=blue!20, minimum size=2em, pin={[pin edge={to-,thin,black}]above:$p_0$}] (c) [right of=a] {$\\frac{1}{s}$}; \\node [coordinate] (end) [right of=c, node distance=2cm]{}; \\path[->] (b) edge node {$a$} (a); \\path[->] (a) edge node {$v$} (c); \\draw[->] (c) edge node {$p$} (end); \\end{tikzpicture} \\begin{tikzpicture} % Draw axes \\draw[help lines, color=gray!30, dashed] (-0.1,-0.1) grid (3.0,2.0); \\draw [<->,thick] (0,2) node (a) [above] {$y$} |- (3,0) node (b) [right] {$x$}; % Draw two intersecting lines \\draw[color=red] (0,0) -- (0,1.5) coordinate (b_1) -- (2.5,1.5) coordinate (b_2); \\end{tikzpicture} \\begin{tikzpicture}[node distance=2.5cm,auto] \\node [draw, fill=blue!20, minimum size=2em, pin={[pin edge={to-,thin,black}]above:$v_0$}] (a) {$\\frac{1}{s}$}; \\node (b) [left of=a,node distance=2cm, coordinate] {a}; \\node [draw, fill=blue!20, minimum size=2em, pin={[pin edge={to-,thin,black}]above:$p_0$}] (c) [right of=a] {$\\frac{1}{s}$}; \\node [coordinate] (end) [right of=c, node distance=2cm]{}; \\path[->] (b) edge node {$a$} (a); \\path[->] (a) edge node {$v$} (c); \\draw[->] (c) edge node {$p$} (end); \\end{tikzpicture}","title":"Stability"},{"location":"theory/modeling-control/control-systems-introduction/#performance","text":"Beside being stable a system needs also to perform well. Performance can be measured in several ways: Time before reaching the target state: How quickly does the system converge to the plan Tracking error: how closely a reference signal is followed or how precisely does the system converge. Maximum acceptable error at any point in time Disturbance rejection: the ability to compensate for external stimuli. How well can the system recover from unexpected external stimuli, such as the noise in the measurements or disturbances, like a sudden gust of wind or hitting a bump on the road. Noise attenuation: the ability to minimize the effect of high-frequency signals added to the measurements or inputs. These are all good examples of performance criteria for controller design. TODO Add performance image with direct path to target and max acceptable error","title":"Performance"},{"location":"theory/modeling-control/control-systems-introduction/#robustness","text":"Robustness, is the ability of the controller to provide stability and performance even in the presence of uncertainty in the mathematical model of the plant. TODO Add model uncertainty image Model uncertainty can mean that the wheels of the robot are slightly of different sizes or the disposition of mass is different than what's assumed. Also the parameters parameters of the system might just change over time because of wear and tear. A robust controller would handle these cases well despite the uncertainty. Robustness and performance are trade offs and striking the right balance is a challenge that really depends on the application. Model uncertainty is defined as a \"bounded variation\" of the parameters describing the controlled system's model. Choosing completely different sensors (e.g., a camera instead of the planned pressure sensor for altitude control of a hot air balloon) might induce a completely different structure in the plant's model, which wouldn't not therefore be a \"bounded\" variation. \"Bounded variation\" in the system behavior of a mobile robot could count as: Wear and tear of components over time. Slight imperfections in the assembly.","title":"Robustness"},{"location":"theory/modeling-control/control-systems-introduction/#controller-structure","text":"TODO Add stability, performance, robustness images With the mentioned objectives in mind, stability, performance, and robustness, it is important to think about the structure of the controller. Open Loop Control Close Loop Control The simplest approach is for the user to send a predetermined sequence of commands directly to the actuators, which is called open loop control. TODO Add open loop image Open Loop Pros it is stable it is convenient works when the model of the system is good Open Loop Cons everything must be planned in advance If our understanding of the platform response is good, we will obtain the desired outcome. If it is not good or if something unexpected happens during the execution, then the end result will diverge from the plan. In open loop control, information flows only in one direction, from the controller to the plant. To enable the controller to take into account the actual execution, it is possible to close the loop by feeding the sensor measurements back to the controller, creating a feedback loop. TODO Add closed loop image Closed Feedback Loop Pros it adapts to circumstances Closed Feedback Loop Cons it is less convenient it can destabalize a stable system Feedback control is very powerful because it allows the whole system to adapt to circumstances as they are unfolding and apply corrections on the fly. The measurements themselves, though, generally need to be processed before being fed back to the controller. This is required because the raw measurements might include a lot of redundant data straight out of the sensors. How to structure this agent is up to the designer, to us. TODO Add closed loop empty agent image We could use different approaches, for example, a deep neural network trained from real data or a simulation to translate the images from the camera directly into commands to the wheels to keep the robot following a desired plan. Traditionally, agents are designed with three main components, perception, planning, and control: to see, to plan, and to act. TODO Add closed loop designed agent image","title":"Controller Structure"},{"location":"theory/modeling-control/control-systems-introduction/#perception","text":"Perception graph LR id1[\"<img src='' />\"] -- observations --> P[perception] subgraph agent/controller P --> |estimate/belief|PL[planner] --> C[controller] P --> |estimate/belief|C end C -- commands --> id2[\"<img src='' />\"] style id1 fill:#ffff,stroke-width:0px style id2 fill:#ffff,stroke-width:0px TODO Add closed loop empty agent image: perception highlighted The perception block is responsible for transforming the data from the sensors into actionable information for the robot. This passage is sometimes called filtering or estimation. These estimates of relevant quantities, for example, the position or the orientation of the robot's reference frame with respect to the world and the other objects in it, represent what is the robot's belief of his current state. This belief might be more or less corresponding to the truth depending on the quality of the measurements and the perception solution used.","title":"Perception"},{"location":"theory/modeling-control/control-systems-introduction/#planner","text":"TODO Add closed loop empty agent image: planner highlighted The planner instead provides a reference path for the robot to follow. For example, between two navigation points in a more general task of reaching a goal position while avoiding obstacles on the road. The planner receives the state estimate as input so it can adjust the nominal plan on the fly.","title":"Planner"},{"location":"theory/modeling-control/control-systems-introduction/#controller","text":"TODO Add closed loop empty agent image: controller highlighted The plan and the state estimate are finally fed into the actual controller, which uses them to compute a decision applying a certain logic. For a mobile robot, the decision of the controller will be a sequence of commands which will be sent to the motors, finally closing the loop.","title":"Controller"},{"location":"theory/modeling-control/control-systems-introduction/#summary","text":"Systems are input/output relations we can formalize Control is about making the system's output follow a given plan The control objectives are stability, performance and robustness The traditional feedback control architecture includes perception planning and control steps Designing a controller allows to have systems behave in a desired way, rather than following their natural dynamics. Although we can measure quantities of interest and drive the system through actuators, not all systems are controllable. Some systems simply can't be controlled. Systems have their natural dynamics, fundamentally dictated by the laws of physics. More often than not, just \"letting the system go\" will not meet the user's requirements. Control systems leverage our abilities to measure quantities of interest and to actuate (or influence the physical world through devices), to drive the system where we want, and how we want. In practice a controller consists of lines of code or one or more mechanical devices. A controller is typically a logic , that outputs decisions . These decisions are translated to the real world through actuators. It is actually possible to create control logics with analog devices too. Stability is the first design objective for most controllers because unstable systems are potentially unsafe. An unstable system might lead to overshooting driving behaviour, not following a reference path and even driving outside of the road without heading back. There are very few instances in which sending a system unstable might be desirable (exceptions being, e.g., acrobatic flight or selected military/destructive applications). An unstable system will behave unpredictably, potentially with catastrophic results.","title":"Summary"},{"location":"theory/modeling-control/control-systems-introduction/#references","text":"Self Driving Cars with Duckietown","title":"References"},{"location":"theory/modeling-control/modeling-differential-drive-robot/","text":"Modeling of a differential drive robot Mathematical models are powerful tools because they allow us to predict the future. \\[ \\begin{align} \\dot{x}_t &= f(x_t,{\\color{orange}{u_t}}) \\\\ {\\color{green}{y}_t} &= g(x_t,{\\color{orange}{u_t}}) \\end{align} \\] Models map between inputs and outputs of systems and can be derived from first principles or learned from data. TODO Add robot input output image We use models to quantify some essential variable that is useful to accomplish a task, not to provide a faithful description of the exact reality of all the physical processes going on. The Diffbot is a differential drive robot, where the motion of each wheel is controlled by one DC motor. TODO Add image of robot and two dc motors as input DC motors receive voltages \\({\\color{orange}V_{l/r,t}}\\) as inputs and produce torques on the motor drive axis that spins the wheels, and leads to angular velocity \\(\\dot{\\phi}_{l/r,t}\\) of the motor shaft and wheel. The movement of the wheels will produce an evolution of the robots pose \\({\\color{green}q_t}\\) over time, which is what we want to quantify. Forward and Inverse Kinematics Through these models, we can answer two questions. (Forward Kinematics) Given a sequence of commands to the wheels \\(\\dot{\\phi}_{1}, \\dot{\\phi}_{2}, \\cdots, \\dot{\\phi}_{t}\\) , how will the robot move? (Inverse Kinematics) If we want the robot to move in a certain way, given a desired movement \\((q_1, q_2, \\cdots, q_t)\\) , what commands should we send to the wheels? Notations We know that the pose of a robot is the position and the orientation of the body frame with respect to the world frame. We define the robot body frame so that the origin, \\(A\\) , is in the mid-axle point. World Frame: \\(\\{{\\color{blue}x^{w}}, {\\color{blue}y^{w}}\\}\\) Body (robot) frame: \\(\\{{\\color{orange}x^{r}}, {\\color{orange}y^{r}}\\}\\) TODO Add image of robot including orange reference frame and blue world reference frame. Assumption 1 : robot is symmetric along longitudinal axis ( \\(x^r\\) ) and we take it as the x direction of the robot frame. Equidsitand wheels (axle length = \\(2L\\) ). Both wheels will be at a distance \\(L\\) from point \\(A\\) . Identical wheels with diameter \\(R\\) ($R_l = R_r = R) Center of mass of the robot will lie on x-axis \\(x^r\\) at distance \\(c\\) from \\(A\\) Assumption 2 : robot chassis is rigid body . Distance between any two points of the robot does not change in time. in particular \\(\\dot{c} = 0\\) , whre \\((\\dot{\\star}) = \\frac{d(\\star)}{dt}\\) .","title":"Modeling a Differential Drive Robot"},{"location":"theory/modeling-control/modeling-differential-drive-robot/#modeling-of-a-differential-drive-robot","text":"Mathematical models are powerful tools because they allow us to predict the future. \\[ \\begin{align} \\dot{x}_t &= f(x_t,{\\color{orange}{u_t}}) \\\\ {\\color{green}{y}_t} &= g(x_t,{\\color{orange}{u_t}}) \\end{align} \\] Models map between inputs and outputs of systems and can be derived from first principles or learned from data. TODO Add robot input output image We use models to quantify some essential variable that is useful to accomplish a task, not to provide a faithful description of the exact reality of all the physical processes going on. The Diffbot is a differential drive robot, where the motion of each wheel is controlled by one DC motor. TODO Add image of robot and two dc motors as input DC motors receive voltages \\({\\color{orange}V_{l/r,t}}\\) as inputs and produce torques on the motor drive axis that spins the wheels, and leads to angular velocity \\(\\dot{\\phi}_{l/r,t}\\) of the motor shaft and wheel. The movement of the wheels will produce an evolution of the robots pose \\({\\color{green}q_t}\\) over time, which is what we want to quantify.","title":"Modeling of a differential drive robot"},{"location":"theory/modeling-control/modeling-differential-drive-robot/#forward-and-inverse-kinematics","text":"Through these models, we can answer two questions. (Forward Kinematics) Given a sequence of commands to the wheels \\(\\dot{\\phi}_{1}, \\dot{\\phi}_{2}, \\cdots, \\dot{\\phi}_{t}\\) , how will the robot move? (Inverse Kinematics) If we want the robot to move in a certain way, given a desired movement \\((q_1, q_2, \\cdots, q_t)\\) , what commands should we send to the wheels?","title":"Forward and Inverse Kinematics"},{"location":"theory/modeling-control/modeling-differential-drive-robot/#notations","text":"We know that the pose of a robot is the position and the orientation of the body frame with respect to the world frame. We define the robot body frame so that the origin, \\(A\\) , is in the mid-axle point. World Frame: \\(\\{{\\color{blue}x^{w}}, {\\color{blue}y^{w}}\\}\\) Body (robot) frame: \\(\\{{\\color{orange}x^{r}}, {\\color{orange}y^{r}}\\}\\) TODO Add image of robot including orange reference frame and blue world reference frame. Assumption 1 : robot is symmetric along longitudinal axis ( \\(x^r\\) ) and we take it as the x direction of the robot frame. Equidsitand wheels (axle length = \\(2L\\) ). Both wheels will be at a distance \\(L\\) from point \\(A\\) . Identical wheels with diameter \\(R\\) ($R_l = R_r = R) Center of mass of the robot will lie on x-axis \\(x^r\\) at distance \\(c\\) from \\(A\\) Assumption 2 : robot chassis is rigid body . Distance between any two points of the robot does not change in time. in particular \\(\\dot{c} = 0\\) , whre \\((\\dot{\\star}) = \\frac{d(\\star)}{dt}\\) .","title":"Notations"},{"location":"theory/modeling-control/odometry/","text":"Odometry As robots move in the world to reach an objective or avoid an obstacle, it is important for them to know where they are. Through odometry, robots can update their pose in time, as long as they know where they started from. Odometry comes from the Greek words \u1f41\u03b4\u1f79\u03c2 [odos] (route) and \u03bc\u1f73\u03c4\u03c1\u03bf\u03bd [metron] (measurement), which literally mean: \"measurement of the route\". The odometry problem can be formulated as: given an initial pose \\(q\\) of \\(t_0\\) at some initial time, find the pose at any future time \\(t_0 + \\Delta t\\) . Given: \\(q(t_0) = q_{t_0} = q_0 = \\begin{bmatrix}x_0 & y_0 & \\theta_0 \\end{bmatrix}^T\\) Find: \\(q_{t_0 + \\Delta t}, \\forall \\Delta t > 0\\) When \\(\\Delta t\\) is small enough to consider the angular speed of the wheels constant, the pose update can be approximated as a simple sum. \\(q_{t} = \\begin{bmatrix}x_t & y_t & \\theta_t \\end{bmatrix}^T\\) \\(q_{t_{k+1}} = q_{t_k} + \\dot{q}_{t_k}(t_{k+1} - t_k)\\) The process can then be applied iteratively to find the pose at any time, and at each iteration using the previous estimate as an initial condition. TODO Add odometry update image in world frame The essence of odometry is to use the measurements of the distance traveled by each wheel in a certain time interval and use them to derive the linear and the angular displacement of the robot in time through a motion model. TODO Add odometry update image with \\(\\Delta x\\) and \\(\\Delta y\\) displacements For example, if our robot starts at time 0 with pose (0, 0, 0), and drives straight for 1 second at the speed of 1 meter per second, the odometery will tell us that the final pose will be (1 meter, 0, 0). Recall, the kinematics model, that maps the wheel velocities to the variation of the pose in time. \\[ q_{t_{k+1}} \\approx q_{t_k} + {\\color{orange}{\\dot{q}_{t_k}}}(t_{k+1} - t_k) \\] Kinematic model \\[ {\\color{orange}{\\dot{q}_{t}}} = \\begin{bmatrix}\\dot{x}_t \\\\ \\dot{y}_t \\\\ \\dot{\\theta}_t \\end{bmatrix} = \\frac{ \\color{green}{R} }{2} \\begin{bmatrix}cos(\\theta_t) & 0 \\\\ sin(\\theta_t) & 0 \\\\ 0 & 1\\end{bmatrix} \\begin{bmatrix}1 & 1 \\\\ \\frac{1}{ \\color{green}{L} } & -\\frac{1}{ \\color{green}{L} } \\end{bmatrix} \\begin{bmatrix} \\color{red}{\\dot{\\phi}_{r,t}} \\\\ \\color{red}{\\dot{\\phi}_{l,t}} \\end{bmatrix} \\] This model allows us to perform the pose update once we determine its parameters, which are the wheel radii, which we assume identical, and the distance between the wheels, or the baseline. Parameters \\({\\color{green}{R}}\\) : wheel radius \\(2 {\\color{green}{L}}\\) : baseline (distance between wheels) What's also needed is to measure the wheel angular velocities. Measurements \\(\\color{red}{\\dot{\\phi}_{l,t}}\\) : wheel angular speeds Wheel Encoders To measure the wheel angular velocities, we can use wheel encoders. Although there are various implementations, the operation principle of wheel encoders is simple. The outer rim of the motor's rotor has some evenly spaced, fixed markers. Optical encoders, for example, have a perforated disk where the empty space are the markers. TODO Add encoder wheel image robotc.net One pulse every \\({\\color{red}{\\alpha}} = \\frac{2\\pi}{N_{tot}}\\) radians Every time any of these markers transitions through some reference position on the stator, the marker is sensed, and a signal is produced. For example, in optical encoders, a light sensor will pick up the light from a source every time an empty space in the disk comes about. Knowing how many markers there are in a whole circumference, we can derive how much each wheel rotated by just counting the pulses in each of the k-th time interval. Wheel rotation (in \\(\\Delta t_k\\) ): \\({\\color{orange}{\\Delta \\phi_k}} = N_k \\cdot {\\color{red}{\\alpha}}\\) By dividing the total rotation by delta t, we can then measure the average wheel angular speed in that time frame. Angular speed: \\(\\dot{\\phi}_{t_k} \\approx \\frac{ {\\color{orange}{\\Delta \\phi_k}} }{\\Delta t_k}\\) Expanding the kinematics model expressions, we gain insight on the pose update process. \\[ \\begin{align} &&{\\color{orange}{\\dot{q}_{t}}} &= \\begin{bmatrix}\\dot{x}_t \\\\ \\dot{y}_t \\\\ \\dot{\\theta}_t \\end{bmatrix} = \\frac{ \\color{green}{R} }{2} \\begin{bmatrix}cos(\\theta_t) & 0 \\\\ sin(\\theta_t) & 0 \\\\ 0 & 1\\end{bmatrix} \\begin{bmatrix}1 & 1 \\\\ \\frac{1}{ \\color{green}{L} } & -\\frac{1}{ \\color{green}{L} } \\end{bmatrix} \\begin{bmatrix} \\color{red}{\\dot{\\phi}_{r,t}} \\\\ \\color{red}{\\dot{\\phi}_{l,t}} \\end{bmatrix} {\\color{red}{\\longleftarrow}} {\\color{red}{\\dot{\\phi}_{t_k}}} \\approx \\frac{ \\Delta \\phi_k }{\\Delta t_k} \\\\ &&{\\color{orange}{\\downarrow}} \\\\ && {\\color{green}{q_{t_{k+1}}}} \\approx q_{t_k} + &{\\color{orange}{\\dot{q}_{t_k}}}(t_{k+1} - t_k) \\\\ &&{\\color{green}{\\downarrow}} \\\\ &&{\\color{green}{x_{t_{k+1}}}} \\approx x_{t_k} + & \\frac{R}{2 \\cancel{\\Delta t_k}} \\left(\\Delta \\phi_{r,t} + \\Delta \\phi_{l,t} \\right) \\cos(\\theta_{t_k}) \\cancel{\\Delta t_k} \\\\ &&{\\color{green}{y_{t_{k+1}}}} \\approx y_{t_k} + & \\frac{R}{2} \\left(\\Delta \\phi_{r,t} + \\Delta \\phi_{l,t} \\right) \\sin(\\theta_{t_k}) \\\\ &&{\\color{green}{\\theta_{t_{k+1}}}} \\approx \\theta_{t_k} + & \\frac{R}{2L} \\left(\\Delta \\phi_{r,t} - \\Delta \\phi_{l,t} \\right) \\\\ \\end{align} \\] Notice how the time intervals cancel out, so we don't need to actually compute the angular speed of each wheel, but just the total rotation. The first step in solving the odometry is transforming the wheel rotations into traveled distances for each wheel. We count the pulses from the encoders, derive the rotation of each wheel, and then multiply by the radius of each wheel. \\[ \\begin{align} x_{t_{k+1}} &\\approx x_{t_k} + \\frac{ {\\color{orange}{R}} }{2} \\left({\\color{orange}{\\Delta\\phi_{r,t}}} + {\\color{orange}{\\Delta\\phi_{l,t}}} \\right) \\cos(\\theta_{t_k}) \\\\ y_{t_{k+1}} &\\approx y_{t_k} + \\frac{ {\\color{orange}{R}} }{2} \\left({\\color{orange}{\\Delta\\phi_{r,t}}} + {\\color{orange}{\\Delta\\phi_{l,t}}} \\right) \\sin(\\theta_{t_k}) \\\\ \\theta_{t_{k+1}} &\\approx \\theta_{t_k} + \\frac{ {\\color{orange}{R}} }{2L} \\left({\\color{orange}{\\Delta\\phi_{r,t}}} - {\\color{orange}{\\Delta\\phi_{l,t}}} \\right) \\\\ \\end{align} \\] Wheel travelled distance: \\(\\color{orange}{d_{l/r}} = R \\cdot \\Delta \\phi_{r/l}\\) TODO Add robot image with \\(d_l\\) \\(d_r\\) travelled distance The second step is to transform the wheel displacements into the linear and the angular displacements of the robot reference frame, as we have seen in the modeling section . \\[ \\begin{align} x_{t_{k+1}} &\\approx x_{t_k} + {\\color{orange}{ \\frac{ d_{r,t_k} + d_{l,t_k} }{2} }} \\cos(\\theta_{t_k}) \\\\ y_{t_{k+1}} &\\approx y_{t_k} + {\\color{orange}{ \\frac{ d_{r,t_k} + d_{l,t_k} }{2} }} \\sin(\\theta_{t_k}) \\\\ \\theta_{t_{k+1}} &\\approx \\theta_{t_k} + {\\color{red}{ \\frac{ d_{r,t_k} - d_{l,t_k} }{2L} }} \\\\ \\end{align} \\] TODO Add robot image with \\(d_l\\) \\(d_r\\) travelled distance and \\(\\Delta \\theta\\) The final step is to represent the displacement in the world frame and add the increments to the previous estimates. \\[ \\begin{align} x_{t_{k+1}} &\\approx x_{t_k} + {\\color{orange}{ d_{A,t_k} \\cos(\\theta_{t_k}) }} \\\\ y_{t_{k+1}} &\\approx y_{t_k} + {\\color{orange}{ d_{A,t_k} \\sin(\\theta_{t_k}) }} \\\\ \\theta_{t_{k+1}} &\\approx \\theta_{t_k} + \\Delta \\theta_{t_k} \\\\ \\end{align} \\] TODO Add robot image showing increment update in world frame Summary of Odometry Equations \\[ \\begin{align} x_{t_{k+1}} &\\approx x_{t_k} + d_{A,t_k} \\cos(\\theta_{t_k}) \\\\ y_{t_{k+1}} &\\approx y_{t_k} + d_{A,t_k} \\sin(\\theta_{t_k}) \\\\ \\theta_{t_{k+1}} &\\approx \\theta_{t_k} + \\Delta \\theta_{t_k} \\\\ \\end{align} \\] \\[ \\begin{align} d_{A,t_k} &= \\frac{ d_{r,t_k} + d_{l,t_k} }{2} \\\\ \\Delta \\theta_{t_k} &= \\frac{ d_{r,t_k} - d_{l,t_k} }{2L} \\\\ d_{r/l,t_k} &= 2\\pi R \\frac{N_k}{N_{tot}} \\end{align} \\] Challenges in Odometry There are practical challenges in odometry. \"Dead reconing\" The first practical challenge stems from using this dead reckoning approach, which is the official name of always adding an increment to a previous estimate in order to obtain a new one. \\[ \\begin{align} x_{t_{k+1}} &\\approx x_{t_k} + \\Delta x_{t_k} \\\\ y_{t_{k+1}} &\\approx y_{t_k} + \\Delta y_{t_k} \\\\ \\theta_{t_{k+1}} &\\approx \\theta_{t_k} + \\Delta \\theta_{t_k} \\\\ \\end{align} \\] TODO Reuse odometry update image in world frame While it might work well for short distances, over time, errors like the discrete time approximation will accumulate, making the estimate drift from reality. Kinematic Model Second, we're using a mathematical model, that of the kinematics of a differential drive robot, to translate the actual measurements to the pose of the robot. \\[ \\dot{q}_{t} = \\frac{R}{2} \\begin{bmatrix}cos(\\theta_t) & 0 \\\\ sin(\\theta_t) & 0 \\\\ 0 & 1\\end{bmatrix} \\begin{bmatrix}1 & 1 \\\\ \\frac{1}{L} & -\\frac{1}{L} \\end{bmatrix} \\begin{bmatrix}\\dot{\\phi}_{r,t} \\\\ \\dot{\\phi}_{l,t} \\end{bmatrix} \\] You might recall that we previously said that all models are wrong, although some are useful. This wisdom is ever more true when the assumptions of the model are not respected. Wheel Slip In particular, we impose the condition of no slippage of the wheels. TODO Add wheel slip images/animation When the wheels slip, it means that the motor will be spinning, the encoders will be producing measurements, but the robot will not be moving the same distance as we are assuming. This will induce errors in the odometry, and they will compound over time. Odometry Calibration Finally, we need to use some actual numerical values for the parameters of the model: the wheel radii - which, by the way, are assumed to be identical, but will they really be - and the robot baseline. \\[ \\begin{align} \\Delta \\theta_{t_k} &= \\frac{ d_{r,t_k} - d_{l,t_k} }{2 {\\color{red}{L}} } \\\\ d_{r/l,t_k} &= 2\\pi {\\color{red}{R}} \\frac{N_k}{N_{tot}} \\end{align} \\] Accurately measuring these parameters is very important. Even small imperfections will induce systematic errors in the odometry that, again, will compound over time. Note that although nominally identical, no two real-world physical robots will ever be the same due to manufacturing, assembly, or handling differences. To find the values of the parameters of the model that best fit our robot, we will need to perform an odometry calibration procedure. Summary Wheel encoders can be used to update the robot's pose in time: Measure the motor's angular displacements \\(\\Delta \\phi\\) in \\(\\Delta t\\) Use the kinematics mdoel to find the robot's \\(\\Delta x\\) , \\(\\Delta y\\) , \\(\\Delta \\theta\\) Update the pose by adding the calculated increments Subject to dfit in time due to accumulation of numerical, slipping/skidding and calibration impercision errors.","title":"Odometry"},{"location":"theory/modeling-control/odometry/#odometry","text":"As robots move in the world to reach an objective or avoid an obstacle, it is important for them to know where they are. Through odometry, robots can update their pose in time, as long as they know where they started from. Odometry comes from the Greek words \u1f41\u03b4\u1f79\u03c2 [odos] (route) and \u03bc\u1f73\u03c4\u03c1\u03bf\u03bd [metron] (measurement), which literally mean: \"measurement of the route\". The odometry problem can be formulated as: given an initial pose \\(q\\) of \\(t_0\\) at some initial time, find the pose at any future time \\(t_0 + \\Delta t\\) . Given: \\(q(t_0) = q_{t_0} = q_0 = \\begin{bmatrix}x_0 & y_0 & \\theta_0 \\end{bmatrix}^T\\) Find: \\(q_{t_0 + \\Delta t}, \\forall \\Delta t > 0\\) When \\(\\Delta t\\) is small enough to consider the angular speed of the wheels constant, the pose update can be approximated as a simple sum. \\(q_{t} = \\begin{bmatrix}x_t & y_t & \\theta_t \\end{bmatrix}^T\\) \\(q_{t_{k+1}} = q_{t_k} + \\dot{q}_{t_k}(t_{k+1} - t_k)\\) The process can then be applied iteratively to find the pose at any time, and at each iteration using the previous estimate as an initial condition. TODO Add odometry update image in world frame The essence of odometry is to use the measurements of the distance traveled by each wheel in a certain time interval and use them to derive the linear and the angular displacement of the robot in time through a motion model. TODO Add odometry update image with \\(\\Delta x\\) and \\(\\Delta y\\) displacements For example, if our robot starts at time 0 with pose (0, 0, 0), and drives straight for 1 second at the speed of 1 meter per second, the odometery will tell us that the final pose will be (1 meter, 0, 0). Recall, the kinematics model, that maps the wheel velocities to the variation of the pose in time. \\[ q_{t_{k+1}} \\approx q_{t_k} + {\\color{orange}{\\dot{q}_{t_k}}}(t_{k+1} - t_k) \\] Kinematic model \\[ {\\color{orange}{\\dot{q}_{t}}} = \\begin{bmatrix}\\dot{x}_t \\\\ \\dot{y}_t \\\\ \\dot{\\theta}_t \\end{bmatrix} = \\frac{ \\color{green}{R} }{2} \\begin{bmatrix}cos(\\theta_t) & 0 \\\\ sin(\\theta_t) & 0 \\\\ 0 & 1\\end{bmatrix} \\begin{bmatrix}1 & 1 \\\\ \\frac{1}{ \\color{green}{L} } & -\\frac{1}{ \\color{green}{L} } \\end{bmatrix} \\begin{bmatrix} \\color{red}{\\dot{\\phi}_{r,t}} \\\\ \\color{red}{\\dot{\\phi}_{l,t}} \\end{bmatrix} \\] This model allows us to perform the pose update once we determine its parameters, which are the wheel radii, which we assume identical, and the distance between the wheels, or the baseline. Parameters \\({\\color{green}{R}}\\) : wheel radius \\(2 {\\color{green}{L}}\\) : baseline (distance between wheels) What's also needed is to measure the wheel angular velocities. Measurements \\(\\color{red}{\\dot{\\phi}_{l,t}}\\) : wheel angular speeds","title":"Odometry"},{"location":"theory/modeling-control/odometry/#wheel-encoders","text":"To measure the wheel angular velocities, we can use wheel encoders. Although there are various implementations, the operation principle of wheel encoders is simple. The outer rim of the motor's rotor has some evenly spaced, fixed markers. Optical encoders, for example, have a perforated disk where the empty space are the markers. TODO Add encoder wheel image robotc.net One pulse every \\({\\color{red}{\\alpha}} = \\frac{2\\pi}{N_{tot}}\\) radians Every time any of these markers transitions through some reference position on the stator, the marker is sensed, and a signal is produced. For example, in optical encoders, a light sensor will pick up the light from a source every time an empty space in the disk comes about. Knowing how many markers there are in a whole circumference, we can derive how much each wheel rotated by just counting the pulses in each of the k-th time interval. Wheel rotation (in \\(\\Delta t_k\\) ): \\({\\color{orange}{\\Delta \\phi_k}} = N_k \\cdot {\\color{red}{\\alpha}}\\) By dividing the total rotation by delta t, we can then measure the average wheel angular speed in that time frame. Angular speed: \\(\\dot{\\phi}_{t_k} \\approx \\frac{ {\\color{orange}{\\Delta \\phi_k}} }{\\Delta t_k}\\) Expanding the kinematics model expressions, we gain insight on the pose update process. \\[ \\begin{align} &&{\\color{orange}{\\dot{q}_{t}}} &= \\begin{bmatrix}\\dot{x}_t \\\\ \\dot{y}_t \\\\ \\dot{\\theta}_t \\end{bmatrix} = \\frac{ \\color{green}{R} }{2} \\begin{bmatrix}cos(\\theta_t) & 0 \\\\ sin(\\theta_t) & 0 \\\\ 0 & 1\\end{bmatrix} \\begin{bmatrix}1 & 1 \\\\ \\frac{1}{ \\color{green}{L} } & -\\frac{1}{ \\color{green}{L} } \\end{bmatrix} \\begin{bmatrix} \\color{red}{\\dot{\\phi}_{r,t}} \\\\ \\color{red}{\\dot{\\phi}_{l,t}} \\end{bmatrix} {\\color{red}{\\longleftarrow}} {\\color{red}{\\dot{\\phi}_{t_k}}} \\approx \\frac{ \\Delta \\phi_k }{\\Delta t_k} \\\\ &&{\\color{orange}{\\downarrow}} \\\\ && {\\color{green}{q_{t_{k+1}}}} \\approx q_{t_k} + &{\\color{orange}{\\dot{q}_{t_k}}}(t_{k+1} - t_k) \\\\ &&{\\color{green}{\\downarrow}} \\\\ &&{\\color{green}{x_{t_{k+1}}}} \\approx x_{t_k} + & \\frac{R}{2 \\cancel{\\Delta t_k}} \\left(\\Delta \\phi_{r,t} + \\Delta \\phi_{l,t} \\right) \\cos(\\theta_{t_k}) \\cancel{\\Delta t_k} \\\\ &&{\\color{green}{y_{t_{k+1}}}} \\approx y_{t_k} + & \\frac{R}{2} \\left(\\Delta \\phi_{r,t} + \\Delta \\phi_{l,t} \\right) \\sin(\\theta_{t_k}) \\\\ &&{\\color{green}{\\theta_{t_{k+1}}}} \\approx \\theta_{t_k} + & \\frac{R}{2L} \\left(\\Delta \\phi_{r,t} - \\Delta \\phi_{l,t} \\right) \\\\ \\end{align} \\] Notice how the time intervals cancel out, so we don't need to actually compute the angular speed of each wheel, but just the total rotation. The first step in solving the odometry is transforming the wheel rotations into traveled distances for each wheel. We count the pulses from the encoders, derive the rotation of each wheel, and then multiply by the radius of each wheel. \\[ \\begin{align} x_{t_{k+1}} &\\approx x_{t_k} + \\frac{ {\\color{orange}{R}} }{2} \\left({\\color{orange}{\\Delta\\phi_{r,t}}} + {\\color{orange}{\\Delta\\phi_{l,t}}} \\right) \\cos(\\theta_{t_k}) \\\\ y_{t_{k+1}} &\\approx y_{t_k} + \\frac{ {\\color{orange}{R}} }{2} \\left({\\color{orange}{\\Delta\\phi_{r,t}}} + {\\color{orange}{\\Delta\\phi_{l,t}}} \\right) \\sin(\\theta_{t_k}) \\\\ \\theta_{t_{k+1}} &\\approx \\theta_{t_k} + \\frac{ {\\color{orange}{R}} }{2L} \\left({\\color{orange}{\\Delta\\phi_{r,t}}} - {\\color{orange}{\\Delta\\phi_{l,t}}} \\right) \\\\ \\end{align} \\] Wheel travelled distance: \\(\\color{orange}{d_{l/r}} = R \\cdot \\Delta \\phi_{r/l}\\) TODO Add robot image with \\(d_l\\) \\(d_r\\) travelled distance The second step is to transform the wheel displacements into the linear and the angular displacements of the robot reference frame, as we have seen in the modeling section . \\[ \\begin{align} x_{t_{k+1}} &\\approx x_{t_k} + {\\color{orange}{ \\frac{ d_{r,t_k} + d_{l,t_k} }{2} }} \\cos(\\theta_{t_k}) \\\\ y_{t_{k+1}} &\\approx y_{t_k} + {\\color{orange}{ \\frac{ d_{r,t_k} + d_{l,t_k} }{2} }} \\sin(\\theta_{t_k}) \\\\ \\theta_{t_{k+1}} &\\approx \\theta_{t_k} + {\\color{red}{ \\frac{ d_{r,t_k} - d_{l,t_k} }{2L} }} \\\\ \\end{align} \\] TODO Add robot image with \\(d_l\\) \\(d_r\\) travelled distance and \\(\\Delta \\theta\\) The final step is to represent the displacement in the world frame and add the increments to the previous estimates. \\[ \\begin{align} x_{t_{k+1}} &\\approx x_{t_k} + {\\color{orange}{ d_{A,t_k} \\cos(\\theta_{t_k}) }} \\\\ y_{t_{k+1}} &\\approx y_{t_k} + {\\color{orange}{ d_{A,t_k} \\sin(\\theta_{t_k}) }} \\\\ \\theta_{t_{k+1}} &\\approx \\theta_{t_k} + \\Delta \\theta_{t_k} \\\\ \\end{align} \\] TODO Add robot image showing increment update in world frame","title":"Wheel Encoders"},{"location":"theory/modeling-control/odometry/#summary-of-odometry-equations","text":"\\[ \\begin{align} x_{t_{k+1}} &\\approx x_{t_k} + d_{A,t_k} \\cos(\\theta_{t_k}) \\\\ y_{t_{k+1}} &\\approx y_{t_k} + d_{A,t_k} \\sin(\\theta_{t_k}) \\\\ \\theta_{t_{k+1}} &\\approx \\theta_{t_k} + \\Delta \\theta_{t_k} \\\\ \\end{align} \\] \\[ \\begin{align} d_{A,t_k} &= \\frac{ d_{r,t_k} + d_{l,t_k} }{2} \\\\ \\Delta \\theta_{t_k} &= \\frac{ d_{r,t_k} - d_{l,t_k} }{2L} \\\\ d_{r/l,t_k} &= 2\\pi R \\frac{N_k}{N_{tot}} \\end{align} \\]","title":"Summary of Odometry Equations"},{"location":"theory/modeling-control/odometry/#challenges-in-odometry","text":"There are practical challenges in odometry.","title":"Challenges in Odometry"},{"location":"theory/modeling-control/odometry/#dead-reconing","text":"The first practical challenge stems from using this dead reckoning approach, which is the official name of always adding an increment to a previous estimate in order to obtain a new one. \\[ \\begin{align} x_{t_{k+1}} &\\approx x_{t_k} + \\Delta x_{t_k} \\\\ y_{t_{k+1}} &\\approx y_{t_k} + \\Delta y_{t_k} \\\\ \\theta_{t_{k+1}} &\\approx \\theta_{t_k} + \\Delta \\theta_{t_k} \\\\ \\end{align} \\] TODO Reuse odometry update image in world frame While it might work well for short distances, over time, errors like the discrete time approximation will accumulate, making the estimate drift from reality.","title":"\"Dead reconing\""},{"location":"theory/modeling-control/odometry/#kinematic-model","text":"Second, we're using a mathematical model, that of the kinematics of a differential drive robot, to translate the actual measurements to the pose of the robot. \\[ \\dot{q}_{t} = \\frac{R}{2} \\begin{bmatrix}cos(\\theta_t) & 0 \\\\ sin(\\theta_t) & 0 \\\\ 0 & 1\\end{bmatrix} \\begin{bmatrix}1 & 1 \\\\ \\frac{1}{L} & -\\frac{1}{L} \\end{bmatrix} \\begin{bmatrix}\\dot{\\phi}_{r,t} \\\\ \\dot{\\phi}_{l,t} \\end{bmatrix} \\] You might recall that we previously said that all models are wrong, although some are useful. This wisdom is ever more true when the assumptions of the model are not respected.","title":"Kinematic Model"},{"location":"theory/modeling-control/odometry/#wheel-slip","text":"In particular, we impose the condition of no slippage of the wheels. TODO Add wheel slip images/animation When the wheels slip, it means that the motor will be spinning, the encoders will be producing measurements, but the robot will not be moving the same distance as we are assuming. This will induce errors in the odometry, and they will compound over time.","title":"Wheel Slip"},{"location":"theory/modeling-control/odometry/#odometry-calibration","text":"Finally, we need to use some actual numerical values for the parameters of the model: the wheel radii - which, by the way, are assumed to be identical, but will they really be - and the robot baseline. \\[ \\begin{align} \\Delta \\theta_{t_k} &= \\frac{ d_{r,t_k} - d_{l,t_k} }{2 {\\color{red}{L}} } \\\\ d_{r/l,t_k} &= 2\\pi {\\color{red}{R}} \\frac{N_k}{N_{tot}} \\end{align} \\] Accurately measuring these parameters is very important. Even small imperfections will induce systematic errors in the odometry that, again, will compound over time. Note that although nominally identical, no two real-world physical robots will ever be the same due to manufacturing, assembly, or handling differences. To find the values of the parameters of the model that best fit our robot, we will need to perform an odometry calibration procedure.","title":"Odometry Calibration"},{"location":"theory/modeling-control/odometry/#summary","text":"Wheel encoders can be used to update the robot's pose in time: Measure the motor's angular displacements \\(\\Delta \\phi\\) in \\(\\Delta t\\) Use the kinematics mdoel to find the robot's \\(\\Delta x\\) , \\(\\Delta y\\) , \\(\\Delta \\theta\\) Update the pose by adding the calculated increments Subject to dfit in time due to accumulation of numerical, slipping/skidding and calibration impercision errors.","title":"Summary"},{"location":"theory/modeling-control/robot-representations/","text":"Robot Representations Representations of the robot and its environment are fundamental to the capabilities that make a vehicle autonomous. To sense, to plan, and to act. Different tasks might require different representations. For example, navigating the city or avoiding a pedestrian on the road. State To quantify representations, we use states. The state \\(x_t\\) of a robot and of the world has the following properties \\(x_t \\in X\\) exists independently of us and the algorithms that we choose to determine it. The state evolves over time, \\[ \\underbrace{x_0, x_1, \\dots,}_{\\color{orange}Past} \\quad \\overbrace{x_t}^{\\color{green}Present} \\quad \\underbrace{x_{t+1}, x_{t+n}, \\dots}_{\\color{red}Future} \\] the robot will need to estimate the \\(\\color{green}present\\) and \\(\\color{red}future\\) state on the fly, so it should be efficiently computable. \\[ {\\color{red}x_{t+1}} = f(\\color{green}x_t, \\color{orange}x_{t-1}, \\dots, x_0; \\color{green}u_t, \\dots, \\color{orange}u_0) \\] Good choices of state are such that given the present information, the future state is independent of the past. This is called Markov property , and it's very desirable because it allows the robot not having to keep track of all the information gathered in the past. The state is typically observed from the sensor measurements, but taking the whole history of the measurements as choice of a state is inefficient, because measurements contain redundant information and increase over time, so they require more and more computation and memory to process. Robot Pose A sufficient and efficient representation of a mobile robot is the pose \\(q_t\\) . Pose Definition \\(q_t\\) : position and orientation of the \\({\\color{orange}\\text{robot}}\\) ( \\({\\color{orange}\\text{body}}\\) ) frame relative to a \\({\\color{red}\\text{world}}\\) ( \\({\\color{red}\\text{fixed}}\\) ) reference frame. That is, the position and the orientation of the robot in space. The pose may also include the linear and the angular velocities. The environment of a mobile robot can be seen as a 2D world, but pose can be generalized to 3D as well. TODO Add reference frame image Reference Frames ( \\(\\mathbb{R}^2\\) ) To formalize a robot's pose, we need to introduce reference systems. We take a world frame with origin in W and a robot, or body, frame, which moves with the robot and has origin in point A at position (x,y) in the world frame. \\({\\color{red}\\text{World frame }}(x^w, y^w)\\) , origin in \\(W\\) \\({\\color{orange}\\text{Robot frame }}(x^r, y^r)\\) , origin in \\(A\\) orientation \\(\\theta\\) with \\(x^w\\) TODO Add reference frames image It is important to express the coordinates of any point with respect to the robot and the world frames, which in the general case are rotated and translated one with respect to the other. Moving between Reference Frames Let's look at the math on how to move between frames, starting from the simpler case of translations. TODO Add points frames image Translations Take two reference frames and assume that they are purely translated with respect to each other by \\(x_A\\) and \\(y_A\\) .","title":"Representations and models"},{"location":"theory/modeling-control/robot-representations/#robot-representations","text":"Representations of the robot and its environment are fundamental to the capabilities that make a vehicle autonomous. To sense, to plan, and to act. Different tasks might require different representations. For example, navigating the city or avoiding a pedestrian on the road.","title":"Robot Representations"},{"location":"theory/modeling-control/robot-representations/#state","text":"To quantify representations, we use states. The state \\(x_t\\) of a robot and of the world has the following properties \\(x_t \\in X\\) exists independently of us and the algorithms that we choose to determine it. The state evolves over time, \\[ \\underbrace{x_0, x_1, \\dots,}_{\\color{orange}Past} \\quad \\overbrace{x_t}^{\\color{green}Present} \\quad \\underbrace{x_{t+1}, x_{t+n}, \\dots}_{\\color{red}Future} \\] the robot will need to estimate the \\(\\color{green}present\\) and \\(\\color{red}future\\) state on the fly, so it should be efficiently computable. \\[ {\\color{red}x_{t+1}} = f(\\color{green}x_t, \\color{orange}x_{t-1}, \\dots, x_0; \\color{green}u_t, \\dots, \\color{orange}u_0) \\] Good choices of state are such that given the present information, the future state is independent of the past. This is called Markov property , and it's very desirable because it allows the robot not having to keep track of all the information gathered in the past. The state is typically observed from the sensor measurements, but taking the whole history of the measurements as choice of a state is inefficient, because measurements contain redundant information and increase over time, so they require more and more computation and memory to process.","title":"State"},{"location":"theory/modeling-control/robot-representations/#robot-pose","text":"A sufficient and efficient representation of a mobile robot is the pose \\(q_t\\) . Pose Definition \\(q_t\\) : position and orientation of the \\({\\color{orange}\\text{robot}}\\) ( \\({\\color{orange}\\text{body}}\\) ) frame relative to a \\({\\color{red}\\text{world}}\\) ( \\({\\color{red}\\text{fixed}}\\) ) reference frame. That is, the position and the orientation of the robot in space. The pose may also include the linear and the angular velocities. The environment of a mobile robot can be seen as a 2D world, but pose can be generalized to 3D as well. TODO Add reference frame image","title":"Robot Pose"},{"location":"theory/modeling-control/robot-representations/#reference-frames-mathbbr2","text":"To formalize a robot's pose, we need to introduce reference systems. We take a world frame with origin in W and a robot, or body, frame, which moves with the robot and has origin in point A at position (x,y) in the world frame. \\({\\color{red}\\text{World frame }}(x^w, y^w)\\) , origin in \\(W\\) \\({\\color{orange}\\text{Robot frame }}(x^r, y^r)\\) , origin in \\(A\\) orientation \\(\\theta\\) with \\(x^w\\) TODO Add reference frames image It is important to express the coordinates of any point with respect to the robot and the world frames, which in the general case are rotated and translated one with respect to the other.","title":"Reference Frames (\\(\\mathbb{R}^2\\))"},{"location":"theory/modeling-control/robot-representations/#moving-between-reference-frames","text":"Let's look at the math on how to move between frames, starting from the simpler case of translations. TODO Add points frames image","title":"Moving between Reference Frames"},{"location":"theory/modeling-control/robot-representations/#translations","text":"Take two reference frames and assume that they are purely translated with respect to each other by \\(x_A\\) and \\(y_A\\) .","title":"Translations"},{"location":"theory/object-detection/introduction-to-neural-networks/","text":"Introduction to Neural Networks References Self-Driving Cars with Duckietown https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/ Gradient Descent","title":"Introduction to neural networks"},{"location":"theory/object-detection/introduction-to-neural-networks/#introduction-to-neural-networks","text":"","title":"Introduction to Neural Networks"},{"location":"theory/object-detection/introduction-to-neural-networks/#references","text":"Self-Driving Cars with Duckietown https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/ Gradient Descent","title":"References"},{"location":"theory/robot-vision/camera-calibration/","text":"Camera Calibration As mentioned previously, the pin-hole camera model describes a light-proof box with a small aperture that allows a limited amount of light reflecting off of objects in the scene to pass through and strike the sensor plane. The result is an idealized, yet surprisingly useful approximation of the cameras commonly used on a variety of robots. The model describes how points in the world are mapped to image-space coordinates. This is done, by first transorming points given in world coordinates into the camera's reference frame, and then projecting the points onto the image plane. In order to use this model, we need to know its parameters, such as the focal length \\(f\\) and skew \\(s\\) of the camera (intrinsic parameters), and its pose relative to the frame of the world or the robot (extrinsic parameters). Calibration refers to the process of estimating these parameters. Note TODO missing transformation image \\[ \\mathbf{x} = \\underbrace{\\begin{bmatrix} f_x & s & p_x \\\\ 0 & f_y & p_y \\\\ 0 & 0 & 1 \\\\ \\end{bmatrix}}_{\\textbf{intrinsics} \\\\ \\text{5 DOF + lens distortion}} \\underbrace{\\begin{bmatrix} R | t \\end{bmatrix}}_{\\text{\\textbf{extrinsics}} \\\\ \\text{6 DOF}} \\mathbf{X} = P \\mathbf{X} \\] Having a mathematical expression for perspective projection allows us to reason over a robot's three-dimensional world from two-dimensional images. Suppose an algorithm that detects lane markings in an image received from the robot's camera. In order to use these detections to keep the robot in its lane, it would be useful to understand where tehy are in relation to the robot. As we will see, calibrating the camera allows us to transform these detections into the robot's reference frame. The result can then be used to understand the lane geometry and where the robot is relative to the lane. There are different approaches to calibrating a camera. The pin-hole camera model can be represendet as a product of two matrices, one defined in terms of up to five parameters intrinsic to the camera, and the other specifying the camera six degrees of freedom pose. Ignoring lens distortions, there is a total of 11 parameters that define the camera model. Nonlinear intrinsic parameters such as lens distortion are also important although they cannot be included in the linear camera model described by the intrinsic parameter matrix. Many modern camera calibration algorithms estimate these intrinsic parameters as well in the form of non-linear optimisation techniques. This is done in the form of optimising the camera and distortion parameters in the form of what is generally known as bundle adjustment. 1 The calibration process typically involves associating the coordinates of points in the scene with their corresponding projections onto the image plane. Each correspondence \\(\\mathbf{x}_i \\leftrightarrow \\mathbf{X}_i\\) provides two constraints on the projection model. One for each of the two image coordinates. With 11 degrees-of-freedom in the model, we need at least six point-pairs but have to be careful about avoiding degeneracies. For example, the three points can't all lie on the same line or the same plane. In practice, we use a calibration target that provides a well-defined set of 3D points that are easy to detect in the image, often involving one or more checkerboard patterns. One option for calibrating the camera is to directly estimate the entries of the three-by-four camera matrix P that maps 3D scene points to their 2D image coordinates. While the matrix has 12 entries, there are only 11 degrees-of-freedom, since perspective projection is only defined up to scale. Calibration via Direct Linear Transformation Calibration as an Optimization Problem Homographies and Homography Estimation References Self-Driving Cars with Duckietown Mathworks Camera Calibration Pin-hole Camera Calibration with Matlab Toolbox Documentation Wikipedia Camera Resectioning \u21a9","title":"Camera Calibration"},{"location":"theory/robot-vision/camera-calibration/#camera-calibration","text":"As mentioned previously, the pin-hole camera model describes a light-proof box with a small aperture that allows a limited amount of light reflecting off of objects in the scene to pass through and strike the sensor plane. The result is an idealized, yet surprisingly useful approximation of the cameras commonly used on a variety of robots. The model describes how points in the world are mapped to image-space coordinates. This is done, by first transorming points given in world coordinates into the camera's reference frame, and then projecting the points onto the image plane. In order to use this model, we need to know its parameters, such as the focal length \\(f\\) and skew \\(s\\) of the camera (intrinsic parameters), and its pose relative to the frame of the world or the robot (extrinsic parameters). Calibration refers to the process of estimating these parameters. Note TODO missing transformation image \\[ \\mathbf{x} = \\underbrace{\\begin{bmatrix} f_x & s & p_x \\\\ 0 & f_y & p_y \\\\ 0 & 0 & 1 \\\\ \\end{bmatrix}}_{\\textbf{intrinsics} \\\\ \\text{5 DOF + lens distortion}} \\underbrace{\\begin{bmatrix} R | t \\end{bmatrix}}_{\\text{\\textbf{extrinsics}} \\\\ \\text{6 DOF}} \\mathbf{X} = P \\mathbf{X} \\] Having a mathematical expression for perspective projection allows us to reason over a robot's three-dimensional world from two-dimensional images. Suppose an algorithm that detects lane markings in an image received from the robot's camera. In order to use these detections to keep the robot in its lane, it would be useful to understand where tehy are in relation to the robot. As we will see, calibrating the camera allows us to transform these detections into the robot's reference frame. The result can then be used to understand the lane geometry and where the robot is relative to the lane. There are different approaches to calibrating a camera. The pin-hole camera model can be represendet as a product of two matrices, one defined in terms of up to five parameters intrinsic to the camera, and the other specifying the camera six degrees of freedom pose. Ignoring lens distortions, there is a total of 11 parameters that define the camera model. Nonlinear intrinsic parameters such as lens distortion are also important although they cannot be included in the linear camera model described by the intrinsic parameter matrix. Many modern camera calibration algorithms estimate these intrinsic parameters as well in the form of non-linear optimisation techniques. This is done in the form of optimising the camera and distortion parameters in the form of what is generally known as bundle adjustment. 1 The calibration process typically involves associating the coordinates of points in the scene with their corresponding projections onto the image plane. Each correspondence \\(\\mathbf{x}_i \\leftrightarrow \\mathbf{X}_i\\) provides two constraints on the projection model. One for each of the two image coordinates. With 11 degrees-of-freedom in the model, we need at least six point-pairs but have to be careful about avoiding degeneracies. For example, the three points can't all lie on the same line or the same plane. In practice, we use a calibration target that provides a well-defined set of 3D points that are easy to detect in the image, often involving one or more checkerboard patterns. One option for calibrating the camera is to directly estimate the entries of the three-by-four camera matrix P that maps 3D scene points to their 2D image coordinates. While the matrix has 12 entries, there are only 11 degrees-of-freedom, since perspective projection is only defined up to scale.","title":"Camera Calibration"},{"location":"theory/robot-vision/camera-calibration/#calibration-via-direct-linear-transformation","text":"","title":"Calibration via Direct Linear Transformation"},{"location":"theory/robot-vision/camera-calibration/#calibration-as-an-optimization-problem","text":"","title":"Calibration as an Optimization Problem"},{"location":"theory/robot-vision/camera-calibration/#homographies-and-homography-estimation","text":"","title":"Homographies and Homography Estimation"},{"location":"theory/robot-vision/camera-calibration/#references","text":"Self-Driving Cars with Duckietown Mathworks Camera Calibration Pin-hole Camera Calibration with Matlab Toolbox Documentation Wikipedia Camera Resectioning \u21a9","title":"References"},{"location":"theory/robot-vision/image-processing/","text":"Image Filtering The concept of (spacial) frequency in images is important to filter certain frequencies and thus enhance or dampen certain features of an image, such as corners or edges. Frequency in Images As with 1-D signals, frequency in images describes a rate of change in pixel intensities. The Fourier Transform (FT) is an important image processing tool which is used to decompose an image into its frequency components. The output of an FT represents the image in the frequency domain, while the input image is the spatial domain (x, y) equivalent. OpenCV Fourier Transform Linear Filtering and Convolution OpenCV Filtering Tutorial Gradient and Sobel Filter Gradients are a measure of intensity change in an image. As images are treated as functions \\(I(x,y)\\) , the gradient is the derivative of this function \\(I'(x,y)\\) , describing the change in intensity \\(\\Delta I\\) at pixel locations \\(x\\) and \\(y\\) . The Sobel filter is very commonly used in edge detection and in finding patterns in intensity in an image. Applying a Sobel filter to an image is a way of taking (an approximation) of the derivative of the image in the \\(x\\) or \\(y\\) direction. \\[ S_x = \\begin{matrix} -1 & 0 & 1 \\\\ -2 & 0 & 2 \\\\ -1 & 0 & 1 \\\\ \\end{matrix} \\] \\[ S_y = \\begin{matrix} -1 & -2 & -1 \\\\ 0 & 0 & 0 \\\\ 1 & 2 & 1 \\\\ \\end{matrix} \\] Note TODO add image examples Magnitude: \\(S = \\sqrt{S_x^2 + S_y^2}\\) Direction: \\(\\theta = \\atan2{S_y, S_x}\\) For more details see also the Sobel operator on Wikipedia . Image Blurring and Low-pass Filter To block noise in an image, use a filter that filters high frequencies, e.g. specle or discoloration, and let low frequency components of an image pass, such as smooth surfaces. Gaussian Kernels References Self-driving cars with Duckietown OpenCV Fourier Transform OpenCV Filtering tutorial Wikipedia Sobel operator","title":"Image Processing"},{"location":"theory/robot-vision/image-processing/#image-filtering","text":"The concept of (spacial) frequency in images is important to filter certain frequencies and thus enhance or dampen certain features of an image, such as corners or edges.","title":"Image Filtering"},{"location":"theory/robot-vision/image-processing/#frequency-in-images","text":"As with 1-D signals, frequency in images describes a rate of change in pixel intensities. The Fourier Transform (FT) is an important image processing tool which is used to decompose an image into its frequency components. The output of an FT represents the image in the frequency domain, while the input image is the spatial domain (x, y) equivalent. OpenCV Fourier Transform","title":"Frequency in Images"},{"location":"theory/robot-vision/image-processing/#linear-filtering-and-convolution","text":"OpenCV Filtering Tutorial","title":"Linear Filtering and Convolution"},{"location":"theory/robot-vision/image-processing/#gradient-and-sobel-filter","text":"Gradients are a measure of intensity change in an image. As images are treated as functions \\(I(x,y)\\) , the gradient is the derivative of this function \\(I'(x,y)\\) , describing the change in intensity \\(\\Delta I\\) at pixel locations \\(x\\) and \\(y\\) . The Sobel filter is very commonly used in edge detection and in finding patterns in intensity in an image. Applying a Sobel filter to an image is a way of taking (an approximation) of the derivative of the image in the \\(x\\) or \\(y\\) direction. \\[ S_x = \\begin{matrix} -1 & 0 & 1 \\\\ -2 & 0 & 2 \\\\ -1 & 0 & 1 \\\\ \\end{matrix} \\] \\[ S_y = \\begin{matrix} -1 & -2 & -1 \\\\ 0 & 0 & 0 \\\\ 1 & 2 & 1 \\\\ \\end{matrix} \\] Note TODO add image examples Magnitude: \\(S = \\sqrt{S_x^2 + S_y^2}\\) Direction: \\(\\theta = \\atan2{S_y, S_x}\\) For more details see also the Sobel operator on Wikipedia .","title":"Gradient and Sobel Filter"},{"location":"theory/robot-vision/image-processing/#image-blurring-and-low-pass-filter","text":"To block noise in an image, use a filter that filters high frequencies, e.g. specle or discoloration, and let low frequency components of an image pass, such as smooth surfaces.","title":"Image Blurring and Low-pass Filter"},{"location":"theory/robot-vision/image-processing/#gaussian-kernels","text":"","title":"Gaussian Kernels"},{"location":"theory/robot-vision/image-processing/#references","text":"Self-driving cars with Duckietown OpenCV Fourier Transform OpenCV Filtering tutorial Wikipedia Sobel operator","title":"References"},{"location":"theory/robot-vision/pinhole-camera-model/","text":"Pinhole Camera Model Pinhole Camera Perspective Projection Resources Duckietown MOOC Edx","title":"Pinhole Camera Model"},{"location":"theory/robot-vision/pinhole-camera-model/#pinhole-camera-model","text":"","title":"Pinhole Camera Model"},{"location":"theory/robot-vision/pinhole-camera-model/#pinhole-camera","text":"","title":"Pinhole Camera"},{"location":"theory/robot-vision/pinhole-camera-model/#perspective-projection","text":"","title":"Perspective Projection"},{"location":"theory/robot-vision/pinhole-camera-model/#resources","text":"Duckietown MOOC Edx","title":"Resources"},{"location":"theory/robot-vision/tracking-image-features/","text":"Tracking Image Features Intensity Gradient and Filtering Harris Corner Detector Overview of Popular Keypoint Detectors HOG Descriptors (SIFT and SURF, etc.) Binary Descriptors (BRISK, ...) References Text Books Peter Corke, Robotics, Vision, and Control. ( Official website ) This book has a particular emphasis on computer vision for robotics, but as the title suggests, it goes beyond robot vision. Richard Szeliski, Computer Vision: Algorithms and Applications, Springer. The official website provides drafts of recent updates to the book. Richard Hartley and Andrew Zisserman, Multiple View Geometry, Cambridge University Press. ( Official website ) David Forsyth and Jean Ponce, Computer Vision: A Modern Approach, Pearson. ( Publisher's website ) Courses Udacity Sensor Fusion Engineer Nanodegree Stanford University, CS231A: Computer Vision, From 3D Reconstruction to Recognition ( Course website ) Georgia Tech, CS 6476: Computer Vision ( Course website ) MIT, 6.819/6.869: Advances in Computer Vision ( Course website ) Online Resources OpenCV Tutorials ( website ) OpenCV Python Tutorials ( website ) Medium SIFT (Scale Invariant Feature Transform)","title":"Tracking image features"},{"location":"theory/robot-vision/tracking-image-features/#tracking-image-features","text":"","title":"Tracking Image Features"},{"location":"theory/robot-vision/tracking-image-features/#intensity-gradient-and-filtering","text":"","title":"Intensity Gradient and Filtering"},{"location":"theory/robot-vision/tracking-image-features/#harris-corner-detector","text":"","title":"Harris Corner Detector"},{"location":"theory/robot-vision/tracking-image-features/#overview-of-popular-keypoint-detectors","text":"","title":"Overview of Popular Keypoint Detectors"},{"location":"theory/robot-vision/tracking-image-features/#hog-descriptors-sift-and-surf-etc","text":"","title":"HOG Descriptors (SIFT and SURF, etc.)"},{"location":"theory/robot-vision/tracking-image-features/#binary-descriptors-brisk","text":"","title":"Binary Descriptors (BRISK, ...)"},{"location":"theory/robot-vision/tracking-image-features/#references","text":"","title":"References"},{"location":"theory/robot-vision/tracking-image-features/#text-books","text":"Peter Corke, Robotics, Vision, and Control. ( Official website ) This book has a particular emphasis on computer vision for robotics, but as the title suggests, it goes beyond robot vision. Richard Szeliski, Computer Vision: Algorithms and Applications, Springer. The official website provides drafts of recent updates to the book. Richard Hartley and Andrew Zisserman, Multiple View Geometry, Cambridge University Press. ( Official website ) David Forsyth and Jean Ponce, Computer Vision: A Modern Approach, Pearson. ( Publisher's website )","title":"Text Books"},{"location":"theory/robot-vision/tracking-image-features/#courses","text":"Udacity Sensor Fusion Engineer Nanodegree Stanford University, CS231A: Computer Vision, From 3D Reconstruction to Recognition ( Course website ) Georgia Tech, CS 6476: Computer Vision ( Course website ) MIT, 6.819/6.869: Advances in Computer Vision ( Course website )","title":"Courses"},{"location":"theory/robot-vision/tracking-image-features/#online-resources","text":"OpenCV Tutorials ( website ) OpenCV Python Tutorials ( website ) Medium SIFT (Scale Invariant Feature Transform)","title":"Online Resources"}]}